{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Teknofest Trendyol E-Ticaret iÃ§in yapay zeka modeli"
      ],
      "metadata": {
        "id": "3vvbeU6lWXHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deneme 1 Baseline OluÅŸturma"
      ],
      "metadata": {
        "id": "B_zr3R1kpYko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 1** EÄŸitim ve modelleri kaydetme"
      ],
      "metadata": {
        "id": "PXOfFp1PpipT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"--- KOD 1: Model EÄŸitme ve KAYDETME SÃ¼reci BaÅŸladÄ± ---\")\n",
        "\n",
        "# --- AdÄ±m 1: Veri YÃ¼kleme ---\n",
        "# Colab'a yÃ¼klediÄŸiniz dosyanÄ±n tam yolunu buraya yazÄ±n\n",
        "TRAIN_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "\n",
        "print(f\"EÄŸitim verisi yÃ¼kleniyor: {TRAIN_PATH}\")\n",
        "train_full_df = pl.read_parquet(TRAIN_PATH)\n",
        "print(f\"Toplam eÄŸitim verisi boyutu: {train_full_df.shape}\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 2: Veriyi Zamana GÃ¶re AyÄ±rma (Train / Validation Split) ---\n",
        "print(\"EÄŸitim verisi, zamana gÃ¶re sÄ±ralanÄ±p yÃ¼zdesel olarak ayrÄ±lÄ±yor...\")\n",
        "if \"ts_hour\" in train_full_df.columns and train_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    train_full_df = train_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "\n",
        "train_full_df = train_full_df.sort(\"ts_hour\")\n",
        "\n",
        "# %85 train, %15 valid olarak ayÄ±r\n",
        "split_index = int(len(train_full_df) * 0.85)\n",
        "train_df = train_full_df[:split_index]\n",
        "val_df = train_full_df[split_index:]\n",
        "\n",
        "print(f\"Train parÃ§asÄ± boyutu: {train_df.shape}\")\n",
        "print(f\"Validation parÃ§asÄ± boyutu: {val_df.shape}\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 3: Veri HazÄ±rlama ve Tip DÃ¶nÃ¼ÅŸÃ¼mÃ¼ ---\n",
        "print(\"Veri setleri Pandas'a Ã§evriliyor ve model iÃ§in hazÄ±rlanÄ±yor...\")\n",
        "train_pd = train_df.to_pandas().fillna(0)\n",
        "val_pd = val_df.to_pandas().fillna(0)\n",
        "\n",
        "# Object tipli sÃ¼tunlarÄ± 'category' tipine Ã§evirerek hatayÄ± gideriyoruz\n",
        "for col in train_pd.columns:\n",
        "    if train_pd[col].dtype == 'object':\n",
        "        train_pd[col] = train_pd[col].astype('category')\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "# Modelde kullanÄ±lacak Ã¶zellikleri ve hedefleri belirliyoruz\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "# Test setinde olmayan ve hedefle iliÅŸkili sÄ±zÄ±ntÄ± yapabilecek sÃ¼tunlarÄ± hariÃ§ tutuyoruz\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features = [col for col in train_pd.columns if col not in exclude_cols]\n",
        "cat_features = [col for col in features if isinstance(train_pd[col].dtype, pd.CategoricalDtype)]\n",
        "\n",
        "print(f\"KullanÄ±lacak Ã¶zellik sayÄ±sÄ±: {len(features)}\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 4: lgb.Dataset OluÅŸturma (RAM Dostu KÄ±sÄ±m) ---\n",
        "print(\"RAM verimliliÄŸi iÃ§in lgb.Dataset nesneleri oluÅŸturuluyor...\")\n",
        "lgb_train_ordered = lgb.Dataset(train_pd[features], label=train_pd['ordered'], categorical_feature=cat_features)\n",
        "lgb_valid_ordered = lgb.Dataset(val_pd[features], label=val_pd['ordered'], categorical_feature=cat_features)\n",
        "\n",
        "lgb_train_clicked = lgb.Dataset(train_pd[features], label=train_pd['clicked'], categorical_feature=cat_features)\n",
        "lgb_valid_clicked = lgb.Dataset(val_pd[features], label=val_pd['clicked'], categorical_feature=cat_features)\n",
        "\n",
        "\n",
        "# --- AdÄ±m 5: Model EÄŸitimi ve Kaydetme ---\n",
        "params = {\n",
        "    \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\",\n",
        "    \"learning_rate\": 0.03, \"num_leaves\": 64, \"verbose\": -1, \"seed\": 42\n",
        "}\n",
        "callbacks = [lgb.early_stopping(100, verbose=True)]\n",
        "\n",
        "# DÃœZELTME: Modelleri kaydetmeden Ã¶nce klasÃ¶rÃ¼n var olduÄŸundan emin ol\n",
        "MODEL_DIR = \"models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# 5.1 'ordered' modeli\n",
        "print(\"\\n'ordered' modeli eÄŸitiliyor...\")\n",
        "ordered_counts = train_pd['ordered'].value_counts()\n",
        "params['scale_pos_weight'] = ordered_counts[0] / ordered_counts[1]\n",
        "model_ordered = lgb.train(params, lgb_train_ordered, valid_sets=[lgb_train_ordered, lgb_valid_ordered], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_ordered.save_model(os.path.join(MODEL_DIR, \"model_ordered.txt\"))\n",
        "print(\"âœ… 'ordered' modeli 'model_ordered.txt' olarak kaydedildi.\")\n",
        "\n",
        "\n",
        "# 5.2 'clicked' modeli\n",
        "print(\"\\n'clicked' modeli eÄŸitiliyor...\")\n",
        "clicked_counts = train_pd['clicked'].value_counts()\n",
        "params['scale_pos_weight'] = clicked_counts[0] / clicked_counts[1]\n",
        "model_clicked = lgb.train(params, lgb_train_clicked, valid_sets=[lgb_train_clicked, lgb_valid_clicked], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_clicked.save_model(os.path.join(MODEL_DIR, \"model_clicked.txt\"))\n",
        "print(\"âœ… 'clicked' modeli 'model_clicked.txt' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n--- EÄŸitim ve Kaydetme Ä°ÅŸlemi TamamlandÄ±! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfRw9tUKpf_e",
        "outputId": "939d0f68-9693-4273-d59f-a1ab74820fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 1: Model EÄŸitme ve KAYDETME SÃ¼reci BaÅŸladÄ± ---\n",
            "EÄŸitim verisi yÃ¼kleniyor: /content/Trendyol Veri/enriched_train_data_FINAL.parquet\n",
            "Toplam eÄŸitim verisi boyutu: (2773805, 37)\n",
            "EÄŸitim verisi, zamana gÃ¶re sÄ±ralanÄ±p yÃ¼zdesel olarak ayrÄ±lÄ±yor...\n",
            "Train parÃ§asÄ± boyutu: (2357734, 37)\n",
            "Validation parÃ§asÄ± boyutu: (416071, 37)\n",
            "Veri setleri Pandas'a Ã§evriliyor ve model iÃ§in hazÄ±rlanÄ±yor...\n",
            "KullanÄ±lacak Ã¶zellik sayÄ±sÄ±: 23\n",
            "RAM verimliliÄŸi iÃ§in lgb.Dataset nesneleri oluÅŸturuluyor...\n",
            "\n",
            "'ordered' modeli eÄŸitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttrain's auc: 0.764107\tvalid's auc: 0.700363\n",
            "âœ… 'ordered' modeli 'model_ordered.txt' olarak kaydedildi.\n",
            "\n",
            "'clicked' modeli eÄŸitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[142]\ttrain's auc: 0.715502\tvalid's auc: 0.658479\n",
            "âœ… 'clicked' modeli 'model_clicked.txt' olarak kaydedildi.\n",
            "\n",
            "--- EÄŸitim ve Kaydetme Ä°ÅŸlemi TamamlandÄ±! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 2** Tahmini Skor Ãœretme Ve Kaggle Ä°Ã§in Uygun Submission DosyasÄ± HazÄ±rlama"
      ],
      "metadata": {
        "id": "4l9IB0C1pxb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "# trendyol_metric_group_auc.py dosyasÄ±nÄ±n Colab ortamÄ±nda olduÄŸundan emin olun\n",
        "from trendyol_metric_group_auc import score\n",
        "\n",
        "print(\"--- KOD 2: Tahmin ve Skorlama SÃ¼reci BaÅŸladÄ± ---\")\n",
        "\n",
        "# --- AdÄ±m 1: Gerekli Veri ve KaydedilmiÅŸ Modelleri YÃ¼kleme ---\n",
        "VAL_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "TEST_PATH = \"/content/Trendyol Veri/enriched_test_data_FINAL.parquet\"\n",
        "MODEL_ORDERED_PATH = '/content/models/model_ordered.txt'\n",
        "MODEL_CLICKED_PATH = '/content/models/model_clicked.txt'\n",
        "\n",
        "print(\"Veriler ve kaydedilmiÅŸ modeller yÃ¼kleniyor...\")\n",
        "val_full_df = pl.read_parquet(VAL_PATH)\n",
        "test_df_pl = pl.read_parquet(TEST_PATH)\n",
        "model_ordered = lgb.Booster(model_file=MODEL_ORDERED_PATH)\n",
        "model_clicked = lgb.Booster(model_file=MODEL_CLICKED_PATH)\n",
        "print(\"âœ… Veriler ve modeller baÅŸarÄ±yla yÃ¼klendi.\")\n",
        "\n",
        "# --- AdÄ±m 2: Validation Seti HazÄ±rlama ve Lokal Skor Hesaplama ---\n",
        "print(\"\\nLokal skor iÃ§in validation seti hazÄ±rlanÄ±yor...\")\n",
        "if \"ts_hour\" in val_full_df.columns and val_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    val_full_df = val_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "val_full_df = val_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(val_full_df) * 0.85)\n",
        "val_pd = val_full_df[split_index:].to_pandas().fillna(0)\n",
        "\n",
        "# Ã–zellik listesini eÄŸitimdekiyle tutarlÄ± olacak ÅŸekilde belirle\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features = [col for col in val_pd.columns if col not in exclude_cols]\n",
        "for col in features:\n",
        "    if val_pd[col].dtype == 'object':\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "val_pd['p_order'] = model_ordered.predict(val_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "val_pd['p_click'] = model_clicked.predict(val_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "val_pd['final_score'] = 0.7 * val_pd['p_order'] + 0.3 * val_pd['p_click']\n",
        "\n",
        "# Skor fonksiyonu iÃ§in gerekli DataFrame'leri oluÅŸtur\n",
        "val_solution = val_pd.groupby('session_id').agg(\n",
        "    ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])),\n",
        "    clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])),\n",
        "    all_items=('content_id_hashed', ' '.join)\n",
        ").reset_index()\n",
        "val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n",
        "val_submission.rename(columns={'content_id_hashed': 'prediction'}, inplace=True)\n",
        "\n",
        "try:\n",
        "    local_final_score = score(val_solution, val_submission, 'session_id')\n",
        "    print(\"\\n-------------------------------------------\")\n",
        "    print(f\"ğŸ† LOKAL SKORUNUZ: {local_final_score:.5f}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "except Exception as e:\n",
        "    print(f\"Skor hesaplanÄ±rken bir hata oluÅŸtu: {e}\")\n",
        "\n",
        "# --- AdÄ±m 3: Kaggle iÃ§in Submission DosyasÄ± OluÅŸturma ---\n",
        "print(\"\\nTest verisi hazÄ±rlanÄ±yor ve Kaggle iÃ§in submission dosyasÄ± oluÅŸturuluyor...\")\n",
        "test_pd = test_df_pl.to_pandas().fillna(0)\n",
        "for col in features:\n",
        "    if test_pd[col].dtype == 'object':\n",
        "        test_pd[col] = test_pd[col].astype('category')\n",
        "\n",
        "p_order_test = model_ordered.predict(test_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "p_click_test = model_clicked.predict(test_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "\n",
        "test_df_pl = test_df_pl.with_columns(\n",
        "    final_score=(0.7 * pl.Series(p_order_test)) + (0.3 * pl.Series(p_click_test))\n",
        ")\n",
        "\n",
        "submission_df = test_df_pl.sort([\"session_id\", \"final_score\"], descending=True).group_by(\"session_id\").agg(\n",
        "    pl.col(\"content_id_hashed\").alias(\"prediction\")\n",
        ").with_columns(\n",
        "    pl.col(\"prediction\").list.join(\" \")\n",
        ")\n",
        "\n",
        "# SaÄŸlama kontrolÃ¼\n",
        "expected_rows = 18589\n",
        "actual_rows = submission_df.shape[0]\n",
        "print(f\"\\nOluÅŸturulan submission satÄ±r sayÄ±sÄ±: {actual_rows} (Beklenen: {expected_rows})\")\n",
        "if actual_rows == expected_rows: print(\"âœ… SatÄ±r sayÄ±sÄ± doÄŸru.\")\n",
        "else: print(\"âŒ UYARI: SatÄ±r sayÄ±sÄ± yanlÄ±ÅŸ!\")\n",
        "\n",
        "submission_path = \"submission_final_v4.csv\"\n",
        "submission_df.write_csv(submission_path)\n",
        "print(f\"\\nSubmission dosyasÄ± '{submission_path}' olarak kaydedildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jceyqVBRpz-Z",
        "outputId": "a64c6c3e-b13a-4de8-cfc9-31895215ccad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 2: Tahmin ve Skorlama SÃ¼reci BaÅŸladÄ± ---\n",
            "Veriler ve kaydedilmiÅŸ modeller yÃ¼kleniyor...\n",
            "âœ… Veriler ve modeller baÅŸarÄ±yla yÃ¼klendi.\n",
            "\n",
            "Lokal skor iÃ§in validation seti hazÄ±rlanÄ±yor...\n",
            "Ordered AUC:  0.6506964811575329\n",
            "Clicked AUC:  0.5915140232878667\n",
            "\n",
            "-------------------------------------------\n",
            "ğŸ† LOKAL SKORUNUZ: 0.63294\n",
            "-------------------------------------------\n",
            "\n",
            "Test verisi hazÄ±rlanÄ±yor ve Kaggle iÃ§in submission dosyasÄ± oluÅŸturuluyor...\n",
            "\n",
            "OluÅŸturulan submission satÄ±r sayÄ±sÄ±: 18589 (Beklenen: 18589)\n",
            "âœ… SatÄ±r sayÄ±sÄ± doÄŸru.\n",
            "\n",
            "Submission dosyasÄ± 'submission_final_v4.csv' olarak kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deneme 2 GÃ¼nÃ¼n Saati / HaftanÄ±n GÃ¼nÃ¼ Ã–zellikleriyle EÄŸitim"
      ],
      "metadata": {
        "id": "yrB-Np4yyumu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BÃ¶lÃ¼m 1** Yeni eklenen Ã¶zellik SÃ¼tunlarÄ± ile modelleri eÄŸitmek ve kaydetmek"
      ],
      "metadata": {
        "id": "RnbCPp-vzm6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"--- KOD 1: Model EÄŸitme (Yeni Zamansal Ã–zellikler ile) ---\")\n",
        "\n",
        "# --- AdÄ±m 1: Veri YÃ¼kleme ---\n",
        "TRAIN_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "print(f\"EÄŸitim verisi yÃ¼kleniyor: {TRAIN_PATH}\")\n",
        "train_full_df = pl.read_parquet(TRAIN_PATH)\n",
        "\n",
        "# --- AdÄ±m 2: Veriyi Zamana GÃ¶re AyÄ±rma ---\n",
        "print(\"EÄŸitim verisi, zamana gÃ¶re sÄ±ralanÄ±p yÃ¼zdesel olarak ayrÄ±lÄ±yor...\")\n",
        "if \"ts_hour\" in train_full_df.columns and train_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    train_full_df = train_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "\n",
        "# --- AdÄ±m 2.5: YENÄ° ZAMANSAL Ã–ZELLÄ°KLER ÃœRETME ---\n",
        "print(\"Yeni zamansal Ã¶zellikler Ã¼retiliyor: 'hour_of_day' ve 'day_of_week'...\")\n",
        "train_full_df = train_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\") # Pazartesi=1, Pazar=7\n",
        ")\n",
        "\n",
        "# --- AdÄ±m 3: Train / Validation Split ---\n",
        "train_full_df = train_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(train_full_df) * 0.85)\n",
        "train_df, val_df = train_full_df[:split_index], train_full_df[split_index:]\n",
        "\n",
        "print(f\"Train parÃ§asÄ± boyutu: {train_df.shape}\")\n",
        "print(f\"Validation parÃ§asÄ± boyutu: {val_df.shape}\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 4: Veri HazÄ±rlama ve Tip DÃ¶nÃ¼ÅŸÃ¼mÃ¼ ---\n",
        "print(\"Veri setleri Pandas'a Ã§evriliyor ve model iÃ§in hazÄ±rlanÄ±yor...\")\n",
        "train_pd = train_df.to_pandas().fillna(0)\n",
        "val_pd = val_df.to_pandas().fillna(0)\n",
        "\n",
        "for col in train_pd.columns:\n",
        "    if train_pd[col].dtype == 'object':\n",
        "        train_pd[col] = train_pd[col].astype('category')\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features = [col for col in train_pd.columns if col not in exclude_cols]\n",
        "cat_features = [col for col in features if isinstance(train_pd[col].dtype, pd.CategoricalDtype) or train_pd[col].dtype.name == 'category']\n",
        "\n",
        "print(f\"KullanÄ±lacak Ã¶zellik sayÄ±sÄ±: {len(features)}\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 5: lgb.Dataset OluÅŸturma ---\n",
        "print(\"lgb.Dataset nesneleri oluÅŸturuluyor...\")\n",
        "lgb_train_ordered = lgb.Dataset(train_pd[features], label=train_pd['ordered'], categorical_feature=cat_features)\n",
        "lgb_valid_ordered = lgb.Dataset(val_pd[features], label=val_pd['ordered'], categorical_feature=cat_features)\n",
        "lgb_train_clicked = lgb.Dataset(train_pd[features], label=train_pd['clicked'], categorical_feature=cat_features)\n",
        "lgb_valid_clicked = lgb.Dataset(val_pd[features], label=val_pd['clicked'], categorical_feature=cat_features)\n",
        "\n",
        "\n",
        "# --- AdÄ±m 6: Model EÄŸitimi ve Kaydetme ---\n",
        "params = {\n",
        "    \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\",\n",
        "    \"learning_rate\": 0.03, \"num_leaves\": 64, \"verbose\": -1, \"seed\": 42\n",
        "}\n",
        "callbacks = [lgb.early_stopping(100, verbose=True)]\n",
        "\n",
        "MODEL_DIR = \"models_2_doldur_HoursAndDay\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# 6.1 'ordered' modeli\n",
        "print(\"\\n'ordered' modeli eÄŸitiliyor...\")\n",
        "ordered_counts = train_pd['ordered'].value_counts()\n",
        "params['scale_pos_weight'] = ordered_counts[0] / ordered_counts[1]\n",
        "model_ordered = lgb.train(params, lgb_train_ordered, valid_sets=[lgb_train_ordered, lgb_valid_ordered], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_ordered.save_model(os.path.join(MODEL_DIR, \"model_ordered.txt\"))\n",
        "print(f\"âœ… 'ordered' modeli '{MODEL_DIR}/model_ordered.txt' olarak kaydedildi.\")\n",
        "\n",
        "# 6.2 'clicked' modeli\n",
        "print(\"\\n'clicked' modeli eÄŸitiliyor...\")\n",
        "clicked_counts = train_pd['clicked'].value_counts()\n",
        "params['scale_pos_weight'] = clicked_counts[0] / clicked_counts[1]\n",
        "model_clicked = lgb.train(params, lgb_train_clicked, valid_sets=[lgb_train_clicked, lgb_valid_clicked], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_clicked.save_model(os.path.join(MODEL_DIR, \"model_clicked.txt\"))\n",
        "print(f\"âœ… 'clicked' modeli '{MODEL_DIR}/model_clicked_v2.txt' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n--- Yeni Ã¶zelliklerle eÄŸitim ve kaydetme iÅŸlemi tamamlandÄ±! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh1v32Fvy2G5",
        "outputId": "7ce7a1fb-cab5-48c9-c19f-50b75098c627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 1: Model EÄŸitme (Yeni Zamansal Ã–zellikler ile) ---\n",
            "EÄŸitim verisi yÃ¼kleniyor: /content/Trendyol Veri/enriched_train_data_FINAL.parquet\n",
            "EÄŸitim verisi, zamana gÃ¶re sÄ±ralanÄ±p yÃ¼zdesel olarak ayrÄ±lÄ±yor...\n",
            "Yeni zamansal Ã¶zellikler Ã¼retiliyor: 'hour_of_day' ve 'day_of_week'...\n",
            "Train parÃ§asÄ± boyutu: (2357734, 39)\n",
            "Validation parÃ§asÄ± boyutu: (416071, 39)\n",
            "Veri setleri Pandas'a Ã§evriliyor ve model iÃ§in hazÄ±rlanÄ±yor...\n",
            "KullanÄ±lacak Ã¶zellik sayÄ±sÄ±: 25\n",
            "lgb.Dataset nesneleri oluÅŸturuluyor...\n",
            "\n",
            "'ordered' modeli eÄŸitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttrain's auc: 0.764107\tvalid's auc: 0.700363\n",
            "âœ… 'ordered' modeli 'models_2_doldur_HoursAndDay/model_ordered.txt' olarak kaydedildi.\n",
            "\n",
            "'clicked' modeli eÄŸitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[122]\ttrain's auc: 0.711014\tvalid's auc: 0.658447\n",
            "âœ… 'clicked' modeli 'models_2_doldur_HoursAndDay/model_clicked_v2.txt' olarak kaydedildi.\n",
            "\n",
            "--- Yeni Ã¶zelliklerle eÄŸitim ve kaydetme iÅŸlemi tamamlandÄ±! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BÃ¶lÃ¼m 2** Yeni eklenen Ã¶zellikleri test etme ve uygun submission oluÅŸturma"
      ],
      "metadata": {
        "id": "2Dy9hEYMz_iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "# trendyol_metric_group_auc.py dosyasÄ±nÄ±n Colab ortamÄ±nda olduÄŸundan emin olun\n",
        "from trendyol_metric_group_auc import score\n",
        "\n",
        "print(\"--- KOD 2: GÃ¼ncellenmiÅŸ Tahmin ve Skorlama SÃ¼reci ---\")\n",
        "\n",
        "# --- AdÄ±m 1: Gerekli Veri ve GÃœNCELLENMÄ°Å Modelleri YÃ¼kleme ---\n",
        "VAL_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "TEST_PATH = \"/content/Trendyol Veri/enriched_test_data_FINAL.parquet\"\n",
        "MODEL_DIR = \"models_2_doldur_HoursAndDay\"\n",
        "MODEL_ORDERED_PATH = os.path.join(MODEL_DIR, 'model_ordered.txt') # GÃœNCELLEME\n",
        "MODEL_CLICKED_PATH = os.path.join(MODEL_DIR, 'model_clicked.txt') # GÃœNCELLEME\n",
        "\n",
        "print(\"Veriler ve kaydedilmiÅŸ v2 modelleri yÃ¼kleniyor...\")\n",
        "val_full_df = pl.read_parquet(VAL_PATH)\n",
        "test_df_pl = pl.read_parquet(TEST_PATH)\n",
        "model_ordered = lgb.Booster(model_file=MODEL_ORDERED_PATH)\n",
        "model_clicked = lgb.Booster(model_file=MODEL_CLICKED_PATH)\n",
        "print(\"âœ… Veriler ve v2 modelleri baÅŸarÄ±yla yÃ¼klendi.\")\n",
        "\n",
        "# --- AdÄ±m 2: Validation Seti HazÄ±rlama (Yeni Ã–zellikler ile) ---\n",
        "print(\"\\nLokal skor iÃ§in validation seti hazÄ±rlanÄ±yor...\")\n",
        "if \"ts_hour\" in val_full_df.columns and val_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    val_full_df = val_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "\n",
        "# GÃœNCELLEME: EÄŸitimde eklediÄŸimiz zamansal Ã¶zellikleri buraya da ekliyoruz\n",
        "val_full_df = val_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "\n",
        "val_full_df = val_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(val_full_df) * 0.85)\n",
        "val_pd = val_full_df[split_index:].to_pandas().fillna(0)\n",
        "\n",
        "# Ã–zellik listesini eÄŸitimdekiyle tutarlÄ± olacak ÅŸekilde belirle\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features = [col for col in val_pd.columns if col not in exclude_cols]\n",
        "for col in features:\n",
        "    if val_pd[col].dtype == 'object':\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "val_pd['p_order'] = model_ordered.predict(val_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "val_pd['p_click'] = model_clicked.predict(val_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "val_pd['final_score'] = 0.7 * val_pd['p_order'] + 0.3 * val_pd['p_click']\n",
        "\n",
        "# Skor fonksiyonu iÃ§in gerekli DataFrame'leri oluÅŸtur\n",
        "val_solution = val_pd.groupby('session_id').agg(ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), all_items=('content_id_hashed', ' '.join)).reset_index()\n",
        "val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n",
        "val_submission.rename(columns={'content_id_hashed': 'prediction'}, inplace=True)\n",
        "\n",
        "try:\n",
        "    local_final_score = score(val_solution, val_submission, 'session_id')\n",
        "    print(\"\\n-------------------------------------------\")\n",
        "    print(f\"ğŸ† YENÄ° MODELLERLE LOKAL SKORUNUZ: {local_final_score:.5f}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "except Exception as e:\n",
        "    print(f\"Skor hesaplanÄ±rken bir hata oluÅŸtu: {e}\")\n",
        "\n",
        "# --- AdÄ±m 3: Kaggle iÃ§in Submission DosyasÄ± OluÅŸturma ---\n",
        "print(\"\\nTest verisi hazÄ±rlanÄ±yor ve Kaggle iÃ§in submission dosyasÄ± oluÅŸturuluyor...\")\n",
        "\n",
        "# GÃœNCELLEME: EÄŸitimde eklediÄŸimiz zamansal Ã¶zellikleri test setine de ekliyoruz\n",
        "if \"ts_hour\" in test_df_pl.columns and test_df_pl[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    test_df_pl = test_df_pl.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "test_df_pl = test_df_pl.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "\n",
        "test_pd = test_df_pl.to_pandas().fillna(0)\n",
        "for col in features:\n",
        "    if test_pd[col].dtype == 'object':\n",
        "        test_pd[col] = test_pd[col].astype('category')\n",
        "\n",
        "p_order_test = model_ordered.predict(test_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "p_click_test = model_clicked.predict(test_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "\n",
        "test_df_pl = test_df_pl.with_columns(\n",
        "    final_score=(0.7 * pl.Series(p_order_test)) + (0.3 * pl.Series(p_click_test))\n",
        ")\n",
        "\n",
        "submission_df = test_df_pl.sort([\"session_id\", \"final_score\"], descending=True).group_by(\"session_id\").agg(\n",
        "    pl.col(\"content_id_hashed\").alias(\"prediction\")\n",
        ").with_columns(\n",
        "    pl.col(\"prediction\").list.join(\" \")\n",
        ")\n",
        "\n",
        "# SaÄŸlama kontrolÃ¼\n",
        "expected_rows = 18589\n",
        "actual_rows = submission_df.shape[0]\n",
        "print(f\"\\nOluÅŸturulan submission satÄ±r sayÄ±sÄ±: {actual_rows} (Beklenen: {expected_rows})\")\n",
        "if actual_rows == expected_rows: print(\"âœ… SatÄ±r sayÄ±sÄ± doÄŸru.\")\n",
        "else: print(\"âŒ UYARI: SatÄ±r sayÄ±sÄ± yanlÄ±ÅŸ!\")\n",
        "\n",
        "submission_path = \"submission_final_v5.csv\" # Versiyonu gÃ¼ncelledim\n",
        "submission_df.write_csv(submission_path)\n",
        "print(f\"\\nSubmission dosyasÄ± '{submission_path}' olarak kaydedildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvNViBJz0FeI",
        "outputId": "06b4be04-cf87-4f4d-aa1b-152ef8c9808d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 2: GÃ¼ncellenmiÅŸ Tahmin ve Skorlama SÃ¼reci ---\n",
            "Veriler ve kaydedilmiÅŸ v2 modelleri yÃ¼kleniyor...\n",
            "âœ… Veriler ve v2 modelleri baÅŸarÄ±yla yÃ¼klendi.\n",
            "\n",
            "Lokal skor iÃ§in validation seti hazÄ±rlanÄ±yor...\n",
            "Ordered AUC:  0.6504508564295987\n",
            "Clicked AUC:  0.5918174552587637\n",
            "\n",
            "-------------------------------------------\n",
            "ğŸ† YENÄ° MODELLERLE LOKAL SKORUNUZ: 0.63286\n",
            "-------------------------------------------\n",
            "\n",
            "Test verisi hazÄ±rlanÄ±yor ve Kaggle iÃ§in submission dosyasÄ± oluÅŸturuluyor...\n",
            "\n",
            "OluÅŸturulan submission satÄ±r sayÄ±sÄ±: 18589 (Beklenen: 18589)\n",
            "âœ… SatÄ±r sayÄ±sÄ± doÄŸru.\n",
            "\n",
            "Submission dosyasÄ± 'submission_final_v5.csv' olarak kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------\n",
        "BaÅŸka Bir Deneme\n",
        "---------------------------------"
      ],
      "metadata": {
        "id": "Rd1DnckP4OkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"--- KOD 1: Nihai Model EÄŸitme ve Kaydetme SÃ¼reci ---\")\n",
        "\n",
        "# --- AdÄ±m 1: Veri YÃ¼kleme ---\n",
        "TRAIN_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "print(f\"EÄŸitim verisi yÃ¼kleniyor: {TRAIN_PATH}\")\n",
        "train_full_df = pl.read_parquet(TRAIN_PATH)\n",
        "print(f\"Toplam eÄŸitim verisi boyutu: {train_full_df.shape}\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 2: Ã–zellik MÃ¼hendisliÄŸi ve Veri AyÄ±rma ---\n",
        "print(\"Yeni zamansal Ã¶zellikler Ã¼retiliyor ve veri ayrÄ±lÄ±yor...\")\n",
        "if \"ts_hour\" in train_full_df.columns and train_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    train_full_df = train_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "\n",
        "# 2.1 Zamansal Ã–zellikleri Ãœretme\n",
        "train_full_df = train_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "\n",
        "# 2.2 Veriyi Zamana GÃ¶re Train/Validation Olarak AyÄ±rma\n",
        "train_full_df = train_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(train_full_df) * 0.85)\n",
        "train_df, val_df = train_full_df[:split_index], train_full_df[split_index:]\n",
        "\n",
        "print(f\"Train parÃ§asÄ± boyutu: {train_df.shape}\")\n",
        "print(f\"Validation parÃ§asÄ± boyutu: {val_df.shape}\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 3: Veri HazÄ±rlama ve Tip DÃ¶nÃ¼ÅŸÃ¼mÃ¼ ---\n",
        "print(\"Veri setleri Pandas'a Ã§evriliyor ve model iÃ§in hazÄ±rlanÄ±yor...\")\n",
        "train_pd = train_df.to_pandas().fillna(0)\n",
        "val_pd = val_df.to_pandas().fillna(0)\n",
        "\n",
        "# Kategorik olarak ele alÄ±nacak sÃ¼tunlarÄ± belirliyoruz\n",
        "string_cols = [col for col in train_pd.columns if train_pd[col].dtype == 'object']\n",
        "temporal_cols_as_cat = [\"hour_of_day\", \"day_of_week\"]\n",
        "cols_to_categorize = string_cols + temporal_cols_as_cat\n",
        "\n",
        "# Belirlenen sÃ¼tunlarÄ±n tipini 'category' olarak deÄŸiÅŸtiriyoruz\n",
        "for col in cols_to_categorize:\n",
        "    if col in train_pd.columns:\n",
        "        train_pd[col] = train_pd[col].astype('category')\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "# Modelde kullanÄ±lacak Ã¶zellikleri ve hedefleri belirliyoruz\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features = [col for col in train_pd.columns if col not in exclude_cols]\n",
        "cat_features = [col for col in features if isinstance(train_pd[col].dtype, pd.CategoricalDtype)]\n",
        "\n",
        "print(f\"KullanÄ±lacak Ã¶zellik sayÄ±sÄ±: {len(features)}\")\n",
        "print(f\"Kategorik olarak belirlenen Ã¶zellik sayÄ±sÄ±: {len(cat_features)}\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 4: lgb.Dataset OluÅŸturma ---\n",
        "print(\"lgb.Dataset nesneleri oluÅŸturuluyor...\")\n",
        "lgb_train_ordered = lgb.Dataset(train_pd[features], label=train_pd['ordered'], categorical_feature=cat_features)\n",
        "lgb_valid_ordered = lgb.Dataset(val_pd[features], label=val_pd['ordered'], categorical_feature=cat_features)\n",
        "\n",
        "lgb_train_clicked = lgb.Dataset(train_pd[features], label=train_pd['clicked'], categorical_feature=cat_features)\n",
        "lgb_valid_clicked = lgb.Dataset(val_pd[features], label=val_pd['clicked'], categorical_feature=cat_features)\n",
        "\n",
        "\n",
        "# --- AdÄ±m 5: Model EÄŸitimi ve Kaydetme ---\n",
        "params = {\n",
        "    \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\",\n",
        "    \"learning_rate\": 0.03, \"num_leaves\": 64, \"verbose\": -1, \"seed\": 42\n",
        "}\n",
        "callbacks = [lgb.early_stopping(100, verbose=True)]\n",
        "\n",
        "MODEL_DIR = \"models_3_doldur_HoursAndDayCategory\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# 5.1 'ordered' modeli\n",
        "print(\"\\n'ordered' modeli eÄŸitiliyor...\")\n",
        "ordered_counts = train_pd['ordered'].value_counts()\n",
        "params['scale_pos_weight'] = ordered_counts[0] / ordered_counts[1]\n",
        "model_ordered = lgb.train(params, lgb_train_ordered, valid_sets=[lgb_train_ordered, lgb_valid_ordered], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_ordered.save_model(os.path.join(MODEL_DIR, \"model_ordered.txt\"))\n",
        "print(f\"âœ… 'ordered' modeli '{MODEL_DIR}/model_ordered.txt' olarak kaydedildi.\")\n",
        "\n",
        "# 5.2 'clicked' modeli\n",
        "print(\"\\n'clicked' modeli eÄŸitiliyor...\")\n",
        "clicked_counts = train_pd['clicked'].value_counts()\n",
        "params['scale_pos_weight'] = clicked_counts[0] / clicked_counts[1]\n",
        "model_clicked = lgb.train(params, lgb_train_clicked, valid_sets=[lgb_train_clicked, lgb_valid_clicked], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_clicked.save_model(os.path.join(MODEL_DIR, \"model_clicked.txt\"))\n",
        "print(f\"âœ… 'clicked' modeli '{MODEL_DIR}/model_clicked.txt' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n--- EÄŸitim ve Kaydetme Ä°ÅŸlemi TamamlandÄ±! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxEsw1yZ4P4G",
        "outputId": "6ec78fd6-8943-49fa-ccc3-3a21ac1a22d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 1: Nihai Model EÄŸitme ve Kaydetme SÃ¼reci ---\n",
            "EÄŸitim verisi yÃ¼kleniyor: /content/Trendyol Veri/enriched_train_data_FINAL.parquet\n",
            "Toplam eÄŸitim verisi boyutu: (2773805, 37)\n",
            "Yeni zamansal Ã¶zellikler Ã¼retiliyor ve veri ayrÄ±lÄ±yor...\n",
            "Train parÃ§asÄ± boyutu: (2357734, 39)\n",
            "Validation parÃ§asÄ± boyutu: (416071, 39)\n",
            "Veri setleri Pandas'a Ã§evriliyor ve model iÃ§in hazÄ±rlanÄ±yor...\n",
            "KullanÄ±lacak Ã¶zellik sayÄ±sÄ±: 25\n",
            "Kategorik olarak belirlenen Ã¶zellik sayÄ±sÄ±: 4\n",
            "lgb.Dataset nesneleri oluÅŸturuluyor...\n",
            "\n",
            "'ordered' modeli eÄŸitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttrain's auc: 0.764107\tvalid's auc: 0.700363\n",
            "âœ… 'ordered' modeli 'models_3_doldur_HoursAndDayCategory/model_ordered.txt' olarak kaydedildi.\n",
            "\n",
            "'clicked' modeli eÄŸitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[127]\ttrain's auc: 0.712264\tvalid's auc: 0.659234\n",
            "âœ… 'clicked' modeli 'models_3_doldur_HoursAndDayCategory/model_clicked.txt' olarak kaydedildi.\n",
            "\n",
            "--- EÄŸitim ve Kaydetme Ä°ÅŸlemi TamamlandÄ±! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "# trendyol_metric_group_auc.py dosyasÄ±nÄ±n Colab ortamÄ±nda olduÄŸundan emin olun\n",
        "from trendyol_metric_group_auc import score\n",
        "\n",
        "print(\"--- KOD 2: Nihai Tahmin ve Skorlama SÃ¼reci ---\")\n",
        "\n",
        "# --- AdÄ±m 1: Gerekli Veri ve KaydedilmiÅŸ Modelleri YÃ¼kleme ---\n",
        "VAL_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "TEST_PATH = \"/content/Trendyol Veri/enriched_test_data_FINAL.parquet\"\n",
        "MODEL_DIR = \"models_3_doldur_HoursAndDayCategory\"\n",
        "MODEL_ORDERED_PATH = os.path.join(MODEL_DIR, 'model_ordered.txt')\n",
        "MODEL_CLICKED_PATH = os.path.join(MODEL_DIR, 'model_clicked.txt')\n",
        "\n",
        "print(\"Veriler ve kaydedilmiÅŸ modeller yÃ¼kleniyor...\")\n",
        "val_full_df = pl.read_parquet(VAL_PATH)\n",
        "test_df_pl = pl.read_parquet(TEST_PATH)\n",
        "model_ordered = lgb.Booster(model_file=MODEL_ORDERED_PATH)\n",
        "model_clicked = lgb.Booster(model_file=MODEL_CLICKED_PATH)\n",
        "print(\"âœ… Veriler ve modeller baÅŸarÄ±yla yÃ¼klendi.\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 2: Lokal Skor Ä°Ã§in Validation Seti HazÄ±rlama ---\n",
        "print(\"\\nLokal skor iÃ§in validation seti hazÄ±rlanÄ±yor...\")\n",
        "if \"ts_hour\" in val_full_df.columns and val_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    val_full_df = val_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "\n",
        "# Yeni zamansal Ã¶zellikleri ekle\n",
        "val_full_df = val_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "\n",
        "val_full_df = val_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(val_full_df) * 0.85)\n",
        "val_pd = val_full_df[split_index:].to_pandas().fillna(0)\n",
        "\n",
        "# Ã–zellik listesini ve kategorik tipleri eÄŸitimdekiyle tutarlÄ± yap\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features = [col for col in val_pd.columns if col not in exclude_cols]\n",
        "\n",
        "string_cols = [col for col in val_pd.columns if val_pd[col].dtype == 'object']\n",
        "temporal_cols_as_cat = [\"hour_of_day\", \"day_of_week\"]\n",
        "cols_to_categorize = string_cols + temporal_cols_as_cat\n",
        "for col in cols_to_categorize:\n",
        "    if col in val_pd.columns:\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "# Lokal skor iÃ§in tahmin Ã¼ret\n",
        "val_pd['p_order'] = model_ordered.predict(val_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "val_pd['p_click'] = model_clicked.predict(val_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "val_pd['final_score'] = 0.7 * val_pd['p_order'] + 0.3 * val_pd['p_click']\n",
        "\n",
        "# Skor fonksiyonu iÃ§in gerekli DataFrame'leri oluÅŸtur\n",
        "val_solution = val_pd.groupby('session_id').agg(ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), all_items=('content_id_hashed', ' '.join)).reset_index()\n",
        "val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n",
        "val_submission.rename(columns={'content_id_hashed': 'prediction'}, inplace=True)\n",
        "\n",
        "try:\n",
        "    local_final_score = score(val_solution, val_submission, 'session_id')\n",
        "    print(\"\\n-------------------------------------------\")\n",
        "    print(f\"ğŸ† LOKAL SKORUNUZ: {local_final_score:.5f}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "except Exception as e:\n",
        "    print(f\"Skor hesaplanÄ±rken bir hata oluÅŸtu: {e}\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 3: Kaggle iÃ§in Submission DosyasÄ± OluÅŸturma ---\n",
        "print(\"\\nTest verisi hazÄ±rlanÄ±yor ve Kaggle iÃ§in submission dosyasÄ± oluÅŸturuluyor...\")\n",
        "if \"ts_hour\" in test_df_pl.columns and test_df_pl[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    test_df_pl = test_df_pl.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "test_df_pl = test_df_pl.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "test_pd = test_df_pl.to_pandas().fillna(0)\n",
        "for col in cols_to_categorize:\n",
        "    if col in test_pd.columns:\n",
        "        test_pd[col] = test_pd[col].astype('category')\n",
        "\n",
        "p_order_test = model_ordered.predict(test_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "p_click_test = model_clicked.predict(test_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "\n",
        "test_df_pl = test_df_pl.with_columns(\n",
        "    final_score=(0.7 * pl.Series(p_order_test)) + (0.3 * pl.Series(p_click_test))\n",
        ")\n",
        "submission_df = test_df_pl.sort([\"session_id\", \"final_score\"], descending=True).group_by(\"session_id\").agg(\n",
        "    pl.col(\"content_id_hashed\").alias(\"prediction\")\n",
        ").with_columns(\n",
        "    pl.col(\"prediction\").list.join(\" \")\n",
        ")\n",
        "\n",
        "# SaÄŸlama kontrolÃ¼\n",
        "expected_rows = 18589\n",
        "actual_rows = submission_df.shape[0]\n",
        "print(f\"\\nOluÅŸturulan submission satÄ±r sayÄ±sÄ±: {actual_rows} (Beklenen: {expected_rows})\")\n",
        "if actual_rows == expected_rows: print(\"âœ… SatÄ±r sayÄ±sÄ± doÄŸru.\")\n",
        "else: print(\"âŒ UYARI: SatÄ±r sayÄ±sÄ± yanlÄ±ÅŸ!\")\n",
        "\n",
        "submission_path = \"submission.csv\"\n",
        "submission_df.write_csv(submission_path)\n",
        "print(f\"\\nSubmission dosyasÄ± '{submission_path}' olarak kaydedildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo8_cTcf4w8B",
        "outputId": "db9e884f-585c-49ab-9405-6cb6c4e04b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 2: Nihai Tahmin ve Skorlama SÃ¼reci ---\n",
            "Veriler ve kaydedilmiÅŸ modeller yÃ¼kleniyor...\n",
            "âœ… Veriler ve modeller baÅŸarÄ±yla yÃ¼klendi.\n",
            "\n",
            "Lokal skor iÃ§in validation seti hazÄ±rlanÄ±yor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3194590946.py:59: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  val_solution = val_pd.groupby('session_id').agg(ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), all_items=('content_id_hashed', ' '.join)).reset_index()\n",
            "/tmp/ipython-input-3194590946.py:60: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ordered AUC:  0.6504243273845817\n",
            "Clicked AUC:  0.5914654200400459\n",
            "\n",
            "-------------------------------------------\n",
            "ğŸ† LOKAL SKORUNUZ: 0.63274\n",
            "-------------------------------------------\n",
            "\n",
            "Test verisi hazÄ±rlanÄ±yor ve Kaggle iÃ§in submission dosyasÄ± oluÅŸturuluyor...\n",
            "\n",
            "OluÅŸturulan submission satÄ±r sayÄ±sÄ±: 18589 (Beklenen: 18589)\n",
            "âœ… SatÄ±r sayÄ±sÄ± doÄŸru.\n",
            "\n",
            "Submission dosyasÄ± 'submission.csv' olarak kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deneme 3 Fiyat UygunluÄŸu"
      ],
      "metadata": {
        "id": "oKKmsWikDSpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 1** Fiyat uygunluÄŸuna gÃ¶re eÄŸitim"
      ],
      "metadata": {
        "id": "P7fQFd5TDZFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"--- KOD 1: Model EÄŸitme (AyrÄ± Ã–zellik Setleriyle) ---\")\n",
        "\n",
        "# --- AdÄ±m 1: Veri YÃ¼kleme ---\n",
        "TRAIN_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "train_full_df = pl.read_parquet(TRAIN_PATH)\n",
        "print(f\"Toplam eÄŸitim verisi boyutu: {train_full_df.shape}\")\n",
        "\n",
        "# --- AdÄ±m 2: Ã–n Ã–zellikler ve Train/Validation AyrÄ±mÄ± ---\n",
        "print(\"Ã–n Ã¶zellikler Ã¼retiliyor ve veri zamana gÃ¶re ayrÄ±lÄ±yor...\")\n",
        "if \"ts_hour\" in train_full_df.columns and train_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    train_full_df = train_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "train_full_df = train_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "train_full_df = train_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(train_full_df) * 0.85)\n",
        "train_df, val_df = train_full_df[:split_index], train_full_df[split_index:]\n",
        "\n",
        "# --- AdÄ±m 3: EtkileÅŸim Ã–zelliÄŸini SADECE Train Setinden Hesaplama ---\n",
        "print(\"Fiyat etkileÅŸim Ã¶zelliÄŸi, SADECE train verisi Ã¼zerinden hesaplanÄ±yor...\")\n",
        "price_features_lazy = pl.scan_parquet(TRAIN_PATH).select([\"content_id_hashed\", \"selling_price\", \"update_date\"])\n",
        "latest_price_features = price_features_lazy.sort(\"update_date\", descending=True).group_by(\"content_id_hashed\").first().collect()\n",
        "user_orders = train_df.filter(pl.col(\"ordered\") == 1).select([\"user_id_hashed\", \"content_id_hashed\"])\n",
        "user_orders_with_price = user_orders.join(latest_price_features, on=\"content_id_hashed\", how=\"left\")\n",
        "agg_user_price_features = user_orders_with_price.group_by(\"user_id_hashed\").agg(pl.mean(\"selling_price\").alias(\"user_avg_order_price\"))\n",
        "FEATURES_DIR = \"features\"\n",
        "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
        "agg_user_price_features.write_parquet(os.path.join(FEATURES_DIR, \"user_price_features_final.parquet\"))\n",
        "print(f\"âœ… SÄ±zÄ±ntÄ±sÄ±z kullanÄ±cÄ± fiyat Ã¶zellikleri kaydedildi.\")\n",
        "\n",
        "# --- AdÄ±m 4: Ã–zellikleri Ekleme ve AkÄ±llÄ± BoÅŸluk Doldurma ---\n",
        "global_avg_order_price = agg_user_price_features.select(pl.mean(\"user_avg_order_price\")).item()\n",
        "train_df = train_df.join(agg_user_price_features, on=\"user_id_hashed\", how=\"left\").with_columns(pl.col(\"user_avg_order_price\").fill_null(global_avg_order_price)).with_columns((pl.col(\"selling_price\") / pl.col(\"user_avg_order_price\")).alias(\"price_vs_user_avg_ratio\"))\n",
        "val_df = val_df.join(agg_user_price_features, on=\"user_id_hashed\", how=\"left\").with_columns(pl.col(\"user_avg_order_price\").fill_null(global_avg_order_price)).with_columns((pl.col(\"selling_price\") / pl.col(\"user_avg_order_price\")).alias(\"price_vs_user_avg_ratio\"))\n",
        "\n",
        "# --- AdÄ±m 5: Veri HazÄ±rlama ve AYRI Ã–ZELLÄ°K SETLERÄ° OLUÅTURMA ---\n",
        "print(\"Veri setleri Pandas'a Ã§evriliyor ve ayrÄ± Ã¶zellik setleri oluÅŸturuluyor...\")\n",
        "train_pd = train_df.to_pandas().fillna(0)\n",
        "val_pd = val_df.to_pandas().fillna(0)\n",
        "string_cols = [col for col in train_pd.columns if train_pd[col].dtype == 'object']\n",
        "temporal_cols_as_cat = [\"hour_of_day\", \"day_of_week\"]\n",
        "cols_to_categorize = string_cols + temporal_cols_as_cat\n",
        "for col in cols_to_categorize:\n",
        "    if col in train_pd.columns:\n",
        "        train_pd[col], val_pd[col] = train_pd[col].astype('category'), val_pd[col].astype('category')\n",
        "\n",
        "# 5.1 Temel Ã–zellik Listesi\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features_base = [col for col in train_pd.columns if col not in exclude_cols]\n",
        "\n",
        "# 5.2 Modele Ã–zel Ã–zellik Listeleri\n",
        "price_features_to_exclude = [\"user_avg_order_price\", \"price_vs_user_avg_ratio\"]\n",
        "features_for_ordered = [f for f in features_base if f not in price_features_to_exclude]\n",
        "features_for_clicked = features_base # clicked modeli tÃ¼m Ã¶zellikleri kullanabilir\n",
        "\n",
        "cat_features_ordered = [f for f in features_for_ordered if isinstance(train_pd[f].dtype, pd.CategoricalDtype)]\n",
        "cat_features_clicked = [f for f in features_for_clicked if isinstance(train_pd[f].dtype, pd.CategoricalDtype)]\n",
        "\n",
        "print(f\"'ordered' modeli iÃ§in kullanÄ±lacak Ã¶zellik sayÄ±sÄ±: {len(features_for_ordered)}\")\n",
        "print(f\"'clicked' modeli iÃ§in kullanÄ±lacak Ã¶zellik sayÄ±sÄ±: {len(features_for_clicked)}\")\n",
        "\n",
        "# --- AdÄ±m 6: lgb.Dataset OluÅŸturma (AyrÄ± AyrÄ±) ---\n",
        "print(\"Modele Ã¶zel lgb.Dataset nesneleri oluÅŸturuluyor...\")\n",
        "lgb_train_ordered = lgb.Dataset(train_pd[features_for_ordered], label=train_pd['ordered'], categorical_feature=cat_features_ordered)\n",
        "lgb_valid_ordered = lgb.Dataset(val_pd[features_for_ordered], label=val_pd['ordered'], categorical_feature=cat_features_ordered)\n",
        "\n",
        "lgb_train_clicked = lgb.Dataset(train_pd[features_for_clicked], label=train_pd['clicked'], categorical_feature=cat_features_clicked)\n",
        "lgb_valid_clicked = lgb.Dataset(val_pd[features_for_clicked], label=val_pd['clicked'], categorical_feature=cat_features_clicked)\n",
        "\n",
        "# --- AdÄ±m 7: Model EÄŸitimi ve Kaydetme ---\n",
        "params = { \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\", \"learning_rate\": 0.03, \"num_leaves\": 64, \"verbose\": -1, \"seed\": 42 }\n",
        "callbacks = [lgb.early_stopping(100, verbose=True)]\n",
        "MODEL_DIR = \"models_5_doldur_FiyatUygunlukAyrÄ±\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(\"\\n'ordered' modeli (sÄ±nÄ±rlÄ± Ã¶zelliklerle) eÄŸitiliyor...\")\n",
        "ordered_counts = train_pd['ordered'].value_counts()\n",
        "params['scale_pos_weight'] = ordered_counts[0] / ordered_counts[1]\n",
        "model_ordered = lgb.train(params, lgb_train_ordered, valid_sets=[lgb_train_ordered, lgb_valid_ordered], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_ordered.save_model(os.path.join(MODEL_DIR, \"model_ordered_final_split.txt\"))\n",
        "print(f\"âœ… 'ordered' modeli '{MODEL_DIR}/model_ordered_final_split.txt' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n'clicked' modeli (tÃ¼m Ã¶zelliklerle) eÄŸitiliyor...\")\n",
        "clicked_counts = train_pd['clicked'].value_counts()\n",
        "params['scale_pos_weight'] = clicked_counts[0] / clicked_counts[1]\n",
        "model_clicked = lgb.train(params, lgb_train_clicked, valid_sets=[lgb_train_clicked, lgb_valid_clicked], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_clicked.save_model(os.path.join(MODEL_DIR, \"model_clicked_final_split.txt\"))\n",
        "print(f\"âœ… 'clicked' modeli '{MODEL_DIR}/model_clicked_final_split.txt' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n--- EÄŸitim ve Kaydetme Ä°ÅŸlemi TamamlandÄ±! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLh0sqmnDe1E",
        "outputId": "983e8584-36be-4921-f345-44e06fa2e6cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 1: Model EÄŸitme (AyrÄ± Ã–zellik Setleriyle) ---\n",
            "Toplam eÄŸitim verisi boyutu: (2773805, 37)\n",
            "Ã–n Ã¶zellikler Ã¼retiliyor ve veri zamana gÃ¶re ayrÄ±lÄ±yor...\n",
            "Fiyat etkileÅŸim Ã¶zelliÄŸi, SADECE train verisi Ã¼zerinden hesaplanÄ±yor...\n",
            "âœ… SÄ±zÄ±ntÄ±sÄ±z kullanÄ±cÄ± fiyat Ã¶zellikleri kaydedildi.\n",
            "Veri setleri Pandas'a Ã§evriliyor ve ayrÄ± Ã¶zellik setleri oluÅŸturuluyor...\n",
            "'ordered' modeli iÃ§in kullanÄ±lacak Ã¶zellik sayÄ±sÄ±: 25\n",
            "'clicked' modeli iÃ§in kullanÄ±lacak Ã¶zellik sayÄ±sÄ±: 27\n",
            "Modele Ã¶zel lgb.Dataset nesneleri oluÅŸturuluyor...\n",
            "\n",
            "'ordered' modeli (sÄ±nÄ±rlÄ± Ã¶zelliklerle) eÄŸitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttrain's auc: 0.764107\tvalid's auc: 0.700363\n",
            "âœ… 'ordered' modeli 'models_5_doldur_FiyatUygunlukAyrÄ±/model_ordered_final_split.txt' olarak kaydedildi.\n",
            "\n",
            "'clicked' modeli (tÃ¼m Ã¶zelliklerle) eÄŸitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[122]\ttrain's auc: 0.732438\tvalid's auc: 0.65745\n",
            "âœ… 'clicked' modeli 'models_5_doldur_FiyatUygunlukAyrÄ±/model_clicked_final_split.txt' olarak kaydedildi.\n",
            "\n",
            "--- EÄŸitim ve Kaydetme Ä°ÅŸlemi TamamlandÄ±! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 2** Fiyat uygunluÄŸuna gÃ¶re test ve submission oluÅŸturma"
      ],
      "metadata": {
        "id": "dqQDTPYgELLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "from trendyol_metric_group_auc import score\n",
        "\n",
        "print(\"--- KOD 2: Nihai Tahmin ve Skorlama (AyrÄ± Ã–zellik Setleriyle) ---\")\n",
        "\n",
        "# --- AdÄ±m 1: Gerekli DosyalarÄ± YÃ¼kleme ---\n",
        "VAL_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "TEST_PATH = \"/content/Trendyol Veri/enriched_test_data_FINAL.parquet\"\n",
        "FEATURES_DIR = \"features\"\n",
        "MODELS_DIR = \"models_5_doldur_FiyatUygunlukAyrÄ±\"\n",
        "USER_PRICE_FEATURES_PATH = os.path.join(FEATURES_DIR, \"user_price_features_final.parquet\")\n",
        "MODEL_ORDERED_PATH = os.path.join(MODELS_DIR, 'model_ordered_final_split.txt')\n",
        "MODEL_CLICKED_PATH = os.path.join(MODELS_DIR, 'model_clicked_final_split.txt')\n",
        "\n",
        "print(\"Veriler, Ã¶zellikler ve kaydedilmiÅŸ nihai modeller yÃ¼kleniyor...\")\n",
        "val_full_df = pl.read_parquet(VAL_PATH)\n",
        "test_df_pl = pl.read_parquet(TEST_PATH)\n",
        "user_price_features = pl.read_parquet(USER_PRICE_FEATURES_PATH)\n",
        "model_ordered = lgb.Booster(model_file=MODEL_ORDERED_PATH)\n",
        "model_clicked = lgb.Booster(model_file=MODEL_CLICKED_PATH)\n",
        "print(\"âœ… YÃ¼kleme tamamlandÄ±.\")\n",
        "\n",
        "# --- AdÄ±m 2: Lokal Skor Ä°Ã§in Validation Seti HazÄ±rlama ---\n",
        "print(\"\\nLokal skor iÃ§in validation seti hazÄ±rlanÄ±yor...\")\n",
        "if \"ts_hour\" in val_full_df.columns and val_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    val_full_df = val_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "val_full_df = val_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "global_avg_order_price = user_price_features.select(pl.mean(\"user_avg_order_price\")).item()\n",
        "val_full_df = val_full_df.join(user_price_features, on=\"user_id_hashed\", how=\"left\").with_columns(pl.col(\"user_avg_order_price\").fill_null(global_avg_order_price)).with_columns((pl.col(\"selling_price\") / pl.col(\"user_avg_order_price\")).alias(\"price_vs_user_avg_ratio\"))\n",
        "\n",
        "val_full_df = val_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(val_full_df) * 0.85)\n",
        "val_pd = val_full_df[split_index:].to_pandas().fillna(0)\n",
        "\n",
        "# AyrÄ± Ã¶zellik setlerini burada da tanÄ±mla\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features_base = [col for col in val_pd.columns if col not in exclude_cols]\n",
        "price_features_to_exclude = [\"user_avg_order_price\", \"price_vs_user_avg_ratio\"]\n",
        "features_for_ordered = [f for f in features_base if f not in price_features_to_exclude]\n",
        "features_for_clicked = features_base\n",
        "string_cols = [col for col in val_pd.columns if val_pd[col].dtype == 'object']\n",
        "temporal_cols_as_cat = [\"hour_of_day\", \"day_of_week\"]\n",
        "cols_to_categorize = string_cols + temporal_cols_as_cat\n",
        "for col in cols_to_categorize:\n",
        "    if col in val_pd.columns:\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "# Tahmin yaparken doÄŸru Ã¶zellik setini kullan\n",
        "val_pd['p_order'] = model_ordered.predict(val_pd[features_for_ordered], num_iteration=model_ordered.best_iteration)\n",
        "val_pd['p_click'] = model_clicked.predict(val_pd[features_for_clicked], num_iteration=model_clicked.best_iteration)\n",
        "val_pd['final_score'] = 0.7 * val_pd['p_order'] + 0.3 * val_pd['p_click']\n",
        "\n",
        "# Skorlama ve submission (bu kÄ±sÄ±mlar aynÄ±)\n",
        "val_solution = val_pd.groupby('session_id').agg(ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), all_items=('content_id_hashed', ' '.join)).reset_index()\n",
        "val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n",
        "val_submission.rename(columns={'content_id_hashed': 'prediction'}, inplace=True)\n",
        "try:\n",
        "    local_final_score = score(val_solution, val_submission, 'session_id')\n",
        "    print(\"\\n-------------------------------------------\")\n",
        "    print(f\"ğŸ† LOKAL SKORUNUZ (AyrÄ± Ã–zelliklerle): {local_final_score:.5f}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "except Exception as e:\n",
        "    print(f\"Skor hesaplanÄ±rken bir hata oluÅŸtu: {e}\")\n",
        "\n",
        "# --- AdÄ±m 3: Kaggle iÃ§in Submission DosyasÄ± OluÅŸturma ---\n",
        "print(\"\\nTest verisi hazÄ±rlanÄ±yor ve Kaggle iÃ§in submission dosyasÄ± oluÅŸturuluyor...\")\n",
        "if \"ts_hour\" in test_df_pl.columns and test_df_pl[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    test_df_pl = test_df_pl.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "test_df_pl = test_df_pl.with_columns(pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"), pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\"))\n",
        "test_df_pl = test_df_pl.join(user_price_features, on=\"user_id_hashed\", how=\"left\").with_columns(pl.col(\"user_avg_order_price\").fill_null(global_avg_order_price)).with_columns((pl.col(\"selling_price\") / pl.col(\"user_avg_order_price\")).alias(\"price_vs_user_avg_ratio\"))\n",
        "test_pd = test_df_pl.to_pandas().fillna(0)\n",
        "for col in cols_to_categorize:\n",
        "    if col in test_pd.columns:\n",
        "        test_pd[col] = test_pd[col].astype('category')\n",
        "\n",
        "# Tahmin yaparken doÄŸru Ã¶zellik setini kullan\n",
        "p_order_test = model_ordered.predict(test_pd[features_for_ordered], num_iteration=model_ordered.best_iteration)\n",
        "p_click_test = model_clicked.predict(test_pd[features_for_clicked], num_iteration=model_clicked.best_iteration)\n",
        "\n",
        "test_df_pl = test_df_pl.with_columns(final_score=(0.7 * pl.Series(p_order_test)) + (0.3 * pl.Series(p_click_test)))\n",
        "submission_df = test_df_pl.sort([\"session_id\", \"final_score\"], descending=True).group_by(\"session_id\").agg(pl.col(\"content_id_hashed\").alias(\"prediction\")).with_columns(pl.col(\"prediction\").list.join(\" \"))\n",
        "expected_rows = 18589\n",
        "actual_rows = submission_df.shape[0]\n",
        "print(f\"\\nOluÅŸturulan submission satÄ±r sayÄ±sÄ±: {actual_rows} (Beklenen: {expected_rows})\")\n",
        "if actual_rows == expected_rows: print(\"âœ… SatÄ±r sayÄ±sÄ± doÄŸru.\")\n",
        "else: print(\"âŒ UYARI: SatÄ±r sayÄ±sÄ± yanlÄ±ÅŸ!\")\n",
        "submission_path = \"submission.csv\"\n",
        "submission_df.write_csv(submission_path)\n",
        "print(f\"\\nSubmission dosyasÄ± '{submission_path}' olarak kaydedildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6wBMTgTEWX7",
        "outputId": "fb32210b-1b95-4492-aec3-5c56fe6cc813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 2: Nihai Tahmin ve Skorlama (AyrÄ± Ã–zellik Setleriyle) ---\n",
            "Veriler, Ã¶zellikler ve kaydedilmiÅŸ nihai modeller yÃ¼kleniyor...\n",
            "âœ… YÃ¼kleme tamamlandÄ±.\n",
            "\n",
            "Lokal skor iÃ§in validation seti hazÄ±rlanÄ±yor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1547542627.py:62: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  val_solution = val_pd.groupby('session_id').agg(ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), all_items=('content_id_hashed', ' '.join)).reset_index()\n",
            "/tmp/ipython-input-1547542627.py:63: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ordered AUC:  0.6501352808701953\n",
            "Clicked AUC:  0.5901990785840607\n",
            "\n",
            "-------------------------------------------\n",
            "ğŸ† LOKAL SKORUNUZ (AyrÄ± Ã–zelliklerle): 0.63215\n",
            "-------------------------------------------\n",
            "\n",
            "Test verisi hazÄ±rlanÄ±yor ve Kaggle iÃ§in submission dosyasÄ± oluÅŸturuluyor...\n",
            "\n",
            "OluÅŸturulan submission satÄ±r sayÄ±sÄ±: 18589 (Beklenen: 18589)\n",
            "âœ… SatÄ±r sayÄ±sÄ± doÄŸru.\n",
            "\n",
            "Submission dosyasÄ± 'submission.csv' olarak kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data SÄ±zÄ±ntÄ±sÄ±nÄ± Engelleyerek Yeni EÄŸitim**"
      ],
      "metadata": {
        "id": "Bfg61JZuInox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"--- KOD 1: Model EÄŸitme (Fiyat Ã–zellikleri Ã‡IKARILDI, Zamansal Ã–zellikler KALDI) ---\")\n",
        "\n",
        "# --- AdÄ±m 1: Veri YÃ¼kleme ---\n",
        "TRAIN_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "print(f\"EÄŸitim verisi yÃ¼kleniyor: {TRAIN_PATH}\")\n",
        "train_full_df = pl.read_parquet(TRAIN_PATH)\n",
        "print(f\"Toplam eÄŸitim verisi boyutu: {train_full_df.shape}\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 2: Ã–n Ã–zellikler (Zamansal) ve Train/Validation AyrÄ±mÄ± ---\n",
        "print(\"Zamansal Ã¶zellikler Ã¼retiliyor ve veri zamana gÃ¶re ayrÄ±lÄ±yor...\")\n",
        "if \"ts_hour\" in train_full_df.columns and train_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    train_full_df = train_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "\n",
        "train_full_df = train_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "\n",
        "train_full_df = train_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(train_full_df) * 0.85)\n",
        "train_df, val_df = train_full_df[:split_index], train_full_df[split_index:]\n",
        "print(f\"Train parÃ§asÄ± boyutu: {train_df.shape}\")\n",
        "print(f\"Validation parÃ§asÄ± boyutu: {val_df.shape}\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 3: Fiyat EtkileÅŸim Ã–zellikleri Ã‡IKARILDI ---\n",
        "# Bu adÄ±mda artÄ±k kullanÄ±cÄ± ortalama fiyatÄ± hesaplanmÄ±yor ve birleÅŸtirilmiyor.\n",
        "print(\"âœ… Fiyat etkileÅŸim Ã¶zellikleri bu denemede kullanÄ±lmayacaktÄ±r.\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 4: Veri HazÄ±rlama ve Tip DÃ¶nÃ¼ÅŸÃ¼mÃ¼ ---\n",
        "print(\"Veri setleri Pandas'a Ã§evriliyor ve model iÃ§in hazÄ±rlanÄ±yor...\")\n",
        "\n",
        "train_pd = train_df.to_pandas()\n",
        "val_pd = val_df.to_pandas()\n",
        "\n",
        "# SayÄ±sal sÃ¼tunlardaki NaN/Inf deÄŸerlerini yÃ¶netme (artÄ±k fiyat oranlarÄ± yok, genel fillna(0) yapabiliriz)\n",
        "numeric_cols = train_pd.select_dtypes(include=np.number).columns.tolist()\n",
        "for col in numeric_cols:\n",
        "    # Sonsuz deÄŸerleri NaN yapÄ±p sonra NaN'larÄ± 0 ile dolduralÄ±m\n",
        "    train_pd[col] = train_pd[col].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "    val_pd[col] = val_pd[col].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "\n",
        "string_cols = [col for col in train_pd.columns if train_pd[col].dtype == 'object']\n",
        "temporal_cols_as_cat = [\"hour_of_day\", \"day_of_week\"]\n",
        "cols_to_categorize = string_cols + temporal_cols_as_cat\n",
        "for col in cols_to_categorize:\n",
        "    if col in train_pd.columns:\n",
        "        train_pd[col] = train_pd[col].astype('category')\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "# DÃ¼zeltme: exclude_cols listesinden price_vs_user_avg_ratio ve user_avg_order_price Ã§Ä±karÄ±ldÄ±\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\", \"price_vs_user_avg_ratio\", \"user_avg_order_price\"]\n",
        "features = [col for col in train_pd.columns if col not in exclude_cols]\n",
        "cat_features = [col for col in features if train_pd[col].dtype.name == 'category']\n",
        "\n",
        "print(f\"KullanÄ±lacak Ã¶zellik sayÄ±sÄ±: {len(features)}\")\n",
        "print(f\"GÃ¼ncel Kategorik Ã¶zellik sayÄ±sÄ±: {len(cat_features)}\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 5: lgb.Dataset OluÅŸturma ---\n",
        "print(\"lgb.Dataset nesneleri oluÅŸturuluyor...\")\n",
        "# Categorical features listesini yeniden kontrol edelim\n",
        "cat_features = [col for col in features if train_pd[col].dtype.name == 'category']\n",
        "print(f\"Son Kategorik Ã¶zellik sayÄ±sÄ±: {len(cat_features)}\")\n",
        "\n",
        "lgb_train_ordered = lgb.Dataset(train_pd[features], label=train_pd['ordered'], categorical_feature=cat_features if cat_features else None)\n",
        "lgb_valid_ordered = lgb.Dataset(val_pd[features], label=val_pd['ordered'], categorical_feature=cat_features if cat_features else None)\n",
        "\n",
        "lgb_train_clicked = lgb.Dataset(train_pd[features], label=train_pd['clicked'], categorical_feature=cat_features if cat_features else None)\n",
        "lgb_valid_clicked = lgb.Dataset(val_pd[features], label=val_pd['clicked'], categorical_feature=cat_features if cat_features else None)\n",
        "\n",
        "\n",
        "# --- AdÄ±m 6: Model EÄŸitimi ve Kaydetme ---\n",
        "params = { \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\", \"learning_rate\": 0.03, \"num_leaves\": 64, \"verbose\": -1, \"seed\": 42 }\n",
        "callbacks = [lgb.early_stopping(100, verbose=True)]\n",
        "MODEL_DIR = \"models_4_doldur_FiyatUygunluk\" # KlasÃ¶r adÄ±nÄ± deÄŸiÅŸtirmedim, Ã¼zerine yazacak\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(\"\\n'ordered' modeli eÄŸitiliyor...\")\n",
        "ordered_counts = train_pd['ordered'].value_counts()\n",
        "params['scale_pos_weight'] = ordered_counts[0] / ordered_counts[1]\n",
        "model_ordered = lgb.train(params, lgb_train_ordered, valid_sets=[lgb_train_ordered, lgb_valid_ordered], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_ordered_path = os.path.join(MODEL_DIR, \"model_ordered_v6_no_price_features.txt\") # Dosya adÄ±nÄ± gÃ¼ncelledim\n",
        "model_ordered.save_model(model_ordered_path)\n",
        "print(f\"âœ… 'ordered' modeli '{model_ordered_path}' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n'clicked' modeli eÄŸitiliyor...\")\n",
        "clicked_counts = train_pd['clicked'].value_counts()\n",
        "params['scale_pos_weight'] = clicked_counts[0] / clicked_counts[1]\n",
        "model_clicked = lgb.train(params, lgb_train_clicked, valid_sets=[lgb_train_clicked, lgb_valid_clicked], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_clicked_path = os.path.join(MODEL_DIR, \"model_clicked_v6_no_price_features.txt\") # Dosya adÄ±nÄ± gÃ¼ncelledim\n",
        "model_clicked.save_model(model_clicked_path)\n",
        "print(f\"âœ… 'clicked' modeli '{model_clicked_path}' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n--- EÄŸitim ve Kaydetme Ä°ÅŸlemi TamamlandÄ±! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3tquE-TIwE5",
        "outputId": "d14739c5-0db9-4574-982a-bf8f1c11c31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 1: Model EÄŸitme (Fiyat Ã–zellikleri Ã‡IKARILDI, Zamansal Ã–zellikler KALDI) ---\n",
            "EÄŸitim verisi yÃ¼kleniyor: /content/Trendyol Veri/enriched_train_data_FINAL.parquet\n",
            "Toplam eÄŸitim verisi boyutu: (2773805, 37)\n",
            "Zamansal Ã¶zellikler Ã¼retiliyor ve veri zamana gÃ¶re ayrÄ±lÄ±yor...\n",
            "Train parÃ§asÄ± boyutu: (2357734, 39)\n",
            "Validation parÃ§asÄ± boyutu: (416071, 39)\n",
            "âœ… Fiyat etkileÅŸim Ã¶zellikleri bu denemede kullanÄ±lmayacaktÄ±r.\n",
            "Veri setleri Pandas'a Ã§evriliyor ve model iÃ§in hazÄ±rlanÄ±yor...\n",
            "KullanÄ±lacak Ã¶zellik sayÄ±sÄ±: 25\n",
            "GÃ¼ncel Kategorik Ã¶zellik sayÄ±sÄ±: 4\n",
            "lgb.Dataset nesneleri oluÅŸturuluyor...\n",
            "Son Kategorik Ã¶zellik sayÄ±sÄ±: 4\n",
            "\n",
            "'ordered' modeli eÄŸitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttrain's auc: 0.764107\tvalid's auc: 0.700363\n",
            "âœ… 'ordered' modeli 'models_4_doldur_FiyatUygunluk/model_ordered_v6_no_price_features.txt' olarak kaydedildi.\n",
            "\n",
            "'clicked' modeli eÄŸitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[127]\ttrain's auc: 0.712264\tvalid's auc: 0.659234\n",
            "âœ… 'clicked' modeli 'models_4_doldur_FiyatUygunluk/model_clicked_v6_no_price_features.txt' olarak kaydedildi.\n",
            "\n",
            "--- EÄŸitim ve Kaydetme Ä°ÅŸlemi TamamlandÄ±! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 2** data sÄ±zÄ±ntÄ±sÄ± giderilmiÅŸ"
      ],
      "metadata": {
        "id": "vZ1SGdM0Jid1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "from trendyol_metric_group_auc import score\n",
        "\n",
        "print(\"--- KOD 2: Nihai Tahmin ve Skorlama (Fiyat Ã–zellikleri Ã‡IKARILMIÅ) ---\")\n",
        "\n",
        "# --- AdÄ±m 1: Gerekli DosyalarÄ± YÃ¼kleme ---\n",
        "VAL_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "TEST_PATH = \"/content/Trendyol Veri/enriched_test_data_FINAL.parquet\"\n",
        "MODELS_DIR = \"models_4_doldur_FiyatUygunluk\" # Model klasÃ¶rÃ¼ aynÄ± kaldÄ±\n",
        "# DÃ¼zeltme: Model dosya adlarÄ±nÄ± gÃ¼ncelledik (no_price_features versiyonlarÄ±)\n",
        "MODEL_ORDERED_PATH = os.path.join(MODELS_DIR, 'model_ordered_v6_no_price_features.txt')\n",
        "MODEL_CLICKED_PATH = os.path.join(MODELS_DIR, 'model_clicked_v6_no_price_features.txt')\n",
        "\n",
        "\n",
        "print(\"Veriler ve kaydedilmiÅŸ v6 modelleri yÃ¼kleniyor...\")\n",
        "val_full_df = pl.read_parquet(VAL_PATH)\n",
        "test_df_pl = pl.read_parquet(TEST_PATH)\n",
        "model_ordered = lgb.Booster(model_file=MODEL_ORDERED_PATH)\n",
        "model_clicked = lgb.Booster(model_file=MODEL_CLICKED_PATH)\n",
        "print(\"âœ… YÃ¼kleme tamamlandÄ±.\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 2: Lokal Skor Ä°Ã§in Validation Seti HazÄ±rlama ---\n",
        "print(\"\\nLokal skor iÃ§in validation seti hazÄ±rlanÄ±yor...\")\n",
        "if \"ts_hour\" in val_full_df.columns and val_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    val_full_df = val_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "\n",
        "# Zamansal Ã¶zellikleri ekle (Bunlar hala kullanÄ±lÄ±yor)\n",
        "val_full_df = val_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "\n",
        "# DÃ¼zeltme: Fiyat Ã¶zelliÄŸi ekleme ve iÅŸleme adÄ±mlarÄ±nÄ± Ã‡IKARDIK\n",
        "\n",
        "\n",
        "val_full_df = val_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(val_full_df) * 0.85)\n",
        "# Pandas'a Ã§evir ve sayÄ±sal null'larÄ± 0 doldur (fiyat Ã¶zellikleri artÄ±k olmadÄ±ÄŸÄ± iÃ§in genel fillna(0) gÃ¼venli)\n",
        "val_pd = val_full_df[split_index:].to_pandas()\n",
        "\n",
        "# SayÄ±sal sÃ¼tunlardaki NaN/Inf deÄŸerlerini yÃ¶netme ve 0 ile doldurma\n",
        "numeric_cols = val_pd.select_dtypes(include=np.number).columns.tolist()\n",
        "for col in numeric_cols:\n",
        "    val_pd[col] = val_pd[col].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "# DÃ¼zeltme: exclude_cols listesi eÄŸitimdekiyle aynÄ± (fiyat Ã¶zellikleri Ã§Ä±karÄ±lmÄ±ÅŸ hali)\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\", \"price_vs_user_avg_ratio\", \"user_avg_order_price\"]\n",
        "features = [col for col in val_pd.columns if col not in exclude_cols] # DÃ¼zeltme: features listesi exclude_cols'a gÃ¶re gÃ¼ncellendi\n",
        "\n",
        "string_cols = [col for col in val_pd.columns if val_pd[col].dtype == 'object']\n",
        "temporal_cols_as_cat = [\"hour_of_day\", \"day_of_week\"]\n",
        "cols_to_categorize = string_cols + temporal_cols_as_cat\n",
        "for col in cols_to_categorize:\n",
        "    if col in val_pd.columns:\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "# Tahmin Ã¼ret\n",
        "val_pd['p_order'] = model_ordered.predict(val_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "val_pd['p_click'] = model_clicked.predict(val_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "val_pd['final_score'] = 0.7 * val_pd['p_order'] + 0.3 * val_pd['p_click']\n",
        "\n",
        "# Skor fonksiyonu iÃ§in gerekli DataFrame'leri oluÅŸtur\n",
        "val_solution = val_pd.groupby('session_id', observed=False).agg(ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), all_items=('content_id_hashed', ' '.join)).reset_index()\n",
        "val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id', observed=False)['content_id_hashed'].apply(' '.join).reset_index()\n",
        "val_submission.rename(columns={'content_id_hashed': 'prediction'}, inplace=True)\n",
        "\n",
        "try:\n",
        "    local_final_score = score(val_solution, val_submission, 'session_id')\n",
        "    print(\"\\n-------------------------------------------\")\n",
        "    print(f\"ğŸ† LOKAL SKORUNUZ (Fiyat Ã–zellikleri Ã‡Ä±karÄ±lmÄ±ÅŸ): {local_final_score:.5f}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "except Exception as e:\n",
        "    print(f\"Skor hesaplanÄ±rken bir hata oluÅŸtu: {e}\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 3: Kaggle iÃ§in Submission DosyasÄ± OluÅŸturma ---\n",
        "print(\"\\nTest verisi hazÄ±rlanÄ±yor ve Kaggle iÃ§in submission dosyasÄ± oluÅŸturuluyor...\")\n",
        "if \"ts_hour\" in test_df_pl.columns and test_df_pl[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    test_df_pl = test_df_pl.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "# Zamansal Ã¶zellikleri ekle (Bunlar hala kullanÄ±lÄ±yor)\n",
        "test_df_pl = test_df_pl.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "# DÃ¼zeltme: Fiyat Ã¶zelliÄŸi ekleme ve iÅŸleme adÄ±mlarÄ±nÄ± Ã‡IKARDIK\n",
        "\n",
        "# Pandas'a Ã§evir ve sayÄ±sal null'larÄ± 0 doldur\n",
        "test_pd = test_df_pl.to_pandas()\n",
        "numeric_cols_test = test_pd.select_dtypes(include=np.number).columns.tolist()\n",
        "for col in numeric_cols_test:\n",
        "     test_pd[col] = test_pd[col].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "\n",
        "for col in cols_to_categorize:\n",
        "    if col in test_pd.columns:\n",
        "        test_pd[col] = test_pd[col].astype('category')\n",
        "\n",
        "# Features listesi eÄŸitimdekiyle aynÄ± olmalÄ±\n",
        "# exclude_cols listesi eÄŸitimdekiyle aynÄ± (fiyat Ã¶zellikleri Ã§Ä±karÄ±lmÄ±ÅŸ hali)\n",
        "exclude_cols_test = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\", \"price_vs_user_avg_ratio\", \"user_avg_order_price\"]\n",
        "features_test = [col for col in test_pd.columns if col not in exclude_cols_test] # DÃ¼zeltme: features_test listesi exclude_cols_test'e gÃ¶re gÃ¼ncellendi\n",
        "\n",
        "p_order_test = model_ordered.predict(test_pd[features_test], num_iteration=model_ordered.best_iteration)\n",
        "p_click_test = model_clicked.predict(test_pd[features_test], num_iteration=model_clicked.best_iteration)\n",
        "\n",
        "\n",
        "test_df_pl = test_df_pl.with_columns(\n",
        "    final_score=(0.7 * pl.Series(p_order_test)) + (0.3 * pl.Series(p_click_test))\n",
        ")\n",
        "submission_df = test_df_pl.sort([\"session_id\", \"final_score\"], descending=True).group_by(\"session_id\").agg(\n",
        "    pl.col(\"content_id_hashed\").alias(\"prediction\")\n",
        ").with_columns(\n",
        "    pl.col(\"prediction\").list.join(\" \")\n",
        ")\n",
        "\n",
        "expected_rows = 18589\n",
        "actual_rows = submission_df.shape[0]\n",
        "print(f\"\\nOluÅŸturulan submission satÄ±r sayÄ±sÄ±: {actual_rows} (Beklenen: {expected_rows})\")\n",
        "if actual_rows == expected_rows: print(\"âœ… SatÄ±r sayÄ±sÄ± doÄŸru.\")\n",
        "else: print(\"âŒ UYARI: SatÄ±r sayÄ±sÄ± yanlÄ±ÅŸ!\")\n",
        "\n",
        "submission_path = \"submission_no_price_features.csv\" # Dosya adÄ±nÄ± gÃ¼ncelledim\n",
        "submission_df.write_csv(submission_path)\n",
        "print(f\"\\nSubmission dosyasÄ± '{submission_path}' olarak kaydedildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjPYqB-KJqAg",
        "outputId": "a44a1393-68ec-436d-a42b-9a3b0c3d5cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 2: Nihai Tahmin ve Skorlama (Fiyat Ã–zellikleri Ã‡IKARILMIÅ) ---\n",
            "Veriler ve kaydedilmiÅŸ v6 modelleri yÃ¼kleniyor...\n",
            "âœ… YÃ¼kleme tamamlandÄ±.\n",
            "\n",
            "Lokal skor iÃ§in validation seti hazÄ±rlanÄ±yor...\n",
            "Ordered AUC:  0.6504243273845817\n",
            "Clicked AUC:  0.5914654200400459\n",
            "\n",
            "-------------------------------------------\n",
            "ğŸ† LOKAL SKORUNUZ (Fiyat Ã–zellikleri Ã‡Ä±karÄ±lmÄ±ÅŸ): 0.63274\n",
            "-------------------------------------------\n",
            "\n",
            "Test verisi hazÄ±rlanÄ±yor ve Kaggle iÃ§in submission dosyasÄ± oluÅŸturuluyor...\n",
            "\n",
            "OluÅŸturulan submission satÄ±r sayÄ±sÄ±: 18589 (Beklenen: 18589)\n",
            "âœ… SatÄ±r sayÄ±sÄ± doÄŸru.\n",
            "\n",
            "Submission dosyasÄ± 'submission_no_price_features.csv' olarak kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deneme 4 Alaka DÃ¼zeyi"
      ],
      "metadata": {
        "id": "WqLtGSFGdSJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 1** Alaka DÃ¼zeyine gÃ¶re eÄŸitmek ve modelleri kaydetmek"
      ],
      "metadata": {
        "id": "wTgktMeGdXs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"--- KOD 1: Model EÄŸitme (Yeni Alaka DÃ¼zeyi Ã–zellikleriyle) ---\")\n",
        "\n",
        "# --- AdÄ±m 1: Veri YÃ¼kleme ---\n",
        "TRAIN_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "train_full_df = pl.read_parquet(TRAIN_PATH)\n",
        "print(f\"Toplam eÄŸitim verisi boyutu: {train_full_df.shape}\")\n",
        "\n",
        "# --- AdÄ±m 2: Ã–zellik MÃ¼hendisliÄŸi ve Veri AyÄ±rma ---\n",
        "print(\"Ã–n Ã¶zellikler Ã¼retiliyor...\")\n",
        "if \"ts_hour\" in train_full_df.columns and train_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    train_full_df = train_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "\n",
        "# 2.1 Zamansal Ã–zellikler\n",
        "train_full_df = train_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "\n",
        "# 2.2 Alaka DÃ¼zeyi (Relevance) Ã–zellikleri\n",
        "print(\"Yeni Alaka DÃ¼zeyi (Relevance) Ã¶zellikleri Ã¼retiliyor...\")\n",
        "train_full_df = train_full_df.with_columns(pl.col(\"search_term_normalized\").fill_null(\"\"), pl.col(\"cv_tags\").fill_null(\"\"))\n",
        "search_words = pl.col(\"search_term_normalized\").str.split(\" \")\n",
        "tag_words = pl.col(\"cv_tags\").str.split(\" \")\n",
        "intersection_len = search_words.list.set_intersection(tag_words).list.len()\n",
        "union_len = search_words.list.set_union(tag_words).list.len()\n",
        "train_full_df = train_full_df.with_columns(\n",
        "    term_in_tags_match = (intersection_len > 0).cast(pl.Int8),\n",
        "    jaccard_similarity_tags = pl.when(union_len > 0).then(intersection_len / union_len).otherwise(0.0)\n",
        ")\n",
        "\n",
        "# 2.3 Train/Validation AyrÄ±mÄ± (Ã–ZELLÄ°K ÃœRETÄ°MÄ°NDEN Ã–NCE)\n",
        "train_full_df = train_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(train_full_df) * 0.85)\n",
        "train_df, val_df = train_full_df[:split_index], train_full_df[split_index:]\n",
        "print(f\"Train parÃ§asÄ± boyutu: {train_df.shape}\")\n",
        "print(f\"Validation parÃ§asÄ± boyutu: {val_df.shape}\")\n",
        "\n",
        "# 2.4 Fiyat EtkileÅŸim Ã–zelliÄŸini SADECE Train Setinden Hesaplama\n",
        "print(\"Fiyat etkileÅŸim Ã¶zelliÄŸi, SADECE train verisi Ã¼zerinden hesaplanÄ±yor...\")\n",
        "price_features_lazy = pl.scan_parquet(TRAIN_PATH).select([\"content_id_hashed\", \"selling_price\", \"update_date\"])\n",
        "latest_price_features = price_features_lazy.sort(\"update_date\", descending=True).group_by(\"content_id_hashed\").first().collect()\n",
        "user_orders = train_df.filter(pl.col(\"ordered\") == 1).select([\"user_id_hashed\", \"content_id_hashed\"])\n",
        "user_orders_with_price = user_orders.join(latest_price_features, on=\"content_id_hashed\", how=\"left\")\n",
        "agg_user_price_features = user_orders_with_price.group_by(\"user_id_hashed\").agg(pl.mean(\"selling_price\").alias(\"user_avg_order_price\"))\n",
        "FEATURES_DIR = \"features\"\n",
        "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
        "agg_user_price_features.write_parquet(os.path.join(FEATURES_DIR, \"user_price_features_final.parquet\"))\n",
        "print(f\"âœ… SÄ±zÄ±ntÄ±sÄ±z kullanÄ±cÄ± fiyat Ã¶zellikleri kaydedildi.\")\n",
        "\n",
        "# 2.5 Ã–zellikleri Ekleme ve AkÄ±llÄ± BoÅŸluk Doldurma\n",
        "global_avg_order_price = agg_user_price_features.select(pl.mean(\"user_avg_order_price\")).item()\n",
        "train_df = train_df.join(agg_user_price_features, on=\"user_id_hashed\", how=\"left\").with_columns(pl.col(\"user_avg_order_price\").fill_null(global_avg_order_price)).with_columns((pl.col(\"selling_price\") / pl.col(\"user_avg_order_price\")).alias(\"price_vs_user_avg_ratio\"))\n",
        "val_df = val_df.join(agg_user_price_features, on=\"user_id_hashed\", how=\"left\").with_columns(pl.col(\"user_avg_order_price\").fill_null(global_avg_order_price)).with_columns((pl.col(\"selling_price\") / pl.col(\"user_avg_order_price\")).alias(\"price_vs_user_avg_ratio\"))\n",
        "\n",
        "# --- AdÄ±m 3: Veri HazÄ±rlama ve AYRI Ã–ZELLÄ°K SETLERÄ° OLUÅTURMA ---\n",
        "print(\"Veri setleri Pandas'a Ã§evriliyor ve ayrÄ± Ã¶zellik setleri oluÅŸturuluyor...\")\n",
        "train_pd = train_df.to_pandas().fillna(0)\n",
        "val_pd = val_df.to_pandas().fillna(0)\n",
        "string_cols = [col for col in train_pd.columns if train_pd[col].dtype == 'object']\n",
        "temporal_cols_as_cat = [\"hour_of_day\", \"day_of_week\"]\n",
        "cols_to_categorize = string_cols + temporal_cols_as_cat\n",
        "for col in cols_to_categorize:\n",
        "    if col in train_pd.columns:\n",
        "        train_pd[col], val_pd[col] = train_pd[col].astype('category'), val_pd[col].astype('category')\n",
        "\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features_base = [col for col in train_pd.columns if col not in exclude_cols]\n",
        "price_features_to_exclude = [\"user_avg_order_price\", \"price_vs_user_avg_ratio\"]\n",
        "features_for_ordered = [f for f in features_base if f not in price_features_to_exclude]\n",
        "features_for_clicked = features_base\n",
        "cat_features_ordered = [f for f in features_for_ordered if isinstance(train_pd[f].dtype, pd.CategoricalDtype)]\n",
        "cat_features_clicked = [f for f in features_for_clicked if isinstance(train_pd[f].dtype, pd.CategoricalDtype)]\n",
        "print(f\"'ordered' modeli iÃ§in Ã¶zellik sayÄ±sÄ±: {len(features_for_ordered)}\")\n",
        "print(f\"'clicked' modeli iÃ§in Ã¶zellik sayÄ±sÄ±: {len(features_for_clicked)}\")\n",
        "\n",
        "# --- AdÄ±m 4: lgb.Dataset OluÅŸturma ---\n",
        "print(\"Modele Ã¶zel lgb.Dataset nesneleri oluÅŸturuluyor...\")\n",
        "lgb_train_ordered = lgb.Dataset(train_pd[features_for_ordered], label=train_pd['ordered'], categorical_feature=cat_features_ordered)\n",
        "lgb_valid_ordered = lgb.Dataset(val_pd[features_for_ordered], label=val_pd['ordered'], categorical_feature=cat_features_ordered)\n",
        "lgb_train_clicked = lgb.Dataset(train_pd[features_for_clicked], label=train_pd['clicked'], categorical_feature=cat_features_clicked)\n",
        "lgb_valid_clicked = lgb.Dataset(val_pd[features_for_clicked], label=val_pd['clicked'], categorical_feature=cat_features_clicked)\n",
        "\n",
        "# --- AdÄ±m 5: Model EÄŸitimi ve Kaydetme ---\n",
        "params = { \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\", \"learning_rate\": 0.03, \"num_leaves\": 64, \"verbose\": -1, \"seed\": 42 }\n",
        "callbacks = [lgb.early_stopping(100, verbose=True)]\n",
        "MODEL_DIR = \"models_6_doldur_AlakaDuzeyi\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(\"\\n'ordered' modeli (sÄ±nÄ±rlÄ± Ã¶zelliklerle) eÄŸitiliyor...\")\n",
        "ordered_counts = train_pd['ordered'].value_counts()\n",
        "params['scale_pos_weight'] = ordered_counts[0] / ordered_counts[1]\n",
        "model_ordered = lgb.train(params, lgb_train_ordered, valid_sets=[lgb_train_ordered, lgb_valid_ordered], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_ordered.save_model(os.path.join(MODEL_DIR, \"model_ordered_v6_relevance.txt\"))\n",
        "print(f\"âœ… 'ordered' modeli '{MODEL_DIR}/model_ordered_v6_relevance.txt' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n'clicked' modeli (tÃ¼m Ã¶zelliklerle) eÄŸitiliyor...\")\n",
        "clicked_counts = train_pd['clicked'].value_counts()\n",
        "params['scale_pos_weight'] = clicked_counts[0] / clicked_counts[1]\n",
        "model_clicked = lgb.train(params, lgb_train_clicked, valid_sets=[lgb_train_clicked, lgb_valid_clicked], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_clicked.save_model(os.path.join(MODEL_DIR, \"model_clicked_v6_relevance.txt\"))\n",
        "print(f\"âœ… 'clicked' modeli '{MODEL_DIR}/model_clicked_v6_relevance.txt' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n--- EÄŸitim ve Kaydetme Ä°ÅŸlemi TamamlandÄ±! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-7VfOPOddum",
        "outputId": "79174104-acae-4961-8629-4599ff6d3eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 1: Model EÄŸitme (Yeni Alaka DÃ¼zeyi Ã–zellikleriyle) ---\n",
            "Toplam eÄŸitim verisi boyutu: (2773805, 37)\n",
            "Ã–n Ã¶zellikler Ã¼retiliyor...\n",
            "Yeni Alaka DÃ¼zeyi (Relevance) Ã¶zellikleri Ã¼retiliyor...\n",
            "Train parÃ§asÄ± boyutu: (2357734, 41)\n",
            "Validation parÃ§asÄ± boyutu: (416071, 41)\n",
            "Fiyat etkileÅŸim Ã¶zelliÄŸi, SADECE train verisi Ã¼zerinden hesaplanÄ±yor...\n",
            "âœ… SÄ±zÄ±ntÄ±sÄ±z kullanÄ±cÄ± fiyat Ã¶zellikleri kaydedildi.\n",
            "Veri setleri Pandas'a Ã§evriliyor ve ayrÄ± Ã¶zellik setleri oluÅŸturuluyor...\n",
            "'ordered' modeli iÃ§in Ã¶zellik sayÄ±sÄ±: 27\n",
            "'clicked' modeli iÃ§in Ã¶zellik sayÄ±sÄ±: 29\n",
            "Modele Ã¶zel lgb.Dataset nesneleri oluÅŸturuluyor...\n",
            "\n",
            "'ordered' modeli (sÄ±nÄ±rlÄ± Ã¶zelliklerle) eÄŸitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttrain's auc: 0.764107\tvalid's auc: 0.700363\n",
            "âœ… 'ordered' modeli 'models_6_doldur_AlakaDuzeyi/model_ordered_v6_relevance.txt' olarak kaydedildi.\n",
            "\n",
            "'clicked' modeli (tÃ¼m Ã¶zelliklerle) eÄŸitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[122]\ttrain's auc: 0.732438\tvalid's auc: 0.65745\n",
            "âœ… 'clicked' modeli 'models_6_doldur_AlakaDuzeyi/model_clicked_v6_relevance.txt' olarak kaydedildi.\n",
            "\n",
            "--- EÄŸitim ve Kaydetme Ä°ÅŸlemi TamamlandÄ±! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 2** Alaka DÃ¼zeyi ile test etme ve submissions oluÅŸturma"
      ],
      "metadata": {
        "id": "R-7_GpBfeWfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "from trendyol_metric_group_auc import score\n",
        "\n",
        "print(\"--- KOD 2: Nihai Tahmin ve Skorlama (Yeni Alaka DÃ¼zeyi Ã–zellikleriyle) ---\")\n",
        "\n",
        "# --- AdÄ±m 1: Gerekli DosyalarÄ± YÃ¼kleme ---\n",
        "VAL_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "TEST_PATH = \"/content/Trendyol Veri/enriched_test_data_FINAL.parquet\"\n",
        "FEATURES_DIR = \"features\"\n",
        "MODELS_DIR = \"models_6_doldur_AlakaDuzeyi\"\n",
        "USER_PRICE_FEATURES_PATH = os.path.join(FEATURES_DIR, \"user_price_features_final.parquet\")\n",
        "MODEL_ORDERED_PATH = os.path.join(MODELS_DIR, 'model_ordered_v6_relevance.txt')\n",
        "MODEL_CLICKED_PATH = os.path.join(MODELS_DIR, 'model_clicked_v6_relevance.txt')\n",
        "\n",
        "print(\"Veriler, Ã¶zellikler ve kaydedilmiÅŸ nihai modeller yÃ¼kleniyor...\")\n",
        "val_full_df = pl.read_parquet(VAL_PATH)\n",
        "test_df_pl = pl.read_parquet(TEST_PATH)\n",
        "user_price_features = pl.read_parquet(USER_PRICE_FEATURES_PATH)\n",
        "model_ordered = lgb.Booster(model_file=MODEL_ORDERED_PATH)\n",
        "model_clicked = lgb.Booster(model_file=MODEL_CLICKED_PATH)\n",
        "print(\"âœ… YÃ¼kleme tamamlandÄ±.\")\n",
        "\n",
        "# --- AdÄ±m 2: Lokal Skor Ä°Ã§in Validation Seti HazÄ±rlama ---\n",
        "print(\"\\nLokal skor iÃ§in validation seti hazÄ±rlanÄ±yor...\")\n",
        "# 2.1 TÃ¼m Ã–zellikleri Ekle\n",
        "if \"ts_hour\" in val_full_df.columns and val_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    val_full_df = val_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "val_full_df = val_full_df.with_columns(pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"), pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\"))\n",
        "val_full_df = val_full_df.with_columns(pl.col(\"search_term_normalized\").fill_null(\"\"), pl.col(\"cv_tags\").fill_null(\"\"))\n",
        "search_words_val = pl.col(\"search_term_normalized\").str.split(\" \")\n",
        "tag_words_val = pl.col(\"cv_tags\").str.split(\" \")\n",
        "intersection_len_val = search_words_val.list.set_intersection(tag_words_val).list.len()\n",
        "union_len_val = search_words_val.list.set_union(tag_words_val).list.len()\n",
        "val_full_df = val_full_df.with_columns(term_in_tags_match=(intersection_len_val > 0).cast(pl.Int8), jaccard_similarity_tags=pl.when(union_len_val > 0).then(intersection_len_val / union_len_val).otherwise(0.0))\n",
        "global_avg_order_price = user_price_features.select(pl.mean(\"user_avg_order_price\")).item()\n",
        "val_full_df = val_full_df.join(user_price_features, on=\"user_id_hashed\", how=\"left\").with_columns(pl.col(\"user_avg_order_price\").fill_null(global_avg_order_price)).with_columns((pl.col(\"selling_price\") / pl.col(\"user_avg_order_price\")).alias(\"price_vs_user_avg_ratio\"))\n",
        "\n",
        "# 2.2 Veriyi AyÄ±r ve HazÄ±rla\n",
        "val_full_df = val_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(val_full_df) * 0.85)\n",
        "val_pd = val_full_df[split_index:].to_pandas().fillna(0)\n",
        "\n",
        "# 2.3 AyrÄ± Ã–zellik Setlerini TanÄ±mla\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features_base = [col for col in val_pd.columns if col not in exclude_cols]\n",
        "price_features_to_exclude = [\"user_avg_order_price\", \"price_vs_user_avg_ratio\"]\n",
        "features_for_ordered = [f for f in features_base if f not in price_features_to_exclude]\n",
        "features_for_clicked = features_base\n",
        "string_cols = [col for col in val_pd.columns if val_pd[col].dtype == 'object']\n",
        "temporal_cols_as_cat = [\"hour_of_day\", \"day_of_week\"]\n",
        "cols_to_categorize = string_cols + temporal_cols_as_cat\n",
        "for col in cols_to_categorize:\n",
        "    if col in val_pd.columns:\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "# 2.4 Tahmin ve Skorlama\n",
        "val_pd['p_order'] = model_ordered.predict(val_pd[features_for_ordered], num_iteration=model_ordered.best_iteration)\n",
        "val_pd['p_click'] = model_clicked.predict(val_pd[features_for_clicked], num_iteration=model_clicked.best_iteration)\n",
        "val_pd['final_score'] = 0.7 * val_pd['p_order'] + 0.3 * val_pd['p_click']\n",
        "val_solution = val_pd.groupby('session_id').agg(ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), all_items=('content_id_hashed', ' '.join)).reset_index()\n",
        "val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n",
        "val_submission.rename(columns={'content_id_hashed': 'prediction'}, inplace=True)\n",
        "try:\n",
        "    local_final_score = score(val_solution, val_submission, 'session_id')\n",
        "    print(\"\\n-------------------------------------------\")\n",
        "    print(f\"ğŸ† LOKAL SKORUNUZ (Yeni Ã–zelliklerle): {local_final_score:.5f}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "except Exception as e:\n",
        "    print(f\"Skor hesaplanÄ±rken bir hata oluÅŸtu: {e}\")\n",
        "\n",
        "# --- AdÄ±m 3: Kaggle iÃ§in Submission DosyasÄ± OluÅŸturma ---\n",
        "print(\"\\nTest verisi hazÄ±rlanÄ±yor ve Kaggle iÃ§in submission dosyasÄ± oluÅŸturuluyor...\")\n",
        "if \"ts_hour\" in test_df_pl.columns and test_df_pl[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    test_df_pl = test_df_pl.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "test_df_pl = test_df_pl.with_columns(pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"), pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\"))\n",
        "test_df_pl = test_df_pl.with_columns(pl.col(\"search_term_normalized\").fill_null(\"\"), pl.col(\"cv_tags\").fill_null(\"\"))\n",
        "search_words_test = pl.col(\"search_term_normalized\").str.split(\" \")\n",
        "tag_words_test = pl.col(\"cv_tags\").str.split(\" \")\n",
        "intersection_len_test = search_words_test.list.set_intersection(tag_words_test).list.len()\n",
        "union_len_test = search_words_test.list.set_union(tag_words_test).list.len()\n",
        "test_df_pl = test_df_pl.with_columns(term_in_tags_match=(intersection_len_test > 0).cast(pl.Int8), jaccard_similarity_tags=pl.when(union_len_test > 0).then(intersection_len_test / union_len_test).otherwise(0.0))\n",
        "test_df_pl = test_df_pl.join(user_price_features, on=\"user_id_hashed\", how=\"left\").with_columns(pl.col(\"user_avg_order_price\").fill_null(global_avg_order_price)).with_columns((pl.col(\"selling_price\") / pl.col(\"user_avg_order_price\")).alias(\"price_vs_user_avg_ratio\"))\n",
        "\n",
        "test_pd = test_df_pl.to_pandas().fillna(0)\n",
        "for col in cols_to_categorize:\n",
        "    if col in test_pd.columns:\n",
        "        test_pd[col] = test_pd[col].astype('category')\n",
        "\n",
        "p_order_test = model_ordered.predict(test_pd[features_for_ordered], num_iteration=model_ordered.best_iteration)\n",
        "p_click_test = model_clicked.predict(test_pd[features_for_clicked], num_iteration=model_clicked.best_iteration)\n",
        "\n",
        "test_df_pl = test_df_pl.with_columns(final_score=(0.7 * pl.Series(p_order_test)) + (0.3 * pl.Series(p_click_test)))\n",
        "submission_df = test_df_pl.sort([\"session_id\", \"final_score\"], descending=True).group_by(\"session_id\").agg(pl.col(\"content_id_hashed\").alias(\"prediction\")).with_columns(pl.col(\"prediction\").list.join(\" \"))\n",
        "expected_rows = 18589\n",
        "actual_rows = submission_df.shape[0]\n",
        "print(f\"\\nOluÅŸturulan submission satÄ±r sayÄ±sÄ±: {actual_rows} (Beklenen: {expected_rows})\")\n",
        "if actual_rows == expected_rows: print(\"âœ… SatÄ±r sayÄ±sÄ± doÄŸru.\")\n",
        "else: print(\"âŒ UYARI: SatÄ±r sayÄ±sÄ± yanlÄ±ÅŸ!\")\n",
        "submission_path = \"submission.csv\"\n",
        "submission_df.write_csv(submission_path)\n",
        "print(f\"\\nSubmission dosyasÄ± '{submission_path}' olarak kaydedildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un0mBHjRebKL",
        "outputId": "abf0ebc4-fcff-4096-f8bf-777a2087d210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 2: Nihai Tahmin ve Skorlama (Yeni Alaka DÃ¼zeyi Ã–zellikleriyle) ---\n",
            "Veriler, Ã¶zellikler ve kaydedilmiÅŸ nihai modeller yÃ¼kleniyor...\n",
            "âœ… YÃ¼kleme tamamlandÄ±.\n",
            "\n",
            "Lokal skor iÃ§in validation seti hazÄ±rlanÄ±yor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1787726108.py:65: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  val_solution = val_pd.groupby('session_id').agg(ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), all_items=('content_id_hashed', ' '.join)).reset_index()\n",
            "/tmp/ipython-input-1787726108.py:66: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ordered AUC:  0.6501352808701953\n",
            "Clicked AUC:  0.5901990785840607\n",
            "\n",
            "-------------------------------------------\n",
            "ğŸ† LOKAL SKORUNUZ (Yeni Ã–zelliklerle): 0.63215\n",
            "-------------------------------------------\n",
            "\n",
            "Test verisi hazÄ±rlanÄ±yor ve Kaggle iÃ§in submission dosyasÄ± oluÅŸturuluyor...\n",
            "\n",
            "OluÅŸturulan submission satÄ±r sayÄ±sÄ±: 18589 (Beklenen: 18589)\n",
            "âœ… SatÄ±r sayÄ±sÄ± doÄŸru.\n",
            "\n",
            "Submission dosyasÄ± 'submission.csv' olarak kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deneme 5 Hiperparametre Optimizasyonu"
      ],
      "metadata": {
        "id": "TRdXhjYphJoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 1** Optuna kullnarak hiperparemetre optimizasyonu yapmak"
      ],
      "metadata": {
        "id": "9YKrMku8hN2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M9cemRajRNb",
        "outputId": "71cf23cd-8941-4271-bc31-a2141419c655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/395.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m \u001b[32m389.1/395.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/247.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "import optuna # Optuna kÃ¼tÃ¼phanesini ekledik\n",
        "\n",
        "print(\"--- KOD 1: Baseline Model Ä°Ã§in Optuna ile HPO ve EÄŸitim ---\")\n",
        "\n",
        "# --- AdÄ±m 1: Veri YÃ¼kleme ---\n",
        "TRAIN_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "print(f\"EÄŸitim verisi yÃ¼kleniyor: {TRAIN_PATH}\")\n",
        "train_full_df = pl.read_parquet(TRAIN_PATH)\n",
        "print(f\"Toplam eÄŸitim verisi boyutu: {train_full_df.shape}\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 2: Veriyi Zamana GÃ¶re AyÄ±rma (Train / Validation Split) ---\n",
        "print(\"EÄŸitim verisi, zamana gÃ¶re sÄ±ralanÄ±p yÃ¼zdesel olarak ayrÄ±lÄ±yor...\")\n",
        "if \"ts_hour\" in train_full_df.columns and train_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    train_full_df = train_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "train_full_df = train_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(train_full_df) * 0.85)\n",
        "train_df = train_full_df[:split_index]\n",
        "val_df = train_full_df[split_index:]\n",
        "print(f\"Train parÃ§asÄ± boyutu: {train_df.shape}\")\n",
        "print(f\"Validation parÃ§asÄ± boyutu: {val_df.shape}\")\n",
        "\n",
        "\n",
        "# --- AdÄ±m 3: Veri HazÄ±rlama ve Tip DÃ¶nÃ¼ÅŸÃ¼mÃ¼ ---\n",
        "print(\"Veri setleri Pandas'a Ã§evriliyor ve model iÃ§in hazÄ±rlanÄ±yor...\")\n",
        "train_pd = train_df.to_pandas().fillna(0)\n",
        "val_pd = val_df.to_pandas().fillna(0)\n",
        "for col in train_pd.columns:\n",
        "    if train_pd[col].dtype == 'object':\n",
        "        train_pd[col] = train_pd[col].astype('category')\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features = [col for col in train_pd.columns if col not in exclude_cols]\n",
        "cat_features = [col for col in features if isinstance(train_pd[col].dtype, pd.CategoricalDtype)]\n",
        "print(f\"KullanÄ±lacak Ã¶zellik sayÄ±sÄ±: {len(features)}\")\n",
        "\n",
        "\n",
        "# --- YENÄ° AdÄ±m 4: Optuna Optimizasyon Fonksiyonu ('ordered' iÃ§in) ---\n",
        "# Bu fonksiyon, Optuna'nÄ±n her denemede Ã§alÄ±ÅŸtÄ±racaÄŸÄ± model eÄŸitim sÃ¼recini tanÄ±mlar\n",
        "def objective_ordered(trial):\n",
        "    # Optuna'nÄ±n deneyeceÄŸi hiperparametre aralÄ±klarÄ±nÄ± belirliyoruz\n",
        "    params = {\n",
        "        \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\",\n",
        "        \"seed\": 42, \"verbose\": -1, \"n_jobs\": -1,\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
        "    }\n",
        "    # SÄ±nÄ±f dengesizliÄŸi iÃ§in aÄŸÄ±rlÄ±ÄŸÄ± ekle\n",
        "    ordered_counts = train_pd['ordered'].value_counts()\n",
        "    params['scale_pos_weight'] = ordered_counts[0] / ordered_counts[1]\n",
        "\n",
        "    # Modeli eÄŸit\n",
        "    lgb_train = lgb.Dataset(train_pd[features], label=train_pd['ordered'], categorical_feature=cat_features)\n",
        "    lgb_valid = lgb.Dataset(val_pd[features], label=val_pd['ordered'], categorical_feature=cat_features)\n",
        "    model = lgb.train(params, lgb_train, valid_sets=[lgb_valid], valid_names=[\"valid\"], num_boost_round=2000, callbacks=[lgb.early_stopping(50, verbose=False)])\n",
        "\n",
        "    # Optuna, bu dÃ¶ndÃ¼rÃ¼len 'valid's auc' skorunu maksimize etmeye Ã§alÄ±ÅŸacak\n",
        "    return model.best_score['valid']['auc']\n",
        "\n",
        "print(\"\\n'ordered' modeli iÃ§in hiperparametre optimizasyonu baÅŸlÄ±yor...\")\n",
        "# Optimizasyon Ã§alÄ±ÅŸmasÄ±nÄ± oluÅŸtur ve baÅŸlat\n",
        "study_ordered = optuna.create_study(direction='maximize')\n",
        "study_ordered.optimize(objective_ordered, n_trials=50) # Deneme sayÄ±sÄ±nÄ± ihtiyaca gÃ¶re ayarlayabilirsiniz\n",
        "\n",
        "# En iyi sonuÃ§larÄ± al\n",
        "best_params_ordered = study_ordered.best_params\n",
        "print(\"En iyi 'ordered' parametreleri bulundu:\", best_params_ordered)\n",
        "print(f\"En iyi validation AUC skoru: {study_ordered.best_value}\")\n",
        "\n",
        "# --- AdÄ±m 5: Nihai Modelleri En Ä°yi Parametrelerle EÄŸitme ---\n",
        "print(\"\\nNihai modeller en iyi bulunan parametrelerle eÄŸitiliyor...\")\n",
        "# Temel sabit parametreler\n",
        "base_params = { \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\", \"verbose\": -1, \"seed\": 42, \"n_jobs\": -1 }\n",
        "callbacks = [lgb.early_stopping(100, verbose=True)]\n",
        "MODEL_DIR = \"models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# 5.1 'ordered' modeli (Optimize EdilmiÅŸ Parametrelerle)\n",
        "final_params_ordered = base_params.copy()\n",
        "final_params_ordered.update(best_params_ordered) # Bulunan en iyi parametreleri ekle\n",
        "ordered_counts = train_pd['ordered'].value_counts()\n",
        "final_params_ordered['scale_pos_weight'] = ordered_counts[0] / ordered_counts[1]\n",
        "lgb_train_ord = lgb.Dataset(train_pd[features], label=train_pd['ordered'], categorical_feature=cat_features)\n",
        "lgb_valid_ord = lgb.Dataset(val_pd[features], label=val_pd['ordered'], categorical_feature=cat_features)\n",
        "model_ordered = lgb.train(final_params_ordered, lgb_train_ord, valid_sets=[lgb_train_ord, lgb_valid_ord], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_ordered.save_model(os.path.join(MODEL_DIR, \"model_ordered_hpo_baseline.txt\"))\n",
        "print(f\"âœ… Optimize 'ordered' modeli '{MODEL_DIR}/model_ordered_hpo_baseline.txt' olarak kaydedildi.\")\n",
        "\n",
        "# 5.2 'clicked' modeli (Standart Parametrelerle)\n",
        "print(\"\\n'clicked' modeli eÄŸitiliyor...\")\n",
        "final_params_clicked = base_params.copy()\n",
        "final_params_clicked.update({'learning_rate': 0.03, 'num_leaves': 64}) # Standart iyi parametreler\n",
        "clicked_counts = train_pd['clicked'].value_counts()\n",
        "final_params_clicked['scale_pos_weight'] = clicked_counts[0] / clicked_counts[1]\n",
        "lgb_train_clk = lgb.Dataset(train_pd[features], label=train_pd['clicked'], categorical_feature=cat_features)\n",
        "lgb_valid_clk = lgb.Dataset(val_pd[features], label=val_pd['clicked'], categorical_feature=cat_features)\n",
        "model_clicked = lgb.train(final_params_clicked, lgb_train_clk, valid_sets=[lgb_train_clk, lgb_valid_clk], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_clicked.save_model(os.path.join(MODEL_DIR, \"model_clicked_standard_baseline.txt\"))\n",
        "print(f\"âœ… 'clicked' modeli '{MODEL_DIR}/model_clicked_standard_baseline.txt' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n--- Optimizasyon ve EÄŸitim Ä°ÅŸlemi TamamlandÄ±! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6r6M86OKhTuY",
        "outputId": "ae0fbffe-b50d-406b-9519-0d89958da842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 1: Baseline Model Ä°Ã§in Optuna ile HPO ve EÄŸitim ---\n",
            "EÄŸitim verisi yÃ¼kleniyor: /content/Trendyol Veri/enriched_train_data_FINAL.parquet\n",
            "Toplam eÄŸitim verisi boyutu: (2773805, 37)\n",
            "EÄŸitim verisi, zamana gÃ¶re sÄ±ralanÄ±p yÃ¼zdesel olarak ayrÄ±lÄ±yor...\n",
            "Train parÃ§asÄ± boyutu: (2357734, 37)\n",
            "Validation parÃ§asÄ± boyutu: (416071, 37)\n",
            "Veri setleri Pandas'a Ã§evriliyor ve model iÃ§in hazÄ±rlanÄ±yor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-10 15:17:21,958] A new study created in memory with name: no-name-c7e75471-104b-45e4-a704-d1eadef2bf47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KullanÄ±lacak Ã¶zellik sayÄ±sÄ±: 23\n",
            "\n",
            "'ordered' modeli iÃ§in hiperparametre optimizasyonu baÅŸlÄ±yor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-10 15:18:00,665] Trial 0 finished with value: 0.7013007336326083 and parameters: {'learning_rate': 0.014947202519089677, 'num_leaves': 73, 'feature_fraction': 0.7584936395519258, 'bagging_fraction': 0.81685896433614, 'bagging_freq': 6, 'lambda_l1': 0.16457017997277235, 'lambda_l2': 0.020673747840117242}. Best is trial 0 with value: 0.7013007336326083.\n",
            "[I 2025-08-10 15:18:48,161] Trial 1 finished with value: 0.7119243462315256 and parameters: {'learning_rate': 0.025261338950755174, 'num_leaves': 40, 'feature_fraction': 0.7889317140090981, 'bagging_fraction': 0.9836379980755368, 'bagging_freq': 3, 'lambda_l1': 0.00026920731374345286, 'lambda_l2': 0.03238193256203181}. Best is trial 1 with value: 0.7119243462315256.\n",
            "[I 2025-08-10 15:19:10,344] Trial 2 finished with value: 0.7043060733532196 and parameters: {'learning_rate': 0.060112489822934954, 'num_leaves': 26, 'feature_fraction': 0.6801168600829408, 'bagging_fraction': 0.6337996810831215, 'bagging_freq': 1, 'lambda_l1': 0.0036206062969729802, 'lambda_l2': 4.964194047232771e-07}. Best is trial 1 with value: 0.7119243462315256.\n",
            "[I 2025-08-10 15:19:31,398] Trial 3 finished with value: 0.6994110335300089 and parameters: {'learning_rate': 0.06648518912786645, 'num_leaves': 73, 'feature_fraction': 0.6436961705535847, 'bagging_fraction': 0.850512026756212, 'bagging_freq': 5, 'lambda_l1': 1.2302638089545537e-07, 'lambda_l2': 3.851355097820139e-05}. Best is trial 1 with value: 0.7119243462315256.\n",
            "[I 2025-08-10 15:20:11,336] Trial 4 finished with value: 0.6909834581995229 and parameters: {'learning_rate': 0.045531297505928124, 'num_leaves': 86, 'feature_fraction': 0.9673989737092311, 'bagging_fraction': 0.7974167431563236, 'bagging_freq': 5, 'lambda_l1': 0.00914161913435502, 'lambda_l2': 1.002853848913955e-06}. Best is trial 1 with value: 0.7119243462315256.\n",
            "[I 2025-08-10 15:20:46,537] Trial 5 finished with value: 0.6812341309313784 and parameters: {'learning_rate': 0.05105736410263445, 'num_leaves': 76, 'feature_fraction': 0.8080264931988437, 'bagging_fraction': 0.7397472947830492, 'bagging_freq': 2, 'lambda_l1': 3.3302432970265814, 'lambda_l2': 0.15120476025349047}. Best is trial 1 with value: 0.7119243462315256.\n",
            "[I 2025-08-10 15:24:22,290] Trial 6 finished with value: 0.7101845956285633 and parameters: {'learning_rate': 0.01605299721097892, 'num_leaves': 84, 'feature_fraction': 0.9547594296411026, 'bagging_fraction': 0.9147402973892569, 'bagging_freq': 2, 'lambda_l1': 2.78491741321844e-05, 'lambda_l2': 0.009573921809314023}. Best is trial 1 with value: 0.7119243462315256.\n",
            "[I 2025-08-10 15:26:10,048] Trial 7 finished with value: 0.7132401251561211 and parameters: {'learning_rate': 0.02466423696325306, 'num_leaves': 32, 'feature_fraction': 0.6505893215458181, 'bagging_fraction': 0.9023703757954353, 'bagging_freq': 1, 'lambda_l1': 6.5835732748459925, 'lambda_l2': 0.00016065926456537927}. Best is trial 7 with value: 0.7132401251561211.\n",
            "[I 2025-08-10 15:30:07,445] Trial 8 finished with value: 0.7202131331951266 and parameters: {'learning_rate': 0.0121399179698488, 'num_leaves': 27, 'feature_fraction': 0.7035905481879955, 'bagging_fraction': 0.7554001825932966, 'bagging_freq': 2, 'lambda_l1': 4.2300720497636693e-08, 'lambda_l2': 3.275962333947313e-08}. Best is trial 8 with value: 0.7202131331951266.\n",
            "[I 2025-08-10 15:30:32,709] Trial 9 finished with value: 0.702530732130137 and parameters: {'learning_rate': 0.05179912759775082, 'num_leaves': 26, 'feature_fraction': 0.88720285160763, 'bagging_fraction': 0.8073725140044434, 'bagging_freq': 6, 'lambda_l1': 4.251824861988653e-07, 'lambda_l2': 0.031442033781253396}. Best is trial 8 with value: 0.7202131331951266.\n",
            "[I 2025-08-10 15:31:05,468] Trial 10 finished with value: 0.7044367950911302 and parameters: {'learning_rate': 0.010833331184176353, 'num_leaves': 50, 'feature_fraction': 0.7281630427422774, 'bagging_fraction': 0.6673590695126174, 'bagging_freq': 4, 'lambda_l1': 3.996454287542767e-06, 'lambda_l2': 1.5067048660399392e-08}. Best is trial 8 with value: 0.7202131331951266.\n",
            "[I 2025-08-10 15:34:00,608] Trial 11 finished with value: 0.7151231498383697 and parameters: {'learning_rate': 0.026450655435820206, 'num_leaves': 41, 'feature_fraction': 0.6058630798134133, 'bagging_fraction': 0.9141149648719081, 'bagging_freq': 1, 'lambda_l1': 2.5065602589073685e-08, 'lambda_l2': 3.614247596813938}. Best is trial 8 with value: 0.7202131331951266.\n",
            "[I 2025-08-10 15:35:14,122] Trial 12 finished with value: 0.7061793332135715 and parameters: {'learning_rate': 0.032166602734370074, 'num_leaves': 51, 'feature_fraction': 0.6314114034744364, 'bagging_fraction': 0.7152529778300574, 'bagging_freq': 2, 'lambda_l1': 1.0135732305076654e-08, 'lambda_l2': 3.6156837763754504}. Best is trial 8 with value: 0.7202131331951266.\n",
            "[I 2025-08-10 15:35:39,900] Trial 13 finished with value: 0.7052803921964734 and parameters: {'learning_rate': 0.09112488500630007, 'num_leaves': 40, 'feature_fraction': 0.61073414061404, 'bagging_fraction': 0.9752051965232567, 'bagging_freq': 3, 'lambda_l1': 9.893330860199337e-08, 'lambda_l2': 1.0388008860145003e-08}. Best is trial 8 with value: 0.7202131331951266.\n",
            "[I 2025-08-10 15:39:11,631] Trial 14 finished with value: 0.7216759686537499 and parameters: {'learning_rate': 0.011071347066905811, 'num_leaves': 20, 'feature_fraction': 0.7138739757223733, 'bagging_fraction': 0.7389156666893876, 'bagging_freq': 1, 'lambda_l1': 1.222238170715288e-08, 'lambda_l2': 6.1507827081461835e-06}. Best is trial 14 with value: 0.7216759686537499.\n",
            "[I 2025-08-10 15:45:24,989] Trial 15 finished with value: 0.7209417412952868 and parameters: {'learning_rate': 0.010331585578846617, 'num_leaves': 20, 'feature_fraction': 0.7067537288640453, 'bagging_fraction': 0.7356998873702343, 'bagging_freq': 3, 'lambda_l1': 1.4708355542339797e-06, 'lambda_l2': 1.7679760534477504e-06}. Best is trial 14 with value: 0.7216759686537499.\n",
            "[W 2025-08-10 15:47:05,024] Trial 16 failed with parameters: {'learning_rate': 0.01804417823067493, 'num_leaves': 100, 'feature_fraction': 0.8398309511807325, 'bagging_fraction': 0.6838769240766708, 'bagging_freq': 4, 'lambda_l1': 2.2936483489974114e-06, 'lambda_l2': 5.93876830896991e-06} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1005364002.py\", line 66, in objective_ordered\n",
            "    model = lgb.train(params, lgb_train, valid_sets=[lgb_valid], valid_names=[\"valid\"], num_boost_round=2000, callbacks=[lgb.early_stopping(50, verbose=False)])\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py\", line 322, in train\n",
            "    booster.update(fobj=fobj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\", line 4155, in update\n",
            "    _LIB.LGBM_BoosterUpdateOneIter(\n",
            "KeyboardInterrupt\n",
            "[W 2025-08-10 15:47:05,031] Trial 16 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1005364002.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m# Optimizasyon Ã§alÄ±ÅŸmasÄ±nÄ± oluÅŸtur ve baÅŸlat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mstudy_ordered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mstudy_ordered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_ordered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Deneme sayÄ±sÄ±nÄ± ihtiyaca gÃ¶re ayarlayabilirsiniz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# En iyi sonuÃ§larÄ± al\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \"\"\"\n\u001b[0;32m--> 489\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     ):\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1005364002.py\u001b[0m in \u001b[0;36mobjective_ordered\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mlgb_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_pd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ordered'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mlgb_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_pd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_pd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ordered'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlgb_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlgb_valid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Optuna, bu dÃ¶ndÃ¼rÃ¼len 'valid's auc' skorunu maksimize etmeye Ã§alÄ±ÅŸacak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    320\u001b[0m             )\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_LGBM_BoosterEvalMethodResultType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4153\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot update due to null objective function.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4154\u001b[0m             _safe_call(\n\u001b[0;32m-> 4155\u001b[0;31m                 _LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   4156\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4157\u001b[0m                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 2** Test Etme"
      ],
      "metadata": {
        "id": "Q-eMfGlhhabl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "# trendyol_metric_group_auc.py dosyasÄ±nÄ±n Colab ortamÄ±nda olduÄŸundan emin olun\n",
        "from trendyol_metric_group_auc import score\n",
        "\n",
        "print(\"--- KOD 2: Tahmin ve Skorlama SÃ¼reci BaÅŸladÄ± ---\")\n",
        "\n",
        "# --- AdÄ±m 1: Gerekli Veri ve KaydedilmiÅŸ Modelleri YÃ¼kleme ---\n",
        "VAL_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "TEST_PATH = \"/content/Trendyol Veri/enriched_test_data_FINAL.parquet\"\n",
        "MODEL_ORDERED_PATH = '/content/models/model_ordered_hpo_baseline.txt'\n",
        "MODEL_CLICKED_PATH = '/content/models/model_clicked_standard_baseline.txt'\n",
        "\n",
        "print(\"Veriler ve kaydedilmiÅŸ modeller yÃ¼kleniyor...\")\n",
        "val_full_df = pl.read_parquet(VAL_PATH)\n",
        "test_df_pl = pl.read_parquet(TEST_PATH)\n",
        "model_ordered = lgb.Booster(model_file=MODEL_ORDERED_PATH)\n",
        "model_clicked = lgb.Booster(model_file=MODEL_CLICKED_PATH)\n",
        "print(\"âœ… Veriler ve modeller baÅŸarÄ±yla yÃ¼klendi.\")\n",
        "\n",
        "# --- AdÄ±m 2: Validation Seti HazÄ±rlama ve Lokal Skor Hesaplama ---\n",
        "print(\"\\nLokal skor iÃ§in validation seti hazÄ±rlanÄ±yor...\")\n",
        "if \"ts_hour\" in val_full_df.columns and val_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    val_full_df = val_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "val_full_df = val_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(val_full_df) * 0.85)\n",
        "val_pd = val_full_df[split_index:].to_pandas().fillna(0)\n",
        "\n",
        "# Ã–zellik listesini eÄŸitimdekiyle tutarlÄ± olacak ÅŸekilde belirle\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features = [col for col in val_pd.columns if col not in exclude_cols]\n",
        "for col in features:\n",
        "    if val_pd[col].dtype == 'object':\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "val_pd['p_order'] = model_ordered.predict(val_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "val_pd['p_click'] = model_clicked.predict(val_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "val_pd['final_score'] = 0.7 * val_pd['p_order'] + 0.3 * val_pd['p_click']\n",
        "\n",
        "# Skor fonksiyonu iÃ§in gerekli DataFrame'leri oluÅŸtur\n",
        "val_solution = val_pd.groupby('session_id').agg(\n",
        "    ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])),\n",
        "    clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])),\n",
        "    all_items=('content_id_hashed', ' '.join)\n",
        ").reset_index()\n",
        "val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n",
        "val_submission.rename(columns={'content_id_hashed': 'prediction'}, inplace=True)\n",
        "\n",
        "try:\n",
        "    local_final_score = score(val_solution, val_submission, 'session_id')\n",
        "    print(\"\\n-------------------------------------------\")\n",
        "    print(f\"ğŸ† LOKAL SKORUNUZ: {local_final_score:.5f}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "except Exception as e:\n",
        "    print(f\"Skor hesaplanÄ±rken bir hata oluÅŸtu: {e}\")\n",
        "\n",
        "# --- AdÄ±m 3: Kaggle iÃ§in Submission DosyasÄ± OluÅŸturma ---\n",
        "print(\"\\nTest verisi hazÄ±rlanÄ±yor ve Kaggle iÃ§in submission dosyasÄ± oluÅŸturuluyor...\")\n",
        "test_pd = test_df_pl.to_pandas().fillna(0)\n",
        "for col in features:\n",
        "    if test_pd[col].dtype == 'object':\n",
        "        test_pd[col] = test_pd[col].astype('category')\n",
        "\n",
        "p_order_test = model_ordered.predict(test_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "p_click_test = model_clicked.predict(test_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "\n",
        "test_df_pl = test_df_pl.with_columns(\n",
        "    final_score=(0.7 * pl.Series(p_order_test)) + (0.3 * pl.Series(p_click_test))\n",
        ")\n",
        "\n",
        "submission_df = test_df_pl.sort([\"session_id\", \"final_score\"], descending=True).group_by(\"session_id\").agg(\n",
        "    pl.col(\"content_id_hashed\").alias(\"prediction\")\n",
        ").with_columns(\n",
        "    pl.col(\"prediction\").list.join(\" \")\n",
        ")\n",
        "\n",
        "# SaÄŸlama kontrolÃ¼\n",
        "expected_rows = 18589\n",
        "actual_rows = submission_df.shape[0]\n",
        "print(f\"\\nOluÅŸturulan submission satÄ±r sayÄ±sÄ±: {actual_rows} (Beklenen: {expected_rows})\")\n",
        "if actual_rows == expected_rows: print(\"âœ… SatÄ±r sayÄ±sÄ± doÄŸru.\")\n",
        "else: print(\"âŒ UYARI: SatÄ±r sayÄ±sÄ± yanlÄ±ÅŸ!\")\n",
        "\n",
        "submission_path = \"submission_final_v4.csv\"\n",
        "submission_df.write_csv(submission_path)\n",
        "print(f\"\\nSubmission dosyasÄ± '{submission_path}' olarak kaydedildi.\")"
      ],
      "metadata": {
        "id": "3yQVA6yMha99"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}