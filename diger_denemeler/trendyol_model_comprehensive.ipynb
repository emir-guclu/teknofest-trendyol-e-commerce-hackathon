{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdbdf27",
   "metadata": {},
   "source": [
    "# Trendyol E-Ticaret Hackathonu 2025 - Kapsamlı Model\n",
    "## Arama Sonuçları Ranking Modeli\n",
    "\n",
    "### Problem Tanımı:\n",
    "- Amaç: Belirli bir kullanıcı, arama terimi ve tarih için her ürünün tıklanma ve sipariş verme olasılığını tahmin etmek\n",
    "- Değerlendirme: Tıklama ve sipariş için ayrı AUC metrikleri (sipariş daha yüksek ağırlık)\n",
    "- Final skor: 0.3 * AUC_click + 0.7 * AUC_order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b7a37",
   "metadata": {},
   "source": [
    "## 1. Kütüphaneler ve Veri Yükleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8352db8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kütüphaneler yüklendi!\n",
      "PyTorch version: 2.8.0+cpu\n",
      "CUDA available: False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch ve Hugging Face kütüphaneleri\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "\n",
    "print(\"Kütüphaneler yüklendi!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff580ef4",
   "metadata": {},
   "source": [
    "## 2. Veri Yükleme ve İnceleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "753a0c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veriler yükleniyor...\n",
      "Train sessions: (2773805, 9)\n",
      "Test sessions: (2988697, 5)\n",
      "User metadata: (38392, 4)\n",
      "User sitewide log: (1891488, 6)\n",
      "User search log: (1142063, 4)\n",
      "Content metadata: (5414049, 10)\n",
      "Content price/rate/review: (1503373, 9)\n",
      "Content search log: (32128237, 4)\n",
      "Content sitewide log: (23977263, 6)\n",
      "\n",
      "Tüm veriler yüklendi!\n",
      "Content metadata: (5414049, 10)\n",
      "Content price/rate/review: (1503373, 9)\n",
      "Content search log: (32128237, 4)\n",
      "Content sitewide log: (23977263, 6)\n",
      "\n",
      "Tüm veriler yüklendi!\n"
     ]
    }
   ],
   "source": [
    "# Veri yolları\n",
    "DATA_PATH = \"trendyol-e-ticaret-hackathonu-2025-kaggle/data/\"\n",
    "\n",
    "print(\"Veriler yükleniyor...\")\n",
    "\n",
    "# Ana veriler\n",
    "train_sessions = pl.read_parquet(f\"{DATA_PATH}train_sessions.parquet\")\n",
    "test_sessions = pl.read_parquet(f\"{DATA_PATH}test_sessions.parquet\")\n",
    "\n",
    "print(f\"Train sessions: {train_sessions.shape}\")\n",
    "print(f\"Test sessions: {test_sessions.shape}\")\n",
    "\n",
    "# Kullanıcı verileri\n",
    "user_metadata = pl.read_parquet(f\"{DATA_PATH}user/metadata.parquet\")\n",
    "user_sitewide_log = pl.read_parquet(f\"{DATA_PATH}user/sitewide_log.parquet\")\n",
    "user_search_log = pl.read_parquet(f\"{DATA_PATH}user/search_log.parquet\")\n",
    "\n",
    "print(f\"User metadata: {user_metadata.shape}\")\n",
    "print(f\"User sitewide log: {user_sitewide_log.shape}\")\n",
    "print(f\"User search log: {user_search_log.shape}\")\n",
    "\n",
    "# İçerik verileri\n",
    "content_metadata = pl.read_parquet(f\"{DATA_PATH}content/metadata.parquet\")\n",
    "content_price_rate_review = pl.read_parquet(f\"{DATA_PATH}content/price_rate_review_data.parquet\")\n",
    "content_search_log = pl.read_parquet(f\"{DATA_PATH}content/search_log.parquet\")\n",
    "content_sitewide_log = pl.read_parquet(f\"{DATA_PATH}content/sitewide_log.parquet\")\n",
    "\n",
    "print(f\"Content metadata: {content_metadata.shape}\")\n",
    "print(f\"Content price/rate/review: {content_price_rate_review.shape}\")\n",
    "print(f\"Content search log: {content_search_log.shape}\")\n",
    "print(f\"Content sitewide log: {content_sitewide_log.shape}\")\n",
    "\n",
    "print(\"\\nTüm veriler yüklendi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba11fb",
   "metadata": {},
   "source": [
    "## 3. Veri Sızıntısını Önlemek İçin Temporal Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e765cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_temporal_data(log_df, session_df, is_train=True):\n",
    "    \"\"\"\n",
    "    Veri sızıntısını önlemek için temporal filtering yapar.\n",
    "    Log verilerinden sadece session zamanından önce olanları alır.\n",
    "    \"\"\"\n",
    "    print(f\"Temporal filtering başlıyor... {'Train' if is_train else 'Test'} için\")\n",
    "    \n",
    "    # Session verilerini user_id ve ts_hour ile groupby yap\n",
    "    session_times = session_df.select([\n",
    "        pl.col(\"user_id_hashed\"),\n",
    "        pl.col(\"ts_hour\")\n",
    "    ]).unique()\n",
    "    \n",
    "    # Log verilerini session verileri ile join et\n",
    "    # Her kullanıcının session zamanından önce olan log verilerini al\n",
    "    filtered_log = (\n",
    "        log_df\n",
    "        .join(\n",
    "            session_times,\n",
    "            on=\"user_id_hashed\",\n",
    "            how=\"inner\",\n",
    "            suffix=\"_session\"\n",
    "        )\n",
    "        .filter(pl.col(\"ts_hour\") < pl.col(\"ts_hour_session\"))  # Sadece session öncesi veriler\n",
    "        .drop(\"ts_hour_session\")\n",
    "    )\n",
    "    \n",
    "    print(f\"Filtreleme tamamlandı. Önceki boyut: {log_df.shape}, Sonraki boyut: {filtered_log.shape}\")\n",
    "    return filtered_log\n",
    "\n",
    "def filter_content_temporal_data(log_df, session_df, is_train=True):\n",
    "    \"\"\"\n",
    "    Content log verileri için temporal filtering\n",
    "    \"\"\"\n",
    "    print(f\"Content temporal filtering başlıyor... {'Train' if is_train else 'Test'} için\")\n",
    "    \n",
    "    # Session verilerini content_id ve ts_hour ile groupby yap\n",
    "    session_times = session_df.select([\n",
    "        pl.col(\"content_id_hashed\"),\n",
    "        pl.col(\"ts_hour\")\n",
    "    ]).unique()\n",
    "    \n",
    "    # Log verilerini session verileri ile join et\n",
    "    filtered_log = (\n",
    "        log_df\n",
    "        .join(\n",
    "            session_times,\n",
    "            on=\"content_id_hashed\",\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        .filter(pl.col(\"date\").dt.replace_time_zone(None) < pl.col(\"ts_hour\").dt.replace_time_zone(None))  # Timezone'ları kaldır\n",
    "        .drop(\"ts_hour\")\n",
    "    )\n",
    "    \n",
    "    print(f\"Content filtreleme tamamlandı. Önceki boyut: {log_df.shape}, Sonraki boyut: {filtered_log.shape}\")\n",
    "    return filtered_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bacd8b8",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering Fonksiyonları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbae5359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_features(user_metadata, user_sitewide_log, user_search_log, session_df, is_train=True):\n",
    "    \"\"\"\n",
    "    Kullanıcı özelliklerini oluşturur\n",
    "    \"\"\"\n",
    "    print(\"Kullanıcı özellikleri oluşturuluyor...\")\n",
    "    \n",
    "    # Temporal filtering uygula\n",
    "    user_sitewide_filtered = filter_temporal_data(user_sitewide_log, session_df, is_train)\n",
    "    user_search_filtered = filter_temporal_data(user_search_log, session_df, is_train)\n",
    "    \n",
    "    # Kullanıcı metadata işleme\n",
    "    user_features = user_metadata.with_columns([\n",
    "        # Yaş hesaplama (2025 - birth_year)\n",
    "        (2025 - pl.col(\"user_birth_year\")).alias(\"user_age\"),\n",
    "        # Gender kategorik encoding\n",
    "        pl.col(\"user_gender\").fill_null(\"UNKNOWN\").alias(\"user_gender_clean\"),\n",
    "        # Tenure kategorilere ayır\n",
    "        pl.when(pl.col(\"user_tenure_in_days\") <= 30).then(pl.lit(\"new\"))\n",
    "        .when(pl.col(\"user_tenure_in_days\") <= 365).then(pl.lit(\"regular\"))\n",
    "        .otherwise(pl.lit(\"loyal\")).alias(\"user_tenure_category\")\n",
    "    ])\n",
    "    \n",
    "    # Kullanıcı sitewide aktivite özellikleri (son zamanlardaki aktivite)\n",
    "    if user_sitewide_filtered.height > 0:\n",
    "        user_sitewide_agg = (\n",
    "            user_sitewide_filtered\n",
    "            .group_by(\"user_id_hashed\")\n",
    "            .agg([\n",
    "                pl.col(\"total_click\").sum().alias(\"user_total_clicks\"),\n",
    "                pl.col(\"total_cart\").sum().alias(\"user_total_cart\"),\n",
    "                pl.col(\"total_fav\").sum().alias(\"user_total_fav\"),\n",
    "                pl.col(\"total_order\").sum().alias(\"user_total_orders\"),\n",
    "                pl.col(\"total_click\").mean().alias(\"user_avg_clicks\"),\n",
    "                pl.col(\"ts_hour\").count().alias(\"user_activity_days\")\n",
    "            ])\n",
    "        )\n",
    "        user_features = user_features.join(user_sitewide_agg, on=\"user_id_hashed\", how=\"left\")\n",
    "    \n",
    "    # Kullanıcı arama aktivite özellikleri\n",
    "    if user_search_filtered.height > 0:\n",
    "        user_search_agg = (\n",
    "            user_search_filtered\n",
    "            .group_by(\"user_id_hashed\")\n",
    "            .agg([\n",
    "                pl.col(\"total_search_impression\").sum().alias(\"user_search_impressions\"),\n",
    "                pl.col(\"total_search_click\").sum().alias(\"user_search_clicks\"),\n",
    "                pl.col(\"total_search_impression\").count().alias(\"user_search_days\"),\n",
    "                # CTR hesaplama\n",
    "                (pl.col(\"total_search_click\").sum() / pl.col(\"total_search_impression\").sum()).alias(\"user_search_ctr\")\n",
    "            ])\n",
    "        )\n",
    "        user_features = user_features.join(user_search_agg, on=\"user_id_hashed\", how=\"left\")\n",
    "    \n",
    "    # NaN değerleri doldur\n",
    "    numeric_cols = [col for col in user_features.columns if col.startswith(\"user_\") and col != \"user_id_hashed\" and col != \"user_gender_clean\" and col != \"user_tenure_category\"]\n",
    "    user_features = user_features.with_columns([\n",
    "        pl.col(col).fill_null(0) for col in numeric_cols\n",
    "    ])\n",
    "    \n",
    "    print(f\"Kullanıcı özellikleri oluşturuldu. Boyut: {user_features.shape}\")\n",
    "    return user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2fb8fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_content_features(content_metadata, content_price_rate_review, content_search_log, content_sitewide_log, session_df, is_train=True):\n",
    "    \"\"\"\n",
    "    İçerik özelliklerini oluşturur\n",
    "    \"\"\"\n",
    "    print(\"İçerik özellikleri oluşturuluyor...\")\n",
    "    \n",
    "    # Temporal filtering uygula\n",
    "    content_search_filtered = filter_content_temporal_data(content_search_log, session_df, is_train)\n",
    "    content_sitewide_filtered = filter_content_temporal_data(content_sitewide_log, session_df, is_train)\n",
    "    \n",
    "    # Content metadata işleme\n",
    "    content_features = content_metadata.with_columns([\n",
    "        # Kategori seviyelerini temizle\n",
    "        pl.col(\"level1_category_name\").fill_null(\"UNKNOWN\").alias(\"level1_category_clean\"),\n",
    "        pl.col(\"level2_category_name\").fill_null(\"UNKNOWN\").alias(\"level2_category_clean\"),\n",
    "        pl.col(\"leaf_category_name\").fill_null(\"UNKNOWN\").alias(\"leaf_category_clean\"),\n",
    "        # İçerik yaşı hesaplama\n",
    "        (pl.date(2025, 7, 12) - pl.col(\"content_creation_date\").dt.date()).dt.total_days().alias(\"content_age_days\"),\n",
    "        # CV tags var mı?\n",
    "        pl.col(\"cv_tags\").is_not_null().alias(\"has_cv_tags\"),\n",
    "        # Numerik özellikleri doldur\n",
    "        pl.col(\"attribute_type_count\").fill_null(0),\n",
    "        pl.col(\"total_attribute_option_count\").fill_null(0),\n",
    "        pl.col(\"merchant_count\").fill_null(1),  # En az 1 merchant olmalı\n",
    "        pl.col(\"filterable_label_count\").fill_null(0)\n",
    "    ])\n",
    "    \n",
    "    # Fiyat ve rating bilgileri - en son bilgiyi al\n",
    "    if content_price_rate_review.height > 0:\n",
    "        latest_price_info = (\n",
    "            content_price_rate_review\n",
    "            .sort([\"content_id_hashed\", \"update_date\"])\n",
    "            .group_by(\"content_id_hashed\")\n",
    "            .last()\n",
    "            .with_columns([\n",
    "                # İndirim oranı hesapla\n",
    "                ((pl.col(\"original_price\") - pl.col(\"selling_price\")) / pl.col(\"original_price\") * 100).alias(\"discount_percentage\"),\n",
    "                # Fiyat seviyesi kategorileri\n",
    "                pl.when(pl.col(\"selling_price\") <= 50).then(pl.lit(\"low\"))\n",
    "                .when(pl.col(\"selling_price\") <= 200).then(pl.lit(\"medium\"))\n",
    "                .when(pl.col(\"selling_price\") <= 500).then(pl.lit(\"high\"))\n",
    "                .otherwise(pl.lit(\"premium\")).alias(\"price_category\"),\n",
    "                # Review yoğunluğu\n",
    "                (pl.col(\"content_review_wth_media_count\") / pl.col(\"content_review_count\")).fill_null(0).alias(\"review_media_ratio\"),\n",
    "                # Rating kategorisi\n",
    "                pl.when(pl.col(\"content_rate_avg\") >= 4.5).then(pl.lit(\"excellent\"))\n",
    "                .when(pl.col(\"content_rate_avg\") >= 4.0).then(pl.lit(\"good\"))\n",
    "                .when(pl.col(\"content_rate_avg\") >= 3.0).then(pl.lit(\"average\"))\n",
    "                .otherwise(pl.lit(\"poor\")).alias(\"rating_category\")\n",
    "            ])\n",
    "        )\n",
    "        content_features = content_features.join(latest_price_info.drop(\"update_date\"), on=\"content_id_hashed\", how=\"left\")\n",
    "    \n",
    "    # Content arama performansı\n",
    "    if content_search_filtered.height > 0:\n",
    "        content_search_agg = (\n",
    "            content_search_filtered\n",
    "            .group_by(\"content_id_hashed\")\n",
    "            .agg([\n",
    "                pl.col(\"total_search_impression\").sum().alias(\"content_search_impressions\"),\n",
    "                pl.col(\"total_search_click\").sum().alias(\"content_search_clicks\"),\n",
    "                (pl.col(\"total_search_click\").sum() / pl.col(\"total_search_impression\").sum()).alias(\"content_search_ctr\"),\n",
    "                pl.col(\"date\").count().alias(\"content_search_days\")\n",
    "            ])\n",
    "        )\n",
    "        content_features = content_features.join(content_search_agg, on=\"content_id_hashed\", how=\"left\")\n",
    "    \n",
    "    # Content site geneli performansı\n",
    "    if content_sitewide_filtered.height > 0:\n",
    "        content_sitewide_agg = (\n",
    "            content_sitewide_filtered\n",
    "            .group_by(\"content_id_hashed\")\n",
    "            .agg([\n",
    "                pl.col(\"total_click\").sum().alias(\"content_total_clicks\"),\n",
    "                pl.col(\"total_cart\").sum().alias(\"content_total_cart\"),\n",
    "                pl.col(\"total_fav\").sum().alias(\"content_total_fav\"),\n",
    "                pl.col(\"total_order\").sum().alias(\"content_total_orders\"),\n",
    "                # Conversion rates\n",
    "                (pl.col(\"total_cart\").sum() / pl.col(\"total_click\").sum()).alias(\"content_cart_rate\"),\n",
    "                (pl.col(\"total_order\").sum() / pl.col(\"total_click\").sum()).alias(\"content_order_rate\"),\n",
    "                pl.col(\"date\").count().alias(\"content_active_days\")\n",
    "            ])\n",
    "        )\n",
    "        content_features = content_features.join(content_sitewide_agg, on=\"content_id_hashed\", how=\"left\")\n",
    "    \n",
    "    # NaN değerleri doldur\n",
    "    numeric_cols = [col for col in content_features.columns \n",
    "                   if col not in [\"content_id_hashed\", \"level1_category_clean\", \"level2_category_clean\", \n",
    "                                \"leaf_category_clean\", \"cv_tags\", \"price_category\", \"rating_category\"] \n",
    "                   and content_features[col].dtype in [pl.Float64, pl.Float32, pl.Int64, pl.Int32]]\n",
    "    \n",
    "    content_features = content_features.with_columns([\n",
    "        pl.col(col).fill_null(0) for col in numeric_cols\n",
    "    ])\n",
    "    \n",
    "    print(f\"İçerik özellikleri oluşturuldu. Boyut: {content_features.shape}\")\n",
    "    return content_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe6e23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_features(session_df, user_features, content_features):\n",
    "    \"\"\"\n",
    "    Kullanıcı-İçerik etkileşim özelliklerini oluşturur\n",
    "    \"\"\"\n",
    "    print(\"Etkileşim özellikleri oluşturuluyor...\")\n",
    "    \n",
    "    # Session verilerini user ve content özellikleri ile birleştir\n",
    "    enriched_data = (\n",
    "        session_df\n",
    "        .join(user_features, on=\"user_id_hashed\", how=\"left\")\n",
    "        .join(content_features, on=\"content_id_hashed\", how=\"left\")\n",
    "    )\n",
    "    \n",
    "    # Zaman özellikleri\n",
    "    enriched_data = enriched_data.with_columns([\n",
    "        pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
    "        pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\"),\n",
    "        pl.col(\"ts_hour\").dt.day().alias(\"day_of_month\"),\n",
    "        # Tatil/hafta sonu\n",
    "        (pl.col(\"ts_hour\").dt.weekday() >= 5).alias(\"is_weekend\"),\n",
    "        # Gün içi saat kategorileri\n",
    "        pl.when(pl.col(\"ts_hour\").dt.hour().is_between(6, 12)).then(pl.lit(\"morning\"))\n",
    "        .when(pl.col(\"ts_hour\").dt.hour().is_between(12, 18)).then(pl.lit(\"afternoon\"))\n",
    "        .when(pl.col(\"ts_hour\").dt.hour().is_between(18, 23)).then(pl.lit(\"evening\"))\n",
    "        .otherwise(pl.lit(\"night\")).alias(\"time_period\")\n",
    "    ])\n",
    "    \n",
    "    # Kullanıcı-kategori uyumu özellikleri\n",
    "    if \"user_gender_clean\" in enriched_data.columns and \"level1_category_clean\" in enriched_data.columns:\n",
    "        enriched_data = enriched_data.with_columns([\n",
    "            # Kadın + moda kategorileri uyumu\n",
    "            ((pl.col(\"user_gender_clean\") == \"Bayan\") & \n",
    "             (pl.col(\"level1_category_clean\").is_in([\"Giyim\", \"Ayakkabı\", \"Aksesuar\"]))).alias(\"gender_category_match\"),\n",
    "            \n",
    "            # Yaş-kategori uyumu\n",
    "            pl.when((pl.col(\"user_age\") <= 30) & (pl.col(\"level2_category_clean\").str.contains(\"Genç|Casual\")))\n",
    "            .then(pl.lit(True))\n",
    "            .when((pl.col(\"user_age\") >= 40) & (pl.col(\"level2_category_clean\").str.contains(\"Klasik|Şık\")))\n",
    "            .then(pl.lit(True))\n",
    "            .otherwise(pl.lit(False)).alias(\"age_category_match\")\n",
    "        ])\n",
    "    \n",
    "    # Arama terimi özellikleri\n",
    "    if \"search_term_normalized\" in enriched_data.columns:\n",
    "        enriched_data = enriched_data.with_columns([\n",
    "            pl.col(\"search_term_normalized\").str.len_chars().alias(\"search_term_length\"),\n",
    "            pl.col(\"search_term_normalized\").str.count_matches(\"_\").alias(\"search_term_word_count\"),\n",
    "            # Marka arama mı?\n",
    "            pl.col(\"search_term_normalized\").str.contains(\"nike|adidas|zara|h&m|mango\").alias(\"is_brand_search\"),\n",
    "            # Renk arama mı?\n",
    "            pl.col(\"search_term_normalized\").str.contains(\"siyah|beyaz|kirmizi|mavi|yesil|sari\").alias(\"is_color_search\")\n",
    "        ])\n",
    "    \n",
    "    # Fiyat-kullanıcı uyumu (eğer fiyat bilgisi varsa)\n",
    "    if \"selling_price\" in enriched_data.columns and \"user_total_orders\" in enriched_data.columns:\n",
    "        enriched_data = enriched_data.with_columns([\n",
    "            # Kullanıcının geçmiş sipariş sayısına göre fiyat toleransı\n",
    "            pl.when(pl.col(\"user_total_orders\") >= 10)\n",
    "            .then(pl.col(\"selling_price\") <= 500)  # Sadık müşteriler daha yüksek fiyat tolere eder\n",
    "            .otherwise(pl.col(\"selling_price\") <= 200).alias(\"price_user_match\")\n",
    "        ])\n",
    "    \n",
    "    print(f\"Etkileşim özellikleri oluşturuldu. Boyut: {enriched_data.shape}\")\n",
    "    return enriched_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01a49ad",
   "metadata": {},
   "source": [
    "## 5. Ana Pipeline - Veri Hazırlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a40768ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN VERİSİ HAZIRLIĞI ===\n",
      "Kullanıcı özellikleri oluşturuluyor...\n",
      "Temporal filtering başlıyor... Train için\n",
      "Filtreleme tamamlandı. Önceki boyut: (1891488, 6), Sonraki boyut: (1034837, 6)\n",
      "Temporal filtering başlıyor... Train için\n",
      "Filtreleme tamamlandı. Önceki boyut: (1142063, 4), Sonraki boyut: (623930, 4)\n",
      "Kullanıcı özellikleri oluşturuldu. Boyut: (38392, 17)\n",
      "İçerik özellikleri oluşturuluyor...\n",
      "Content temporal filtering başlıyor... Train için\n",
      "Content filtreleme tamamlandı. Önceki boyut: (32128237, 4), Sonraki boyut: (29390308, 4)\n",
      "Content temporal filtering başlıyor... Train için\n",
      "Content filtreleme tamamlandı. Önceki boyut: (32128237, 4), Sonraki boyut: (29390308, 4)\n",
      "Content temporal filtering başlıyor... Train için\n",
      "Content filtreleme tamamlandı. Önceki boyut: (23977263, 6), Sonraki boyut: (24784826, 6)\n",
      "Content filtreleme tamamlandı. Önceki boyut: (23977263, 6), Sonraki boyut: (24784826, 6)\n",
      "İçerik özellikleri oluşturuldu. Boyut: (5414049, 37)\n",
      "Etkileşim özellikleri oluşturuluyor...\n",
      "İçerik özellikleri oluşturuldu. Boyut: (5414049, 37)\n",
      "Etkileşim özellikleri oluşturuluyor...\n",
      "Etkileşim özellikleri oluşturuldu. Boyut: (2773805, 73)\n",
      "\n",
      "Train verisi hazır. Boyut: (2773805, 73)\n",
      "Özellik sayısı: 73\n",
      "\n",
      "=== TEST VERİSİ HAZIRLIĞI ===\n",
      "Kullanıcı özellikleri oluşturuluyor...\n",
      "Temporal filtering başlıyor... Test için\n",
      "Filtreleme tamamlandı. Önceki boyut: (1891488, 6), Sonraki boyut: (868024, 6)\n",
      "Temporal filtering başlıyor... Test için\n",
      "Filtreleme tamamlandı. Önceki boyut: (1142063, 4), Sonraki boyut: (525253, 4)\n",
      "Etkileşim özellikleri oluşturuldu. Boyut: (2773805, 73)\n",
      "\n",
      "Train verisi hazır. Boyut: (2773805, 73)\n",
      "Özellik sayısı: 73\n",
      "\n",
      "=== TEST VERİSİ HAZIRLIĞI ===\n",
      "Kullanıcı özellikleri oluşturuluyor...\n",
      "Temporal filtering başlıyor... Test için\n",
      "Filtreleme tamamlandı. Önceki boyut: (1891488, 6), Sonraki boyut: (868024, 6)\n",
      "Temporal filtering başlıyor... Test için\n",
      "Filtreleme tamamlandı. Önceki boyut: (1142063, 4), Sonraki boyut: (525253, 4)\n",
      "Kullanıcı özellikleri oluşturuldu. Boyut: (38392, 17)\n",
      "İçerik özellikleri oluşturuluyor...\n",
      "Content temporal filtering başlıyor... Test için\n",
      "Kullanıcı özellikleri oluşturuldu. Boyut: (38392, 17)\n",
      "İçerik özellikleri oluşturuluyor...\n",
      "Content temporal filtering başlıyor... Test için\n",
      "Content filtreleme tamamlandı. Önceki boyut: (32128237, 4), Sonraki boyut: (21318258, 4)\n",
      "Content temporal filtering başlıyor... Test için\n",
      "Content filtreleme tamamlandı. Önceki boyut: (32128237, 4), Sonraki boyut: (21318258, 4)\n",
      "Content temporal filtering başlıyor... Test için\n",
      "Content filtreleme tamamlandı. Önceki boyut: (23977263, 6), Sonraki boyut: (16791534, 6)\n",
      "Content filtreleme tamamlandı. Önceki boyut: (23977263, 6), Sonraki boyut: (16791534, 6)\n",
      "İçerik özellikleri oluşturuldu. Boyut: (5414049, 37)\n",
      "Etkileşim özellikleri oluşturuluyor...\n",
      "İçerik özellikleri oluşturuldu. Boyut: (5414049, 37)\n",
      "Etkileşim özellikleri oluşturuluyor...\n",
      "Etkileşim özellikleri oluşturuldu. Boyut: (2988697, 69)\n",
      "\n",
      "Test verisi hazır. Boyut: (2988697, 69)\n",
      "Özellik sayısı: 69\n",
      "Etkileşim özellikleri oluşturuldu. Boyut: (2988697, 69)\n",
      "\n",
      "Test verisi hazır. Boyut: (2988697, 69)\n",
      "Özellik sayısı: 69\n"
     ]
    }
   ],
   "source": [
    "# Train verisi için feature engineering\n",
    "print(\"=== TRAIN VERİSİ HAZIRLIĞI ===\")\n",
    "train_user_features = create_user_features(\n",
    "    user_metadata, user_sitewide_log, user_search_log, train_sessions, is_train=True\n",
    ")\n",
    "\n",
    "train_content_features = create_content_features(\n",
    "    content_metadata, content_price_rate_review, content_search_log, \n",
    "    content_sitewide_log, train_sessions, is_train=True\n",
    ")\n",
    "\n",
    "train_enriched = create_interaction_features(\n",
    "    train_sessions, train_user_features, train_content_features\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain verisi hazır. Boyut: {train_enriched.shape}\")\n",
    "print(f\"Özellik sayısı: {len(train_enriched.columns)}\")\n",
    "\n",
    "# Test verisi için feature engineering\n",
    "print(\"\\n=== TEST VERİSİ HAZIRLIĞI ===\")\n",
    "test_user_features = create_user_features(\n",
    "    user_metadata, user_sitewide_log, user_search_log, test_sessions, is_train=False\n",
    ")\n",
    "\n",
    "test_content_features = create_content_features(\n",
    "    content_metadata, content_price_rate_review, content_search_log, \n",
    "    content_sitewide_log, test_sessions, is_train=False\n",
    ")\n",
    "\n",
    "test_enriched = create_interaction_features(\n",
    "    test_sessions, test_user_features, test_content_features\n",
    ")\n",
    "\n",
    "print(f\"\\nTest verisi hazır. Boyut: {test_enriched.shape}\")\n",
    "print(f\"Özellik sayısı: {len(test_enriched.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809bb62a",
   "metadata": {},
   "source": [
    "## 6. Model Hazırlığı ve Eğitim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccb6a19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-Tower model sınıfı tanımlandı!\n"
     ]
    }
   ],
   "source": [
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        # Türkçe BERT modeli\n",
    "        self.bert_model_name = \"dbmdz/bert-base-turkish-cased\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.bert_model_name)\n",
    "        self.bert = AutoModel.from_pretrained(self.bert_model_name)\n",
    "        \n",
    "        # BERT'i dondurmak için (opsiyonel - performans için)\n",
    "        if config.get('freeze_bert', False):\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Embedding boyutları\n",
    "        self.bert_dim = self.bert.config.hidden_size  # 768\n",
    "        self.user_embedding_dim = config['user_embedding_dim']\n",
    "        self.content_embedding_dim = config['content_embedding_dim']\n",
    "        self.tower_dim = config['tower_dim']\n",
    "        \n",
    "        # Kullanıcı ve içerik embedding katmanları\n",
    "        self.user_embedding = nn.Embedding(config['num_users'], self.user_embedding_dim)\n",
    "        self.content_embedding = nn.Embedding(config['num_contents'], self.content_embedding_dim)\n",
    "        \n",
    "        # Sorgu Kulesi (Query Tower)\n",
    "        self.query_tower = nn.Sequential(\n",
    "            nn.Linear(self.bert_dim + self.user_embedding_dim, self.tower_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.tower_dim * 2, self.tower_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.tower_dim, self.tower_dim)\n",
    "        )\n",
    "        \n",
    "        # Ürün Kulesi (Item Tower)  \n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(self.content_embedding_dim, self.tower_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.tower_dim * 2, self.tower_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.tower_dim, self.tower_dim)\n",
    "        )\n",
    "        \n",
    "        # Çıktı başlıkları (Output heads)\n",
    "        self.click_head = nn.Linear(self.tower_dim * 2, 1)  # Query + Item concat\n",
    "        self.order_head = nn.Linear(self.tower_dim * 2, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Weight initialization\"\"\"\n",
    "        # Embedding layers\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.content_embedding.weight)\n",
    "            \n",
    "        # Sequential modules\n",
    "        for module in [self.query_tower, self.item_tower]:\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "        \n",
    "        # Head layers\n",
    "        for layer in [self.click_head, self.order_head]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def encode_query(self, input_ids, attention_mask, user_ids):\n",
    "        \"\"\"Sorgu kulesinde arama terimi + kullanıcı ID'si işlenir\"\"\"\n",
    "        # BERT ile arama terimini encode et\n",
    "        bert_outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        # CLS token'ı al (ilk token)\n",
    "        query_text_vec = bert_outputs.last_hidden_state[:, 0, :]  # [batch_size, 768]\n",
    "        \n",
    "        # Kullanıcı embedding'ini al\n",
    "        user_vec = self.user_embedding(user_ids)  # [batch_size, user_embedding_dim]\n",
    "        \n",
    "        # Birleştir ve sorgu kulesinden geçir\n",
    "        query_input = torch.cat([query_text_vec, user_vec], dim=1)\n",
    "        query_vec = self.query_tower(query_input)  # [batch_size, tower_dim]\n",
    "        \n",
    "        return query_vec\n",
    "    \n",
    "    def encode_item(self, content_ids):\n",
    "        \"\"\"Ürün kulesinde içerik ID'si işlenir\"\"\"\n",
    "        # İçerik embedding'ini al\n",
    "        content_vec = self.content_embedding(content_ids)  # [batch_size, content_embedding_dim]\n",
    "        \n",
    "        # Ürün kulesinden geçir\n",
    "        item_vec = self.item_tower(content_vec)  # [batch_size, tower_dim]\n",
    "        \n",
    "        return item_vec\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, user_ids, content_ids):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # Sorgu vektörü\n",
    "        query_vec = self.encode_query(input_ids, attention_mask, user_ids)\n",
    "        \n",
    "        # Ürün vektörü\n",
    "        item_vec = self.encode_item(content_ids)\n",
    "        \n",
    "        # Vektörleri birleştir\n",
    "        combined_vec = torch.cat([query_vec, item_vec], dim=1)  # [batch_size, tower_dim * 2]\n",
    "        \n",
    "        # Çıktı başlıkları\n",
    "        click_logit = self.click_head(combined_vec)  # [batch_size, 1]\n",
    "        order_logit = self.order_head(combined_vec)  # [batch_size, 1]\n",
    "        \n",
    "        return click_logit.squeeze(-1), order_logit.squeeze(-1)  # [batch_size], [batch_size]\n",
    "\n",
    "\n",
    "class TrendyolDataset(Dataset):\n",
    "    def __init__(self, data_df, tokenizer, user_id_map, content_id_map, max_length=128):\n",
    "        self.data = data_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.user_id_map = user_id_map\n",
    "        self.content_id_map = content_id_map\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Arama terimini tokenize et\n",
    "        search_term = str(row.get('search_term_normalized', ''))\n",
    "        if pd.isna(search_term) or search_term == 'nan':\n",
    "            search_term = ''\n",
    "            \n",
    "        encoding = self.tokenizer(\n",
    "            search_term,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # ID'leri mapping'e çevir\n",
    "        user_id = self.user_id_map.get(row['user_id_hashed'], 0)\n",
    "        content_id = self.content_id_map.get(row['content_id_hashed'], 0)\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'user_id': torch.tensor(user_id, dtype=torch.long),\n",
    "            'content_id': torch.tensor(content_id, dtype=torch.long),\n",
    "        }\n",
    "        \n",
    "        # Eğitim verisi için hedefleri ekle\n",
    "        if 'clicked' in self.data.columns:\n",
    "            item['clicked'] = torch.tensor(row['clicked'], dtype=torch.float)\n",
    "            item['ordered'] = torch.tensor(row['ordered'], dtype=torch.float)\n",
    "        \n",
    "        return item\n",
    "\n",
    "print(\"Two-Tower model sınıfı tanımlandı!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "228755a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eğitim fonksiyonları tanımlandı!\n"
     ]
    }
   ],
   "source": [
    "def create_id_mappings(train_df, test_df):\n",
    "    \"\"\"Kullanıcı ve içerik ID'leri için mapping oluştur\"\"\"\n",
    "    print(\"ID mappings oluşturuluyor...\")\n",
    "    \n",
    "    # Tüm benzersiz ID'leri al\n",
    "    all_user_ids = set(train_df['user_id_hashed'].unique()) | set(test_df['user_id_hashed'].unique())\n",
    "    all_content_ids = set(train_df['content_id_hashed'].unique()) | set(test_df['content_id_hashed'].unique())\n",
    "    \n",
    "    # 0 index'ini boşta bırak (unknown/padding için)\n",
    "    user_id_map = {user_id: idx + 1 for idx, user_id in enumerate(sorted(all_user_ids))}\n",
    "    content_id_map = {content_id: idx + 1 for idx, content_id in enumerate(sorted(all_content_ids))}\n",
    "    \n",
    "    print(f\"Benzersiz kullanıcı sayısı: {len(user_id_map)}\")\n",
    "    print(f\"Benzersiz içerik sayısı: {len(content_id_map)}\")\n",
    "    \n",
    "    return user_id_map, content_id_map\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion_click, criterion_order, device, scaler=None):\n",
    "    \"\"\"Bir epoch eğitimi\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    click_preds, click_targets = [], []\n",
    "    order_preds, order_targets = [], []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # Veriyi device'a taşı\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        user_ids = batch['user_id'].to(device)\n",
    "        content_ids = batch['content_id'].to(device)\n",
    "        click_labels = batch['clicked'].to(device)\n",
    "        order_labels = batch['ordered'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision training\n",
    "        if scaler:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                click_logits, order_logits = model(input_ids, attention_mask, user_ids, content_ids)\n",
    "                click_loss = criterion_click(click_logits, click_labels)\n",
    "                order_loss = criterion_order(order_logits, order_labels)\n",
    "                total_batch_loss = 0.3 * click_loss + 0.7 * order_loss  # Ağırlıklı kayıp\n",
    "            \n",
    "            scaler.scale(total_batch_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            click_logits, order_logits = model(input_ids, attention_mask, user_ids, content_ids)\n",
    "            click_loss = criterion_click(click_logits, click_labels)\n",
    "            order_loss = criterion_order(order_logits, order_labels)\n",
    "            total_batch_loss = 0.3 * click_loss + 0.7 * order_loss\n",
    "            \n",
    "            total_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += total_batch_loss.item()\n",
    "        \n",
    "        # Tahminleri topla (AUC hesabı için)\n",
    "        with torch.no_grad():\n",
    "            click_probs = torch.sigmoid(click_logits).cpu().numpy()\n",
    "            order_probs = torch.sigmoid(order_logits).cpu().numpy()\n",
    "            \n",
    "            click_preds.extend(click_probs)\n",
    "            click_targets.extend(click_labels.cpu().numpy())\n",
    "            order_preds.extend(order_probs)\n",
    "            order_targets.extend(order_labels.cpu().numpy())\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Batch {batch_idx}/{len(dataloader)}, Loss: {total_batch_loss.item():.4f}')\n",
    "    \n",
    "    # AUC hesapla\n",
    "    try:\n",
    "        click_auc = roc_auc_score(click_targets, click_preds)\n",
    "        order_auc = roc_auc_score(order_targets, order_preds)\n",
    "    except:\n",
    "        click_auc = 0.5\n",
    "        order_auc = 0.5\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, click_auc, order_auc\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion_click, criterion_order, device):\n",
    "    \"\"\"Validation epoch\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    click_preds, click_targets = [], []\n",
    "    order_preds, order_targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            user_ids = batch['user_id'].to(device)\n",
    "            content_ids = batch['content_id'].to(device)\n",
    "            click_labels = batch['clicked'].to(device)\n",
    "            order_labels = batch['ordered'].to(device)\n",
    "            \n",
    "            click_logits, order_logits = model(input_ids, attention_mask, user_ids, content_ids)\n",
    "            \n",
    "            click_loss = criterion_click(click_logits, click_labels)\n",
    "            order_loss = criterion_order(order_logits, order_labels)\n",
    "            total_batch_loss = 0.3 * click_loss + 0.7 * order_loss\n",
    "            \n",
    "            total_loss += total_batch_loss.item()\n",
    "            \n",
    "            # Tahminleri topla\n",
    "            click_probs = torch.sigmoid(click_logits).cpu().numpy()\n",
    "            order_probs = torch.sigmoid(order_logits).cpu().numpy()\n",
    "            \n",
    "            click_preds.extend(click_probs)\n",
    "            click_targets.extend(click_labels.cpu().numpy())\n",
    "            order_preds.extend(order_probs)\n",
    "            order_targets.extend(order_labels.cpu().numpy())\n",
    "    \n",
    "    # AUC hesapla\n",
    "    try:\n",
    "        click_auc = roc_auc_score(click_targets, click_preds)\n",
    "        order_auc = roc_auc_score(order_targets, order_preds)\n",
    "    except:\n",
    "        click_auc = 0.5\n",
    "        order_auc = 0.5\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, click_auc, order_auc\n",
    "\n",
    "print(\"Eğitim fonksiyonları tanımlandı!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca7d6bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch veri hazırlama fonksiyonu tanımlandı.\n",
      "\n",
      "=== PYTORCH MODEL VERİ HAZIRLIĞI ===\n",
      "PyTorch model verisi hazırlanıyor... Train\n",
      "Çalışma verisi boyutu: (2773805, 5)\n",
      "Sütunlar: ['user_id_hashed', 'content_id_hashed', 'search_term_normalized', 'clicked', 'ordered']\n",
      "Click dağılımı: {0: 2674602, 1: 99203}\n",
      "Order dağılımı: {0: 2765269, 1: 8536}\n",
      "PyTorch model verisi hazırlanıyor... Test\n",
      "Çalışma verisi boyutu: (2773805, 5)\n",
      "Sütunlar: ['user_id_hashed', 'content_id_hashed', 'search_term_normalized', 'clicked', 'ordered']\n",
      "Click dağılımı: {0: 2674602, 1: 99203}\n",
      "Order dağılımı: {0: 2765269, 1: 8536}\n",
      "PyTorch model verisi hazırlanıyor... Test\n",
      "Çalışma verisi boyutu: (2988697, 4)\n",
      "Sütunlar: ['user_id_hashed', 'content_id_hashed', 'search_term_normalized', 'session_id']\n",
      "Çalışma verisi boyutu: (2988697, 4)\n",
      "Sütunlar: ['user_id_hashed', 'content_id_hashed', 'search_term_normalized', 'session_id']\n"
     ]
    }
   ],
   "source": [
    "def prepare_model_data_pytorch(enriched_df, is_train=True):\n",
    "    \"\"\"PyTorch modeli için veriyi hazırlar\"\"\"\n",
    "    print(f\"PyTorch model verisi hazırlanıyor... {'Train' if is_train else 'Test'}\")\n",
    "    \n",
    "    # Gerekli sütunları seç\n",
    "    required_cols = ['user_id_hashed', 'content_id_hashed', 'search_term_normalized']\n",
    "    if is_train:\n",
    "        required_cols.extend(['clicked', 'ordered'])\n",
    "    else:\n",
    "        required_cols.append('session_id')\n",
    "    \n",
    "    # Mevcut sütunları kontrol et\n",
    "    available_cols = [col for col in required_cols if col in enriched_df.columns]\n",
    "    \n",
    "    # DataFrame'i seç ve pandas'a çevir\n",
    "    df_work = enriched_df.select(available_cols).to_pandas()\n",
    "    \n",
    "    print(f\"Çalışma verisi boyutu: {df_work.shape}\")\n",
    "    print(f\"Sütunlar: {list(df_work.columns)}\")\n",
    "    \n",
    "    # NaN değerleri temizle\n",
    "    df_work['search_term_normalized'] = df_work['search_term_normalized'].fillna('')\n",
    "    \n",
    "    if is_train:\n",
    "        print(f\"Click dağılımı: {df_work['clicked'].value_counts().to_dict()}\")\n",
    "        print(f\"Order dağılımı: {df_work['ordered'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return df_work\n",
    "\n",
    "print(\"PyTorch veri hazırlama fonksiyonu tanımlandı.\")\n",
    "\n",
    "# Veri hazırlama\n",
    "print(\"\\n=== PYTORCH MODEL VERİ HAZIRLIĞI ===\")\n",
    "train_data_pytorch = prepare_model_data_pytorch(train_enriched, is_train=True)\n",
    "test_data_pytorch = prepare_model_data_pytorch(test_enriched, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abca1c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID mappings oluşturuluyor...\n",
      "Benzersiz kullanıcı sayısı: 38392\n",
      "Benzersiz içerik sayısı: 1172785\n",
      "Model config: {'user_embedding_dim': 128, 'content_embedding_dim': 128, 'tower_dim': 256, 'num_users': 38393, 'num_contents': 1172786, 'freeze_bert': False}\n",
      "Benzersiz kullanıcı sayısı: 38392\n",
      "Benzersiz içerik sayısı: 1172785\n",
      "Model config: {'user_embedding_dim': 128, 'content_embedding_dim': 128, 'tower_dim': 256, 'num_users': 38393, 'num_contents': 1172786, 'freeze_bert': False}\n",
      "Model parametre sayısı: 266,568,834\n",
      "Eğitilebilir parametre sayısı: 266,568,834\n",
      "Model parametre sayısı: 266,568,834\n",
      "Eğitilebilir parametre sayısı: 266,568,834\n",
      "Train boyutu: (2219044, 5)\n",
      "Validation boyutu: (554761, 5)\n",
      "Train boyutu: (2219044, 5)\n",
      "Validation boyutu: (554761, 5)\n",
      "Train batches: 69346\n",
      "Val batches: 17337\n",
      "Test batches: 93397\n",
      "Eğitim hazırlıkları tamamlandı!\n",
      "Train batches: 69346\n",
      "Val batches: 17337\n",
      "Test batches: 93397\n",
      "Eğitim hazırlıkları tamamlandı!\n"
     ]
    }
   ],
   "source": [
    "# ID mappings oluştur\n",
    "user_id_map, content_id_map = create_id_mappings(train_data_pytorch, test_data_pytorch)\n",
    "\n",
    "# Model config\n",
    "model_config = {\n",
    "    'user_embedding_dim': 64,  # Küçülttük\n",
    "    'content_embedding_dim': 64,  # Küçülttük\n",
    "    'tower_dim': 128,  # Küçülttük\n",
    "    'num_users': len(user_id_map) + 1,  # +1 for padding\n",
    "    'num_contents': len(content_id_map) + 1,  # +1 for padding\n",
    "    'freeze_bert': True  # BERT'i dondur - hızlandırmak için\n",
    "}\n",
    "\n",
    "print(f\"Model config: {model_config}\")\n",
    "\n",
    "# Model oluştur\n",
    "model = TwoTowerModel(model_config).to(device)\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "print(f\"Model parametre sayısı: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Eğitilebilir parametre sayısı: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Eğitim verisi için daha küçük bir sample al (hızlandırmak için)\n",
    "train_sample = train_data_pytorch.sample(n=100000, random_state=42)  # 100k sample\n",
    "\n",
    "# Train/validation split\n",
    "train_df, val_df = train_test_split(\n",
    "    train_sample,  # Sample kullan\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=train_sample['clicked']\n",
    ")\n",
    "\n",
    "print(f\"Train boyutu: {train_df.shape}\")\n",
    "print(f\"Validation boyutu: {val_df.shape}\")\n",
    "\n",
    "# Test verisi de küçültelim (ilk 10k)\n",
    "test_sample = test_data_pytorch.head(10000)\n",
    "\n",
    "# Dataset oluştur\n",
    "train_dataset = TrendyolDataset(train_df, tokenizer, user_id_map, content_id_map, max_length=64)  # Max length küçülttük\n",
    "val_dataset = TrendyolDataset(val_df, tokenizer, user_id_map, content_id_map, max_length=64)\n",
    "test_dataset = TrendyolDataset(test_sample, tokenizer, user_id_map, content_id_map, max_length=64)\n",
    "\n",
    "# DataLoader oluştur\n",
    "batch_size = 16  # Küçülttük\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)  # num_workers=0 Windows için\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Loss functions ve optimizer\n",
    "criterion_click = nn.BCEWithLogitsLoss()\n",
    "criterion_order = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# AdamW optimizer with warmup\n",
    "optimizer = optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=5e-4, weight_decay=0.01)  # Sadece eğitilebilir parametreler\n",
    "\n",
    "# Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=3, eta_min=1e-6)\n",
    "\n",
    "# Mixed precision training (CPU'da kullanılmayacak)\n",
    "scaler = None  # CPU'da mixed precision yok\n",
    "\n",
    "print(\"Eğitim hazırlıkları tamamlandı!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55de63ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN değerler temizlendi.\n",
      "Final train shape: (2773805, 56)\n",
      "Final test shape: (2988697, 56)\n",
      "\n",
      "=== LIGHTGBM MODEL EĞİTİMİ ===\n",
      "Train split shape: (2219044, 56)\n",
      "Validation split shape: (554761, 56)\n",
      "\n",
      "1. Click modeli eğitiliyor...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's auc: 0.70537\tvalid's auc: 0.69114\n",
      "[200]\ttrain's auc: 0.724852\tvalid's auc: 0.698947\n",
      "[300]\ttrain's auc: 0.740355\tvalid's auc: 0.703547\n",
      "[400]\ttrain's auc: 0.75489\tvalid's auc: 0.708156\n",
      "[500]\ttrain's auc: 0.766745\tvalid's auc: 0.710712\n",
      "[600]\ttrain's auc: 0.778112\tvalid's auc: 0.713393\n",
      "[700]\ttrain's auc: 0.787896\tvalid's auc: 0.714938\n",
      "[800]\ttrain's auc: 0.797007\tvalid's auc: 0.716225\n",
      "[900]\ttrain's auc: 0.805097\tvalid's auc: 0.717238\n",
      "[1000]\ttrain's auc: 0.813035\tvalid's auc: 0.718533\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttrain's auc: 0.813035\tvalid's auc: 0.718533\n",
      "Click Model Validation AUC: 0.718533\n",
      "\n",
      "2. Order modeli eğitiliyor...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's auc: 0.870342\tvalid's auc: 0.758621\n",
      "[200]\ttrain's auc: 0.917004\tvalid's auc: 0.763568\n",
      "Early stopping, best iteration is:\n",
      "[225]\ttrain's auc: 0.924911\tvalid's auc: 0.764514\n",
      "Order Model Validation AUC: 0.764514\n",
      "\n",
      "Combined Score (0.3*click + 0.7*order): 0.750720\n",
      "\n",
      "Model eğitimi tamamlandı!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== PYTORCH TWO-TOWER MODEL EĞİTİMİ ===\")\n",
    "\n",
    "num_epochs = 3  # BERT ile az epoch yeterli\n",
    "best_val_score = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n=== EPOCH {epoch+1}/{num_epochs} ===\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_click_auc, train_order_auc = train_epoch(\n",
    "        model, train_loader, optimizer, criterion_click, criterion_order, device, scaler\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_click_auc, val_order_auc = validate_epoch(\n",
    "        model, val_loader, criterion_click, criterion_order, device\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Combined score hesapla\n",
    "    train_combined = 0.3 * train_click_auc + 0.7 * train_order_auc\n",
    "    val_combined = 0.3 * val_click_auc + 0.7 * val_order_auc\n",
    "    \n",
    "    print(f\"Train - Loss: {train_loss:.4f}, Click AUC: {train_click_auc:.4f}, Order AUC: {train_order_auc:.4f}, Combined: {train_combined:.4f}\")\n",
    "    print(f\"Val   - Loss: {val_loss:.4f}, Click AUC: {val_click_auc:.4f}, Order AUC: {val_order_auc:.4f}, Combined: {val_combined:.4f}\")\n",
    "    print(f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # En iyi modeli kaydet\n",
    "    if val_combined > best_val_score:\n",
    "        best_val_score = val_combined\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        print(f\"✅ En iyi model kaydedildi! Score: {best_val_score:.4f}\")\n",
    "\n",
    "# En iyi modeli yükle\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\n✅ En iyi model yüklendi. Final validation score: {best_val_score:.4f}\")\n",
    "\n",
    "print(\"\\n🎉 Model eğitimi tamamlandı!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ed8a84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST TAHMİNLERİ ===\n",
      "Click tahmin dağılımı: min=0.0002, max=0.8133, mean=0.0350\n",
      "Order tahmin dağılımı: min=0.0000, max=0.3779, mean=0.0024\n",
      "Combined tahmin dağılımı: min=0.0001, max=0.2834, mean=0.0122\n",
      "\n",
      "Submission shape: (2988697, 5)\n",
      "Unique session sayısı: 18589\n",
      "Unique content sayısı: 755886\n",
      "\n",
      "Session bazlı sıralama yapılıyor...\n",
      "Sample submission shape: (18589, 2)\n",
      "Sample submission columns: ['session_id', 'prediction']\n",
      "\n",
      "Final submission saved! Shape: (371780, 2)\n",
      "Final submission head:\n",
      "               session_id        content_id\n",
      "71  test_0001ff614df60933  52fda2c36243ef51\n",
      "83  test_0001ff614df60933  4e1a2bcf236c7f54\n",
      "51  test_0001ff614df60933  5193e92449cb3c31\n",
      "11  test_0001ff614df60933  2dd3e675656a92db\n",
      "66  test_0001ff614df60933  d13c66f775662c28\n",
      "91  test_0001ff614df60933  b1ae3df07f40a185\n",
      "23  test_0001ff614df60933  4725484b4a90c923\n",
      "24  test_0001ff614df60933  6a878e9d33487cbc\n",
      "45  test_0001ff614df60933  445032265c99c830\n",
      "28  test_0001ff614df60933  89b1a5be8f804fe7\n",
      "\n",
      "=== ÖZET ===\n",
      "Validation Click AUC: 0.718533\n",
      "Validation Order AUC: 0.764514\n",
      "Combined Validation Score: 0.750720\n",
      "Test predictions completed and saved to submission_comprehensive.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"=== PYTORCH MODEL TEST TAHMİNLERİ ===\")\n",
    "\n",
    "model.eval()\n",
    "all_click_preds = []\n",
    "all_order_preds = []\n",
    "all_session_ids = []\n",
    "all_content_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Test batch {batch_idx+1}/{len(test_loader)}\")\n",
    "        \n",
    "        # Veriyi device'a taşı\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        user_ids = batch['user_id'].to(device)\n",
    "        content_ids = batch['content_id'].to(device)\n",
    "        \n",
    "        # Tahmin yap\n",
    "        click_logits, order_logits = model(input_ids, attention_mask, user_ids, content_ids)\n",
    "        \n",
    "        # Sigmoid ile probability'ye çevir\n",
    "        click_probs = torch.sigmoid(click_logits).cpu().numpy()\n",
    "        order_probs = torch.sigmoid(order_logits).cpu().numpy()\n",
    "        \n",
    "        all_click_preds.extend(click_probs)\n",
    "        all_order_preds.extend(order_probs)\n",
    "\n",
    "# Test data'dan session ve content ID'lerini al\n",
    "test_session_ids = test_data_pytorch['session_id'].values\n",
    "test_content_ids = test_data_pytorch['content_id_hashed'].values\n",
    "\n",
    "click_pred_test = np.array(all_click_preds)\n",
    "order_pred_test = np.array(all_order_preds)\n",
    "\n",
    "print(f\"Click tahmin dağılımı: min={click_pred_test.min():.4f}, max={click_pred_test.max():.4f}, mean={click_pred_test.mean():.4f}\")\n",
    "print(f\"Order tahmin dağılımı: min={order_pred_test.min():.4f}, max={order_pred_test.max():.4f}, mean={order_pred_test.mean():.4f}\")\n",
    "\n",
    "# Final skor hesaplama (0.3*click + 0.7*order)\n",
    "combined_pred_test = 0.3 * click_pred_test + 0.7 * order_pred_test\n",
    "print(f\"Combined tahmin dağılımı: min={combined_pred_test.min():.4f}, max={combined_pred_test.max():.4f}, mean={combined_pred_test.mean():.4f}\")\n",
    "\n",
    "# Test DataFrame oluştur\n",
    "test_predictions = pd.DataFrame({\n",
    "    'session_id': test_session_ids,\n",
    "    'content_id_hashed': test_content_ids, \n",
    "    'combined_score': combined_pred_test\n",
    "})\n",
    "\n",
    "print(f\"Test predictions shape: {test_predictions.shape}\")\n",
    "print(f\"Unique session sayısı: {test_predictions['session_id'].nunique()}\")\n",
    "\n",
    "# Sample submission kontrol et\n",
    "sample_submission = pd.read_csv('c:/Users/pc/Desktop/trendyol_hekaton/trendyol-e-ticaret-hackathonu-2025-kaggle/data/sample_submission.csv')\n",
    "print(f\"\\nSample submission shape: {sample_submission.shape}\")\n",
    "\n",
    "# Sample'da content sayısı kontrol\n",
    "sample_lengths = sample_submission['prediction'].str.split().str.len()\n",
    "print(f\"Sample'da ortalama content sayısı per session: {sample_lengths.mean():.1f}\")\n",
    "print(f\"Sample'da min-max content sayısı: {sample_lengths.min()}-{sample_lengths.max()}\")\n",
    "\n",
    "print(\"\\nSample formatı örneği:\")\n",
    "print(f\"İlk session: {sample_submission.iloc[0]['session_id']}\")\n",
    "first_prediction = sample_submission.iloc[0]['prediction'].split()\n",
    "print(f\"Content sayısı: {len(first_prediction)}\")\n",
    "print(f\"İlk 5 content: {' '.join(first_prediction[:5])}\")\n",
    "\n",
    "# ÖNEMLİ: Her session için TÜM content'leri sıralayıp tek string yapmalıyız\n",
    "def create_correct_submission(test_df):\n",
    "    \"\"\"\n",
    "    Sample submission formatında doğru submission oluşturur:\n",
    "    - Her session için TÜM content'leri combined_score'a göre sıralar\n",
    "    - Boşlukla ayrılmış tek string yapar\n",
    "    \"\"\"\n",
    "    submission_list = []\n",
    "    \n",
    "    print(\"Her session için content'ler sıralanıyor...\")\n",
    "    unique_sessions = test_df['session_id'].unique()\n",
    "    \n",
    "    for i, session_id in enumerate(unique_sessions):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"İşlenen session: {i+1}/{len(unique_sessions)}\")\n",
    "        \n",
    "        # Bu session'daki tüm content'leri al ve sırala\n",
    "        session_data = test_df[test_df['session_id'] == session_id].sort_values(\n",
    "            'combined_score', ascending=False\n",
    "        )\n",
    "        \n",
    "        # TÜM content_id'leri boşlukla ayırarak birleştir\n",
    "        content_ids_string = ' '.join(session_data['content_id_hashed'].astype(str))\n",
    "        \n",
    "        submission_list.append({\n",
    "            'session_id': session_id,\n",
    "            'prediction': content_ids_string\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(submission_list)\n",
    "\n",
    "# Doğru submission oluştur\n",
    "print(\"\\n=== DOĞRU SUBMISSION OLUŞTURULUYOR ===\")\n",
    "final_submission = create_correct_submission(test_predictions)\n",
    "\n",
    "# Kontrol et\n",
    "our_lengths = final_submission['prediction'].str.split().str.len()\n",
    "print(f\"\\nFinal submission shape: {final_submission.shape}\")\n",
    "print(f\"Bizim submission'da ortalama content sayısı: {our_lengths.mean():.1f}\")\n",
    "print(f\"Bizim submission'da min-max content sayısı: {our_lengths.min()}-{our_lengths.max()}\")\n",
    "\n",
    "# Örnek kontrol\n",
    "print(f\"\\nBizim submission örneği:\")\n",
    "print(f\"İlk session: {final_submission.iloc[0]['session_id']}\")\n",
    "our_first = final_submission.iloc[0]['prediction'].split()\n",
    "print(f\"Content sayısı: {len(our_first)}\")\n",
    "print(f\"İlk 5 content: {' '.join(our_first[:5])}\")\n",
    "\n",
    "# Format kontrolü\n",
    "if final_submission.shape[0] == sample_submission.shape[0]:\n",
    "    print(\"✅ Session sayısı doğru!\")\n",
    "else:\n",
    "    print(f\"❌ Session sayısı hatalı! Bizde: {final_submission.shape[0]}, Sample'da: {sample_submission.shape[0]}\")\n",
    "\n",
    "# Kaydet\n",
    "final_submission.to_csv('c:/Users/pc/Desktop/trendyol_hekaton/submission_final_correct.csv', index=False)\n",
    "print(f\"\\n✅ Doğru submission kaydedildi: submission_final_correct.csv\")\n",
    "\n",
    "print(f\"\\n=== PYTORCH MODEL ÖZETİ ===\")\n",
    "print(f\"✅ Model: Two-Tower with Turkish BERT\")\n",
    "print(f\"✅ Final Validation Score: {best_val_score:.6f}\")\n",
    "print(f\"✅ Test sessions: {final_submission.shape[0]:,}\")\n",
    "print(f\"✅ Total predictions: {len(test_predictions):,}\")\n",
    "print(\"✅ Her session için TÜM content'ler sıralandı ve boşlukla ayrıldı!\")\n",
    "print(\"✅ Turkish BERT (dbmdz/bert-base-turkish-cased) kullanıldı!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e1b52",
   "metadata": {},
   "source": [
    "## 10. Model Kaydetme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8562e1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Two-Tower model eğitimi başlıyor...\n",
      "\n",
      "==================================================\n",
      "EPOCH 1/3\n",
      "==================================================\n",
      "Eğitim başlıyor...\n"
     ]
    }
   ],
   "source": [
    "# MODEL EĞİTİMİNİ BAŞLAT\n",
    "print(\"🚀 Two-Tower model eğitimi başlıyor...\")\n",
    "\n",
    "# En iyi skor takibi\n",
    "best_val_score = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "# Eğitim döngüsü\n",
    "num_epochs = 3  # GPU ile hızlı eğitim için 3 epoch yeterli\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"EPOCH {epoch+1}/{num_epochs}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Eğitim moduna geç\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_click_preds = []\n",
    "    train_click_targets = []\n",
    "    train_order_preds = []  \n",
    "    train_order_targets = []\n",
    "    \n",
    "    print(\"Eğitim başlıyor...\")\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Batch {batch_idx+1}/{len(train_loader)}\")\n",
    "            \n",
    "        # Veriyi device'a taşı\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        user_ids = batch['user_id'].to(device)\n",
    "        content_ids = batch['content_id'].to(device)\n",
    "        clicked = batch['clicked'].to(device)\n",
    "        ordered = batch['ordered'].to(device)\n",
    "        \n",
    "        # Gradientları sıfırla\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        click_logits, order_logits = model(input_ids, attention_mask, user_ids, content_ids)\n",
    "        \n",
    "        # Loss hesapla\n",
    "        click_loss = criterion_click(click_logits, clicked)\n",
    "        order_loss = criterion_order(order_logits, ordered)\n",
    "        total_loss = click_loss + order_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # İstatistikleri kaydet\n",
    "        train_loss += total_loss.item()\n",
    "        \n",
    "        # Tahminleri sigmoid'e geçir\n",
    "        click_probs = torch.sigmoid(click_logits).detach().cpu().numpy()\n",
    "        order_probs = torch.sigmoid(order_logits).detach().cpu().numpy()\n",
    "        \n",
    "        train_click_preds.extend(click_probs)\n",
    "        train_click_targets.extend(clicked.detach().cpu().numpy())\n",
    "        train_order_preds.extend(order_probs)\n",
    "        train_order_targets.extend(ordered.detach().cpu().numpy())\n",
    "    \n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Eğitim metrikleri\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    train_click_auc = roc_auc_score(train_click_targets, train_click_preds)\n",
    "    train_order_auc = roc_auc_score(train_order_targets, train_order_preds)\n",
    "    train_combined = 0.3 * train_click_auc + 0.7 * train_order_auc\n",
    "    \n",
    "    # Validation\n",
    "    print(\"Validation başlıyor...\")\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_click_preds = []\n",
    "    val_click_targets = []\n",
    "    val_order_preds = []\n",
    "    val_order_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            # Veriyi device'a taşı\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            user_ids = batch['user_id'].to(device)\n",
    "            content_ids = batch['content_id'].to(device)\n",
    "            clicked = batch['clicked'].to(device)\n",
    "            ordered = batch['ordered'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            click_logits, order_logits = model(input_ids, attention_mask, user_ids, content_ids)\n",
    "            \n",
    "            # Loss hesapla\n",
    "            click_loss = criterion_click(click_logits, clicked)\n",
    "            order_loss = criterion_order(order_logits, ordered)\n",
    "            total_loss = click_loss + order_loss\n",
    "            \n",
    "            val_loss += total_loss.item()\n",
    "            \n",
    "            # Tahminleri sigmoid'e geçir\n",
    "            click_probs = torch.sigmoid(click_logits).cpu().numpy()\n",
    "            order_probs = torch.sigmoid(order_logits).cpu().numpy()\n",
    "            \n",
    "            val_click_preds.extend(click_probs)\n",
    "            val_click_targets.extend(clicked.cpu().numpy())\n",
    "            val_order_preds.extend(order_probs)\n",
    "            val_order_targets.extend(ordered.cpu().numpy())\n",
    "    \n",
    "    # Validation metrikleri\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_click_auc = roc_auc_score(val_click_targets, val_click_preds)\n",
    "    val_order_auc = roc_auc_score(val_order_targets, val_order_preds)\n",
    "    val_combined = 0.3 * val_click_auc + 0.7 * val_order_auc\n",
    "    \n",
    "    # Sonuçları yazdır\n",
    "    print(f\"Train - Loss: {train_loss:.4f}, Click AUC: {train_click_auc:.4f}, Order AUC: {train_order_auc:.4f}, Combined: {train_combined:.4f}\")\n",
    "    print(f\"Val   - Loss: {val_loss:.4f}, Click AUC: {val_click_auc:.4f}, Order AUC: {val_order_auc:.4f}, Combined: {val_combined:.4f}\")\n",
    "    print(f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # En iyi modeli kaydet\n",
    "    if val_combined > best_val_score:\n",
    "        best_val_score = val_combined\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        print(f\"✅ En iyi model kaydedildi! Score: {best_val_score:.4f}\")\n",
    "\n",
    "# En iyi modeli yükle\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\n✅ En iyi model yüklendi. Final validation score: {best_val_score:.4f}\")\n",
    "\n",
    "print(\"\\n🎉 Model eğitimi tamamlandı!\")\n",
    "\n",
    "# Model checkpointing\n",
    "print(\"PyTorch Two-Tower modeli kaydediliyor...\")\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': model_config,\n",
    "    'user_id_map': user_id_map,\n",
    "    'content_id_map': content_id_map,\n",
    "    'best_val_score': best_val_score,\n",
    "    'tokenizer_name': model.bert_model_name\n",
    "}, 'c:/Users/pc/Desktop/trendyol_hekaton/two_tower_model.pt')\n",
    "\n",
    "print(f\"\\n📊 Final PyTorch Two-Tower Model Performance:\")\n",
    "print(f\"Architecture: Two-Tower with Turkish BERT\")\n",
    "print(f\"BERT Model: dbmdz/bert-base-turkish-cased\")\n",
    "print(f\"Final Validation Score: {best_val_score:.6f}\")\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "print(\"\\n🚀 İki kuleli mimari başarıyla oluşturuldu ve eğitildi!\")\n",
    "print(\"📝 Sorgu kulesi: BERT + Kullanıcı Embedding\")\n",
    "print(\"🛍️  Ürün kulesi: İçerik Embedding\")  \n",
    "print(\"🎯 Çıktı başlıkları: Tıklama + Sipariş tahminleri\")\n",
    "\n",
    "# Model inference fonksiyonu tanımla\n",
    "def load_trained_model(checkpoint_path):\n",
    "    \"\"\"Eğitilmiş modeli yükle\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    loaded_model = TwoTowerModel(checkpoint['model_config']).to(device)\n",
    "    loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loaded_model.eval()\n",
    "    \n",
    "    return loaded_model, checkpoint['user_id_map'], checkpoint['content_id_map']\n",
    "\n",
    "print(\"\\n💾 Model yükleme fonksiyonu tanımlandı: load_trained_model()\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
