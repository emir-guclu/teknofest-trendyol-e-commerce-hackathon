{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e9e636",
   "metadata": {},
   "source": [
    "# Data Preparation for Teknofest Trendyol E-Commerce-Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf58024",
   "metadata": {},
   "source": [
    "**Step 1 : Create some base features by using raw data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef671da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "NUM_THREADS = 4\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "def _print(msg: str):\n",
    "    print(f\"[buid_dataset] {msg}\")\n",
    "\n",
    "\n",
    "def read_parquet(path: Path, columns: Optional[List[str]] = None, fast_limit: Optional[int] = None) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_parquet(path, engine=\"pyarrow\", columns=columns)\n",
    "    except Exception:\n",
    "        df = pd.read_parquet(path, columns=columns)\n",
    "    if fast_limit is not None and len(df) > fast_limit:\n",
    "        return df.iloc[: fast_limit].copy()\n",
    "    return df\n",
    "\n",
    "\n",
    "def agg_sum_rate(df: pd.DataFrame, num_col: str, den_col: str, out_rate: str, alpha: float = 1.0) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[out_rate] = (out[num_col] + alpha) / (out[den_col] + alpha)\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_content_static(root: Path, fast: bool = False) -> pd.DataFrame:\n",
    "    p = root / \"data\" / \"content\" / \"metadata.parquet\"\n",
    "    use_cols = [\n",
    "        \"content_id_hashed\",\n",
    "        \"leaf_category_name\",\n",
    "        \"merchant_count\",\n",
    "        \"attribute_type_count\",\n",
    "        \"total_attribute_option_count\",\n",
    "        \"filterable_label_count\",\n",
    "    ]\n",
    "    df = read_parquet(p, columns=use_cols)\n",
    "    for c in [\"merchant_count\", \"attribute_type_count\", \"total_attribute_option_count\", \"filterable_label_count\"]:\n",
    "        if c in df:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    if \"leaf_category_name\" in df:\n",
    "        df[\"leaf_category_name\"] = df[\"leaf_category_name\"].astype(\"string\").fillna(\"UNKNOWN\")\n",
    "    return df.drop_duplicates(\"content_id_hashed\")\n",
    "\n",
    "\n",
    "# ------------------------- Time-aware aggregates ------------------------------\n",
    "\n",
    "def _ensure_ts_hour(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    ts = pd.to_datetime(df[col], utc=True, errors=\"coerce\").dt.floor(\"h\")\n",
    "    df[col] = ts\n",
    "    # integer nanoseconds since epoch, stable key for asof joins\n",
    "    df[\"ts_hour_ns\"] = ts.astype(\"int64\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_content_price_timeaware(root: Path, fast: bool = False) -> pd.DataFrame:\n",
    "    p = root / \"data\" / \"content\" / \"price_rate_review_data.parquet\"\n",
    "    use_cols = [\n",
    "        \"content_id_hashed\",\n",
    "        \"update_date\",\n",
    "        \"selling_price\",\n",
    "        \"content_review_count\",\n",
    "        \"content_rate_count\",\n",
    "        \"content_rate_avg\",\n",
    "    ]\n",
    "    df = read_parquet(p, columns=use_cols, fast_limit=400_000 if fast else None)\n",
    "    df = df.rename(columns={\"update_date\": \"ts_hour\"})\n",
    "    df = _ensure_ts_hour(df, \"ts_hour\")\n",
    "    for c in [\"selling_price\", \"content_review_count\", \"content_rate_count\", \"content_rate_avg\"]:\n",
    "        if c in df:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    # Keep all rows for as-of merge (last known snapshot before ts_hour)\n",
    "    df = df.sort_values([\"content_id_hashed\", \"ts_hour\"])  # needed for merge_asof\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_content_sitewide_timeaware(root: Path, fast: bool = False) -> pd.DataFrame:\n",
    "    p = root / \"data\" / \"content\" / \"sitewide_log.parquet\"\n",
    "    use_cols = [\"content_id_hashed\", \"date\", \"total_click\", \"total_cart\", \"total_fav\", \"total_order\"]\n",
    "    df = read_parquet(p, columns=use_cols, fast_limit=2_000_000 if fast else None)\n",
    "    df = df.rename(columns={\"date\": \"ts_hour\"})\n",
    "    df = _ensure_ts_hour(df, \"ts_hour\")\n",
    "    # Aggregate per key-time then cumulative per key\n",
    "    g = df.groupby([\"content_id_hashed\", \"ts_hour\", \"ts_hour_ns\"], as_index=False).sum(numeric_only=True)\n",
    "    g = g.sort_values([\"content_id_hashed\", \"ts_hour_ns\"])  # for cumsum/asof\n",
    "    for col in [\"total_click\", \"total_cart\", \"total_fav\", \"total_order\"]:\n",
    "        g[f\"cum_{col}\"] = g.groupby(\"content_id_hashed\")[col].cumsum()\n",
    "    # Derive rates from cumulative sums\n",
    "    out = g[[\"content_id_hashed\", \"ts_hour\", \"ts_hour_ns\", \"cum_total_click\", \"cum_total_cart\", \"cum_total_fav\", \"cum_total_order\"]].copy()\n",
    "    out = agg_sum_rate(out, \"cum_total_order\", \"cum_total_click\", \"content_sitewide_avg_order_rate\", alpha=1.0)\n",
    "    out = agg_sum_rate(out, \"cum_total_cart\", \"cum_total_click\", \"content_sitewide_avg_cart_rate\", alpha=1.0)\n",
    "    out = agg_sum_rate(out, \"cum_total_fav\", \"cum_total_click\", \"content_sitewide_avg_fav_rate\", alpha=1.0)\n",
    "    # Rename cumulative totals with prefixes\n",
    "    out = out.rename(\n",
    "        columns={\n",
    "            \"cum_total_click\": \"content_sw_total_click\",\n",
    "            \"cum_total_cart\": \"content_sw_total_cart\",\n",
    "            \"cum_total_fav\": \"content_sw_total_fav\",\n",
    "            \"cum_total_order\": \"content_sw_total_order\",\n",
    "        }\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_content_search_timeaware(root: Path, fast: bool = False) -> pd.DataFrame:\n",
    "    p = root / \"data\" / \"content\" / \"search_log.parquet\"\n",
    "    use_cols = [\"content_id_hashed\", \"date\", \"total_search_impression\", \"total_search_click\"]\n",
    "    df = read_parquet(p, columns=use_cols, fast_limit=2_000_000 if fast else None)\n",
    "    df = df.rename(columns={\"date\": \"ts_hour\"})\n",
    "    df = _ensure_ts_hour(df, \"ts_hour\")\n",
    "    g = df.groupby([\"content_id_hashed\", \"ts_hour\", \"ts_hour_ns\"], as_index=False).sum(numeric_only=True)\n",
    "    g = g.sort_values([\"content_id_hashed\", \"ts_hour_ns\"])  # for cumsum/asof\n",
    "    for col in [\"total_search_impression\", \"total_search_click\"]:\n",
    "        g[f\"cum_{col}\"] = g.groupby(\"content_id_hashed\")[col].cumsum()\n",
    "    out = g[[\"content_id_hashed\", \"ts_hour\", \"ts_hour_ns\", \"cum_total_search_impression\", \"cum_total_search_click\"]].copy()\n",
    "    out = out.rename(\n",
    "        columns={\n",
    "            \"cum_total_search_impression\": \"content_search_total_impression\",\n",
    "            \"cum_total_search_click\": \"content_search_total_click\",\n",
    "        }\n",
    "    )\n",
    "    out = agg_sum_rate(out, \"content_search_total_click\", \"content_search_total_impression\", \"content_search_ctr\", alpha=1.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_term_content_timeaware(root: Path, fast: bool = False) -> pd.DataFrame:\n",
    "    p = root / \"data\" / \"content\" / \"top_terms_log.parquet\"\n",
    "    use_cols = [\"content_id_hashed\", \"search_term_normalized\", \"date\", \"total_search_impression\", \"total_search_click\"]\n",
    "    df = read_parquet(p, columns=use_cols, fast_limit=2_000_000 if fast else None)\n",
    "    df = df.rename(columns={\"date\": \"ts_hour\"})\n",
    "    df = _ensure_ts_hour(df, \"ts_hour\")\n",
    "    df[\"search_term_normalized\"] = df[\"search_term_normalized\"].astype(\"string\")\n",
    "    g = df.groupby([\"content_id_hashed\", \"search_term_normalized\", \"ts_hour\", \"ts_hour_ns\"], as_index=False).sum(numeric_only=True)\n",
    "    g = g.sort_values([\"content_id_hashed\", \"search_term_normalized\", \"ts_hour_ns\"])  # for cumsum/asof\n",
    "    g[\"cum_impr\"] = g.groupby([\"content_id_hashed\", \"search_term_normalized\"]) [\"total_search_impression\"].cumsum()\n",
    "    g[\"cum_click\"] = g.groupby([\"content_id_hashed\", \"search_term_normalized\"]) [\"total_search_click\"].cumsum()\n",
    "    out = g[[\"content_id_hashed\", \"search_term_normalized\", \"ts_hour\", \"ts_hour_ns\", \"cum_impr\", \"cum_click\"]].copy()\n",
    "    out = out.rename(columns={\"cum_impr\": \"term_content_total_impression\", \"cum_click\": \"term_content_total_click\"})\n",
    "    out = agg_sum_rate(out, \"term_content_total_click\", \"term_content_total_impression\", \"term_content_search_ctr\", alpha=1.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_user_metadata(root: Path, fast: bool = False) -> pd.DataFrame:\n",
    "    p = root / \"data\" / \"user\" / \"metadata.parquet\"\n",
    "    use_cols = [\"user_id_hashed\", \"user_gender\", \"user_birth_year\", \"user_tenure_in_days\"]\n",
    "    df = read_parquet(p, columns=use_cols)\n",
    "    df[\"user_gender\"] = df[\"user_gender\"].astype(\"string\").fillna(\"UNKNOWN\")\n",
    "    df[\"user_birth_year\"] = pd.to_numeric(df[\"user_birth_year\"], errors=\"coerce\")\n",
    "    df[\"user_tenure_in_days\"] = pd.to_numeric(df[\"user_tenure_in_days\"], errors=\"coerce\")\n",
    "    return df.drop_duplicates(\"user_id_hashed\")\n",
    "\n",
    "\n",
    "def compute_user_sitewide_timeaware(root: Path, fast: bool = False) -> pd.DataFrame:\n",
    "    p = root / \"data\" / \"user\" / \"sitewide_log.parquet\"\n",
    "    use_cols = [\"user_id_hashed\", \"ts_hour\", \"total_click\", \"total_cart\", \"total_fav\", \"total_order\"]\n",
    "    df = read_parquet(p, columns=use_cols, fast_limit=1_000_000 if fast else None)\n",
    "    df = _ensure_ts_hour(df, \"ts_hour\")\n",
    "    g = df.groupby([\"user_id_hashed\", \"ts_hour\", \"ts_hour_ns\"], as_index=False).sum(numeric_only=True)\n",
    "    g = g.sort_values([\"user_id_hashed\", \"ts_hour_ns\"])  # for cumsum/asof\n",
    "    for col in [\"total_click\", \"total_cart\", \"total_fav\", \"total_order\"]:\n",
    "        g[f\"cum_{col}\"] = g.groupby(\"user_id_hashed\")[col].cumsum()\n",
    "    out = g[[\"user_id_hashed\", \"ts_hour\", \"ts_hour_ns\", \"cum_total_click\", \"cum_total_cart\", \"cum_total_fav\", \"cum_total_order\"]].copy()\n",
    "    out = agg_sum_rate(out, \"cum_total_order\", \"cum_total_click\", \"user_sitewide_avg_order_rate\", alpha=1.0)\n",
    "    out = agg_sum_rate(out, \"cum_total_cart\", \"cum_total_click\", \"user_sitewide_avg_cart_rate\", alpha=1.0)\n",
    "    out = agg_sum_rate(out, \"cum_total_fav\", \"cum_total_click\", \"user_sitewide_avg_fav_rate\", alpha=1.0)\n",
    "    out = out.rename(\n",
    "        columns={\n",
    "            \"cum_total_click\": \"user_sw_total_click\",\n",
    "            \"cum_total_cart\": \"user_sw_total_cart\",\n",
    "            \"cum_total_fav\": \"user_sw_total_fav\",\n",
    "            \"cum_total_order\": \"user_sw_total_order\",\n",
    "        }\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_user_search_timeaware(root: Path, fast: bool = False) -> pd.DataFrame:\n",
    "    p = root / \"data\" / \"user\" / \"search_log.parquet\"\n",
    "    use_cols = [\"user_id_hashed\", \"ts_hour\", \"total_search_impression\", \"total_search_click\"]\n",
    "    df = read_parquet(p, columns=use_cols, fast_limit=500_000 if fast else None)\n",
    "    df = _ensure_ts_hour(df, \"ts_hour\")\n",
    "    g = df.groupby([\"user_id_hashed\", \"ts_hour\", \"ts_hour_ns\"], as_index=False).sum(numeric_only=True)\n",
    "    g = g.sort_values([\"user_id_hashed\", \"ts_hour_ns\"])  # for cumsum/asof\n",
    "    g[\"cum_impr\"] = g.groupby(\"user_id_hashed\")[\"total_search_impression\"].cumsum()\n",
    "    g[\"cum_click\"] = g.groupby(\"user_id_hashed\")[\"total_search_click\"].cumsum()\n",
    "    out = g[[\"user_id_hashed\", \"ts_hour\", \"ts_hour_ns\", \"cum_impr\", \"cum_click\"]].copy()\n",
    "    out = out.rename(columns={\"cum_impr\": \"user_search_total_impression\", \"cum_click\": \"user_search_total_click\"})\n",
    "    out = agg_sum_rate(out, \"user_search_total_click\", \"user_search_total_impression\", \"user_search_ctr\", alpha=1.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_term_search_timeaware(root: Path, fast: bool = False) -> pd.DataFrame:\n",
    "    p = root / \"data\" / \"term\" / \"search_log.parquet\"\n",
    "    use_cols = [\"search_term_normalized\", \"ts_hour\", \"total_search_impression\", \"total_search_click\"]\n",
    "    df = read_parquet(p, columns=use_cols, fast_limit=400_000 if fast else None)\n",
    "    df = _ensure_ts_hour(df, \"ts_hour\")\n",
    "    df[\"search_term_normalized\"] = df[\"search_term_normalized\"].astype(\"string\")\n",
    "    g = df.groupby([\"search_term_normalized\", \"ts_hour\", \"ts_hour_ns\"], as_index=False).sum(numeric_only=True)\n",
    "    g = g.sort_values([\"search_term_normalized\", \"ts_hour_ns\"])  # for cumsum/asof\n",
    "    g[\"cum_impr\"] = g.groupby(\"search_term_normalized\")[\"total_search_impression\"].cumsum()\n",
    "    g[\"cum_click\"] = g.groupby(\"search_term_normalized\")[\"total_search_click\"].cumsum()\n",
    "    out = g[[\"search_term_normalized\", \"ts_hour\", \"ts_hour_ns\", \"cum_impr\", \"cum_click\"]].copy()\n",
    "    out = out.rename(columns={\"cum_impr\": \"term_search_total_impression\", \"cum_click\": \"term_search_total_click\"})\n",
    "    out = agg_sum_rate(out, \"term_search_total_click\", \"term_search_total_impression\", \"term_search_ctr\", alpha=1.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------- Sessions load ----------------------------------\n",
    "\n",
    "def load_sessions(root: Path, train: bool, debug_n: Optional[int] = None) -> pd.DataFrame:\n",
    "    p = root / \"data\" / (\"train_sessions.parquet\" if train else \"test_sessions.parquet\")\n",
    "    cols = [\n",
    "        \"ts_hour\",\n",
    "        \"search_term_normalized\",\n",
    "        \"user_id_hashed\",\n",
    "        \"content_id_hashed\",\n",
    "        \"session_id\",\n",
    "    ]\n",
    "    if train:\n",
    "        cols += [\"clicked\", \"ordered\"]\n",
    "    df = read_parquet(p, columns=cols)\n",
    "    if debug_n is not None and len(df) > debug_n:\n",
    "        first_sessions = df[\"session_id\"].drop_duplicates().iloc[: max(1, debug_n // 50)]\n",
    "        df = df[df[\"session_id\"].isin(first_sessions)].copy()\n",
    "    df[\"ts_hour\"] = pd.to_datetime(df[\"ts_hour\"], utc=True, errors=\"coerce\").dt.floor(\"h\")\n",
    "    df[\"search_term_normalized\"] = df[\"search_term_normalized\"].astype(\"string\")\n",
    "    df[\"hour_of_day\"] = df[\"ts_hour\"].dt.hour\n",
    "    df[\"day_of_week\"] = df[\"ts_hour\"].dt.dayofweek\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_enriched(root: Path, outdir: Path, debug_n: Optional[int] = None, fast: bool = False) -> Tuple[Path, Path, List[str]]:\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    _print(\"Loading sessionsâ€¦\")\n",
    "    train_df = load_sessions(root, train=True, debug_n=debug_n)\n",
    "    test_df = load_sessions(root, train=False, debug_n=debug_n)\n",
    "    base_cols = [\n",
    "        \"session_id\",\n",
    "        \"ts_hour\",\n",
    "        \"search_term_normalized\",\n",
    "        \"user_id_hashed\",\n",
    "        \"content_id_hashed\",\n",
    "        \"hour_of_day\",\n",
    "        \"day_of_week\",\n",
    "    ]\n",
    "\n",
    "    _print(\"Computing time-aware aggregatesâ€¦ (fast=%s)\" % fast)\n",
    "    content_static = compute_content_static(root, fast)\n",
    "    content_price_t = compute_content_price_timeaware(root, fast)\n",
    "    content_sw_t = compute_content_sitewide_timeaware(root, fast)\n",
    "    content_search_t = compute_content_search_timeaware(root, fast)\n",
    "    term_content_t = compute_term_content_timeaware(root, fast)\n",
    "    user_meta = compute_user_metadata(root, fast)\n",
    "    user_sw_t = compute_user_sitewide_timeaware(root, fast)\n",
    "    user_search_t = compute_user_search_timeaware(root, fast)\n",
    "    term_search_t = compute_term_search_timeaware(root, fast)\n",
    "\n",
    "    def join_all(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Keep only clicked/ordered as targets in train; never include added_to_cart/fav\n",
    "        keep = base_cols + ([\"clicked\", \"ordered\"] if \"clicked\" in df.columns else [])\n",
    "        X = df[keep].copy()\n",
    "        # Internal numeric hour key for robust as-of merges (not written to output)\n",
    "        X[\"ts_hour_ns\"] = X[\"ts_hour\"].astype(\"int64\")\n",
    "        # Regular equals joins (static)\n",
    "        X = X.merge(content_static, on=\"content_id_hashed\", how=\"left\")\n",
    "        X = X.merge(user_meta, on=\"user_id_hashed\", how=\"left\")\n",
    "        # As-of joins (time-aware, no leakage)\n",
    "        X = pd.merge_asof(\n",
    "            X.sort_values([\"ts_hour_ns\"]),\n",
    "            content_price_t.drop(columns=[\"ts_hour\"], errors=\"ignore\").sort_values([\"ts_hour_ns\"]),\n",
    "            by=\"content_id_hashed\",\n",
    "            on=\"ts_hour_ns\",\n",
    "            direction=\"backward\",\n",
    "        )\n",
    "        X = pd.merge_asof(\n",
    "            X.sort_values([\"ts_hour_ns\"]),\n",
    "            content_sw_t.drop(columns=[\"ts_hour\"], errors=\"ignore\").sort_values([\"ts_hour_ns\"]),\n",
    "            by=\"content_id_hashed\",\n",
    "            on=\"ts_hour_ns\",\n",
    "            direction=\"backward\",\n",
    "        )\n",
    "        X = pd.merge_asof(\n",
    "            X.sort_values([\"ts_hour_ns\"]),\n",
    "            content_search_t.drop(columns=[\"ts_hour\"], errors=\"ignore\").sort_values([\"ts_hour_ns\"]),\n",
    "            by=\"content_id_hashed\",\n",
    "            on=\"ts_hour_ns\",\n",
    "            direction=\"backward\",\n",
    "        )\n",
    "        # content-term\n",
    "        X = pd.merge_asof(\n",
    "            X.sort_values([\"ts_hour_ns\"]),\n",
    "            term_content_t.drop(columns=[\"ts_hour\"], errors=\"ignore\").sort_values([\"ts_hour_ns\"]),\n",
    "            by=[\"content_id_hashed\", \"search_term_normalized\"],\n",
    "            on=\"ts_hour_ns\",\n",
    "            direction=\"backward\",\n",
    "        )\n",
    "        # user side\n",
    "        X = pd.merge_asof(\n",
    "            X.sort_values([\"ts_hour_ns\"]),\n",
    "            user_sw_t.drop(columns=[\"ts_hour\"], errors=\"ignore\").sort_values([\"ts_hour_ns\"]),\n",
    "            by=\"user_id_hashed\",\n",
    "            on=\"ts_hour_ns\",\n",
    "            direction=\"backward\",\n",
    "        )\n",
    "        X = pd.merge_asof(\n",
    "            X.sort_values([\"ts_hour_ns\"]),\n",
    "            user_search_t.drop(columns=[\"ts_hour\"], errors=\"ignore\").sort_values([\"ts_hour_ns\"]),\n",
    "            by=\"user_id_hashed\",\n",
    "            on=\"ts_hour_ns\",\n",
    "            direction=\"backward\",\n",
    "        )\n",
    "        # term side\n",
    "        X = pd.merge_asof(\n",
    "            X.sort_values([\"ts_hour_ns\"]),\n",
    "            term_search_t.drop(columns=[\"ts_hour\"], errors=\"ignore\").sort_values([\"ts_hour_ns\"]),\n",
    "            by=\"search_term_normalized\",\n",
    "            on=\"ts_hour_ns\",\n",
    "            direction=\"backward\",\n",
    "        )\n",
    "        # Fill NA\n",
    "        num_cols = X.select_dtypes(include=[\"float64\", \"float32\", \"int64\", \"int32\"]).columns.tolist()\n",
    "        cat_cols = X.select_dtypes(include=[\"object\", \"string\"]).columns.tolist()\n",
    "        X[num_cols] = X[num_cols].fillna(0)\n",
    "        for c in cat_cols:\n",
    "            X[c] = X[c].astype(\"string\").fillna(\"UNKNOWN\")\n",
    "        # Drop internal merge key from final feature set\n",
    "        if \"ts_hour_ns\" in X.columns:\n",
    "            X = X.drop(columns=[\"ts_hour_ns\"])\n",
    "        return X\n",
    "\n",
    "    _print(\"Joining trainâ€¦\")\n",
    "    train_X = join_all(train_df)\n",
    "    _print(f\"Train shape: {train_X.shape}\")\n",
    "\n",
    "    _print(\"Joining testâ€¦\")\n",
    "    test_X = join_all(test_df)\n",
    "    _print(f\"Test shape: {test_X.shape}\")\n",
    "\n",
    "    cat_features: List[str] = []\n",
    "    for c, dt in train_X.dtypes.items():\n",
    "        if dt == \"string[python]\" or str(dt).startswith(\"string\"):\n",
    "            if c in {\"user_id_hashed\", \"content_id_hashed\", \"leaf_category_name\", \"user_gender\", \"search_term_normalized\"}:\n",
    "                cat_features.append(str(c))\n",
    "\n",
    "    train_out = outdir / \"train_data_v2.parquet\"\n",
    "    test_out = outdir / \"test_data_v2.parquet\"\n",
    "    _print(f\"Writing: {train_out}\")\n",
    "    train_X.to_parquet(train_out, index=False)\n",
    "    _print(f\"Writing: {test_out}\")\n",
    "    test_X.to_parquet(test_out, index=False)\n",
    "\n",
    "    import json\n",
    "\n",
    "    (outdir / \"cat_features.json\").write_text(json.dumps(cat_features, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    _print(f\"Saved cat_features: {cat_features}\")\n",
    "    return train_out, test_out, cat_features\n",
    "\n",
    "\n",
    "def parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:\n",
    "    p = argparse.ArgumentParser(description=\"Build enriched datasets from sessions and auxiliary logs.\")\n",
    "    p.add_argument(\"--outdir\", type=str, default=str(Path(\"CatBoost\").resolve()), help=\"Output directory for enriched datasets\")\n",
    "    p.add_argument(\"--debug-n\", type=int, default=None, help=\"Limit to approximately N rows (keeps whole sessions)\")\n",
    "    p.add_argument(\"--fast\", action=\"store_true\", help=\"Use truncated aggregates for a quick dry run\")\n",
    "    return p.parse_args(argv)\n",
    "\n",
    "\n",
    "def main(argv: Optional[List[str]] = None) -> None:\n",
    "    args = parse_args(argv)\n",
    "    root = Path(\"C:/Projects/trendyol\")\n",
    "    outdir = Path(args.outdir)\n",
    "    _print(f\"Root: {root}\")\n",
    "    _print(f\"Outdir: {outdir}\")\n",
    "    _print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640ab684",
   "metadata": {},
   "source": [
    "**Step 2 : Create Session Stats, Price Normalization, User-Term Affinity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc168fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def time_split(df_pl: pl.DataFrame, val_frac: float = 0.15) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    if \"ts_hour\" in df_pl.columns and df_pl[\"ts_hour\"].dtype != pl.Datetime:\n",
    "        df_pl = df_pl.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
    "    df_pl = df_pl.sort(\"ts_hour\")\n",
    "    i = int(len(df_pl) * (1 - val_frac))\n",
    "    return df_pl[:i].to_pandas(), df_pl[i:].to_pandas()\n",
    "\n",
    "def _print(msg: str):\n",
    "    print(f\"[v2_plus] {msg}\")\n",
    "\n",
    "\n",
    "# Minimal, safe enrichments: session stats, price-by-category z, time-decayed proxies\n",
    "# Mirrors selected blocks from auto_feature_search_v2.py (sanitized)\n",
    "\n",
    "def fb_session_pos_extras(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Session stats extras (leakage-safe): only session-level price stats.\n",
    "    Avoids positional flags like first/last or cumcount indices that can encode labels\n",
    "    depending on upstream row ordering.\n",
    "    \"\"\"\n",
    "    if \"session_id\" in df and \"selling_price\" in df:\n",
    "        g = df.groupby(\"session_id\")[\"selling_price\"]\n",
    "        df[\"sess_price_mean\"] = g.transform(\"mean\")\n",
    "        df[\"sess_price_std\"] = g.transform(\"std\").fillna(0.0)\n",
    "        df[\"sess_price_z\"] = (pd.to_numeric(df[\"selling_price\"], errors=\"coerce\").fillna(0.0) - df[\"sess_price_mean\"]) / (df[\"sess_price_std\"].replace(0, np.nan))\n",
    "        df[\"sess_price_z\"] = df[\"sess_price_z\"].replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "    # ensure sess_n_items present via safe count\n",
    "    if \"session_id\" in df and \"sess_n_items\" not in df:\n",
    "        df[\"sess_n_items\"] = df.groupby(\"session_id\")[\"content_id_hashed\"].transform(\"size\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def fb_price_norm_by_category_fit(train_df: pd.DataFrame):\n",
    "    state = {}\n",
    "    if \"leaf_category_name\" in train_df and \"selling_price\" in train_df:\n",
    "        g = train_df.groupby(\"leaf_category_name\")[\"selling_price\"].agg([\"mean\", \"std\"]).rename(columns={\"mean\": \"mu\", \"std\": \"sigma\"})\n",
    "        state[\"cat_price_stats\"] = g\n",
    "    return state\n",
    "\n",
    "\n",
    "def fb_price_norm_by_category_transform(df: pd.DataFrame, state):\n",
    "    if \"cat_price_stats\" in state and \"leaf_category_name\" in df and \"selling_price\" in df:\n",
    "        g = state[\"cat_price_stats\"]\n",
    "        df = df.merge(g, left_on=\"leaf_category_name\", right_index=True, how=\"left\")\n",
    "        df[\"price_z_cat\"] = (df[\"selling_price\"] - df[\"mu\"]) / (df[\"sigma\"].replace(0, np.nan))\n",
    "        df[\"price_z_cat\"] = df[\"price_z_cat\"].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        df.drop(columns=[\"mu\", \"sigma\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def fb_time_decayed_pop(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"content_sitewide_avg_click_rate\" in df and \"content_sitewide_avg_order_rate\" in df:\n",
    "        r = (pd.to_numeric(df[\"content_sitewide_avg_click_rate\"], errors=\"coerce\").fillna(0.0) + 1e-6) / (\n",
    "            pd.to_numeric(df[\"content_sitewide_avg_order_rate\"], errors=\"coerce\").fillna(0.0) + 1e-6\n",
    "        )\n",
    "        df[\"content_pop_ratio_click_order\"] = r\n",
    "    if \"content_search_ctr\" in df:\n",
    "        c = pd.to_numeric(df[\"content_search_ctr\"], errors=\"coerce\").fillna(0.0).clip(lower=0)\n",
    "        df[\"content_search_ctr_sqrt\"] = c.pow(0.5)\n",
    "        df[\"content_search_ctr_log1p\"] = np.log1p(c)\n",
    "    if \"term_content_search_ctr\" in df and \"content_search_ctr\" in df:\n",
    "        a = pd.to_numeric(df[\"term_content_search_ctr\"], errors=\"coerce\").fillna(0.0) + 1e-6\n",
    "        b = pd.to_numeric(df[\"content_search_ctr\"], errors=\"coerce\").fillna(0.0) + 1e-6\n",
    "        df[\"term_over_content_ctr\"] = a / b\n",
    "    if \"user_search_ctr\" in df and \"content_search_ctr\" in df:\n",
    "        a = pd.to_numeric(df[\"user_search_ctr\"], errors=\"coerce\").fillna(0.0) + 1e-6\n",
    "        b = pd.to_numeric(df[\"content_search_ctr\"], errors=\"coerce\").fillna(0.0) + 1e-6\n",
    "        df[\"user_over_content_search_ctr\"] = a / b\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_user_term_affinity(tr: pd.DataFrame, te: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Fit on train only: user_term_freq, user_term_unique_content, user_term_avg_price; merge to train/test.\"\"\"\n",
    "    required = [\"user_id_hashed\", \"search_term_normalized\"]\n",
    "    for col in required:\n",
    "        if col not in tr.columns or col not in te.columns:\n",
    "            raise KeyError(f\"Missing column '{col}' in train/test for user-term affinity\")\n",
    "    # ensure string keys\n",
    "    tr[\"user_id_hashed\"] = tr[\"user_id_hashed\"].astype(str)\n",
    "    te[\"user_id_hashed\"] = te[\"user_id_hashed\"].astype(str)\n",
    "    tr[\"search_term_normalized\"] = tr[\"search_term_normalized\"].astype(str)\n",
    "    te[\"search_term_normalized\"] = te[\"search_term_normalized\"].astype(str)\n",
    "\n",
    "    cols = [\"user_id_hashed\", \"search_term_normalized\", \"content_id_hashed\"]\n",
    "    if \"selling_price\" in tr.columns:\n",
    "        cols.append(\"selling_price\")\n",
    "    g = tr[cols].groupby([\"user_id_hashed\", \"search_term_normalized\"])  # train-only fit\n",
    "    stats = pd.DataFrame({\n",
    "        \"user_term_freq\": g.size(),\n",
    "        \"user_term_unique_content\": g[\"content_id_hashed\"].nunique(),\n",
    "    }).reset_index()\n",
    "    if \"selling_price\" in cols:\n",
    "        stats_price = g[\"selling_price\"].mean().reset_index().rename(columns={\"selling_price\": \"user_term_avg_price\"})\n",
    "        stats = stats.merge(stats_price, on=[\"user_id_hashed\", \"search_term_normalized\"], how=\"left\")\n",
    "\n",
    "    # merge to train/test\n",
    "    tr2 = tr.merge(stats, on=[\"user_id_hashed\", \"search_term_normalized\"], how=\"left\")\n",
    "    te2 = te.merge(stats, on=[\"user_id_hashed\", \"search_term_normalized\"], how=\"left\")\n",
    "\n",
    "    # fill types\n",
    "    for c in [\"user_term_freq\", \"user_term_unique_content\"]:\n",
    "        if c in tr2.columns:\n",
    "            tr2[c] = pd.to_numeric(tr2[c], errors=\"coerce\").fillna(0).astype(np.int32)\n",
    "            te2[c] = pd.to_numeric(te2[c], errors=\"coerce\").fillna(0).astype(np.int32)\n",
    "    if \"user_term_avg_price\" in tr2.columns:\n",
    "        med = pd.to_numeric(tr2[\"user_term_avg_price\"], errors=\"coerce\").median()\n",
    "        tr2[\"user_term_avg_price\"] = pd.to_numeric(tr2[\"user_term_avg_price\"], errors=\"coerce\").fillna(med)\n",
    "        te2[\"user_term_avg_price\"] = pd.to_numeric(te2[\"user_term_avg_price\"], errors=\"coerce\").fillna(med)\n",
    "\n",
    "    return tr2, te2\n",
    "\n",
    "\n",
    "def main():\n",
    "    root = Path(\"C:/Projects/trendyol/data\")\n",
    "    train_p = root / \"train_data_v2.parquet\"\n",
    "    test_p = root / \"test_data_v2.parquet\"\n",
    "\n",
    "    out_train = root / \"train_data_v3.parquet\"\n",
    "    out_test = root / \"test_data_v3.parquet\"\n",
    "\n",
    "    _print(\"Loading v2â€¦\")\n",
    "    tr = pl.read_parquet(str(train_p))\n",
    "    te = pl.read_parquet(str(test_p)).to_pandas()\n",
    "\n",
    "    tr_tr, tr_val = time_split(tr)\n",
    "    tr_full = tr.to_pandas()\n",
    "\n",
    "    _print(\"Fitting category price stats on train and transforming train/testâ€¦\")\n",
    "    state = fb_price_norm_by_category_fit(tr_tr)\n",
    "\n",
    "    tr_tr = fb_price_norm_by_category_transform(tr_tr, state)\n",
    "    tr_val = fb_price_norm_by_category_transform(tr_val, state)\n",
    "    te = fb_price_norm_by_category_transform(te, state)\n",
    "\n",
    "    _print(\"Adding session and time-decayed extrasâ€¦\")\n",
    "    tr_full = fb_session_pos_extras(tr_full)\n",
    "    failing_col = tr_full[[\"sess_price_mean\", \"sess_price_std\", \"sess_price_z\"]].copy()\n",
    "\n",
    "    tr_tr = fb_session_pos_extras(tr_tr)\n",
    "    tr_val = fb_session_pos_extras(tr_val)\n",
    "    te = fb_session_pos_extras(te)\n",
    "\n",
    "    tr = pd.concat([tr_tr, tr_val], ignore_index=True)\n",
    "    tr[[\"sess_price_mean\", \"sess_price_std\", \"sess_price_z\"]] = failing_col\n",
    "    _print(\"Adding userâ€“term affinity (train-fit â†’ train/test-merge)â€¦\")\n",
    "    tr, te = add_user_term_affinity(tr, te)\n",
    "\n",
    "    _print(\"Writing v2_plus (no reranker)â€¦\")\n",
    "    tr.to_parquet(out_train, index=False)\n",
    "    te.to_parquet(out_test, index=False)\n",
    "    _print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b800f",
   "metadata": {},
   "source": [
    "**Step 3 : Prodcut Text and Reranker Score by using gwen3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "def _print(msg: str):\n",
    "    print(f\"[v2_plus_rerank] {msg}\")\n",
    "\n",
    "\n",
    "def choose_model(device: torch.device, prefer_qwen: bool = True):\n",
    "    if device.type == \"cuda\" and prefer_qwen:\n",
    "        return \"Qwen/Qwen3-Reranker-0.6B\"\n",
    "    return \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "\n",
    "def build_product_text(df: pd.DataFrame) -> pd.Series:\n",
    "    parts: List[pd.Series] = []\n",
    "    for c in [\"leaf_category_name\", \"cv_tags\", \"content_name\", \"brand_name\"]:\n",
    "        if c in df.columns:\n",
    "            parts.append(df[c].astype(str))\n",
    "    if parts:\n",
    "        out = parts[0]\n",
    "        for s in parts[1:]:\n",
    "            out = out + \" \" + s\n",
    "        return out.fillna(\"\")\n",
    "    return df.get(\"content_id_hashed\", pd.Series([\"\" for _ in range(len(df))])).astype(str).fillna(\"\")\n",
    "\n",
    "\n",
    "def score_batch(tokenizer, model, queries: List[str], docs: List[str], device: torch.device, max_len: int = 512):\n",
    "    enc = tokenizer(queries, docs, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc)\n",
    "        logits = out.logits\n",
    "        if logits.ndim == 2:\n",
    "            l = logits[:, 0] if logits.shape[1] == 1 else logits[:, -1]\n",
    "        else:\n",
    "            l = logits.view(-1)\n",
    "        probs = torch.sigmoid(l).detach().float().cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "\n",
    "def add_reranker_score(df: pd.DataFrame, tokenizer, model, device: torch.device, batch_size: int = 64, max_rows: Optional[int] = None) -> np.ndarray:\n",
    "    if max_rows is not None:\n",
    "        df = df.head(max_rows).copy()\n",
    "    if \"search_term_normalized\" not in df.columns:\n",
    "        raise KeyError(\"search_term_normalized column missing\")\n",
    "    df[\"search_term_normalized\"] = df[\"search_term_normalized\"].astype(str).fillna(\"\")\n",
    "    prod_text = build_product_text(df)\n",
    "    n = len(df)\n",
    "    scores = np.zeros(n, dtype=np.float32)\n",
    "    for i in range(0, n, batch_size):\n",
    "        j = min(i + batch_size, n)\n",
    "        q = df[\"search_term_normalized\"].iloc[i:j].tolist()\n",
    "        d = prod_text.iloc[i:j].tolist()\n",
    "        scores[i:j] = score_batch(tokenizer, model, q, d, device)\n",
    "        if (i // batch_size) % 20 == 0:\n",
    "            _print(f\"scored {j}/{n}\")\n",
    "    return scores\n",
    "\n",
    "\n",
    "def main():\n",
    "    root = Path(\"C:/Projects/trendyol/data\")\n",
    "    \n",
    "    train_p = root / \"train_data_v3.parquet\"\n",
    "    test_p = root / \"test_data_v3.parquet\"\n",
    "\n",
    "    out_train = root / \"train_data_v4.parquet\"\n",
    "    out_test = root / \"test_data_v4.parquet\"\n",
    "\n",
    "    _print(\"Loading v2_plusâ€¦\")\n",
    "    tr = pl.read_parquet(str(train_p)).to_pandas()\n",
    "    te = pl.read_parquet(str(test_p)).to_pandas()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_name = choose_model(device, prefer_qwen=True)\n",
    "    _print(f\"Device={device.type}, model={model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Persist product_text into the datasets before scoring\n",
    "    tr[\"product_text\"] = build_product_text(tr)\n",
    "    te[\"product_text\"] = build_product_text(te)\n",
    "\n",
    "    _print(\"Scoring trainâ€¦\")\n",
    "    tr_scores = add_reranker_score(tr, tokenizer, model, device, batch_size=64)\n",
    "    _print(\"Scoring testâ€¦\")\n",
    "    te_scores = add_reranker_score(te, tokenizer, model, device, batch_size=64)\n",
    "\n",
    "    tr[\"reranker_score\"] = tr_scores\n",
    "    te[\"reranker_score\"] = te_scores\n",
    "\n",
    "    _print(\"Writing back v2_plus with reranker_scoreâ€¦\")\n",
    "    tr.to_parquet(out_train, index=False)\n",
    "    te.to_parquet(out_test, index=False)\n",
    "    _print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3090d73",
   "metadata": {},
   "source": [
    "**Final Step : Product and user meta features, user-product interaction feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894585c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Step\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "print(\"Starting all feature engineering steps...\")\n",
    "input_path = Path(\"C:/Projects/trendyol/data\")\n",
    "\n",
    "# --- 1. Load Raw Data ---\n",
    "print(\"[1/5] Reading raw data files...\")\n",
    "try:\n",
    "    df_train_sessions = pd.read_parquet(input_path / 'train_data_v4.parquet')\n",
    "    df_test_sessions = pd.read_parquet(input_path / 'test_data_v4.parquet')\n",
    "\n",
    "    df_content_meta = pd.read_parquet('data/content/metadata.parquet')\n",
    "    df_price = pd.read_parquet('data/content/price_rate_review_data.parquet')\n",
    "    df_user_meta = pd.read_parquet('data/user/metadata.parquet')\n",
    "\n",
    "    # Needed interaction log file for new feature\n",
    "    df_interaction_logs = pd.read_parquet('data/user/fashion_sitewide_log.parquet')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: File not found! Please check the file path: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Build Product & User Meta Features ---\n",
    "print(\"[2/5] Creating product age, discount percentage and user age features...\")\n",
    "\n",
    "# Determine the 'present time' across datasets\n",
    "present_time = pd.concat([df_train_sessions['ts_hour'], df_test_sessions['ts_hour']]).max()\n",
    "\n",
    "# Compute product age in days\n",
    "df_content_meta['product_age_days'] = (present_time - df_content_meta['content_creation_date']).dt.days\n",
    "\n",
    "# Compute discount percentage (guard against original_price < selling_price)\n",
    "df_price['original_price'] = np.where(df_price['original_price'] < df_price['selling_price'], df_price['selling_price'], df_price['original_price'])\n",
    "df_price['discount_percentage'] = np.where(\n",
    "    df_price['original_price'] > 0,\n",
    "    (df_price['original_price'] - df_price['selling_price']) / df_price['original_price'],\n",
    "    0\n",
    ") * 100\n",
    "# Keep the most recent price record per content\n",
    "df_price = df_price.sort_values('update_date', ascending=False).drop_duplicates('content_id_hashed')\n",
    "\n",
    "# Compute user age\n",
    "present_year = present_time.year\n",
    "df_user_meta['user_age'] = present_year - df_user_meta['user_birth_year']\n",
    "\n",
    "# Prepare feature subsets for merging\n",
    "df_product_age = df_content_meta[['content_id_hashed', 'product_age_days']]\n",
    "df_discount = df_price[['content_id_hashed', 'discount_percentage']]\n",
    "df_user_age = df_user_meta[['user_id_hashed', 'user_age']]\n",
    "\n",
    "# --- 3. Merge Meta Features Into Main Datasets ---\n",
    "print(\"[3/5] Merging meta features into main datasets...\")\n",
    "\n",
    "def enrich_with_metadata(df, df_product_age, df_discount, df_user_age):\n",
    "    df_enriched = pd.merge(df, df_product_age, on='content_id_hashed', how='left')\n",
    "    df_enriched = pd.merge(df_enriched, df_discount, on='content_id_hashed', how='left')\n",
    "    df_enriched = pd.merge(df_enriched, df_user_age, on='user_id_hashed', how='left')\n",
    "\n",
    "    # Fill NaNs\n",
    "    df_enriched['product_age_days'].fillna(0, inplace=True)\n",
    "    df_enriched['discount_percentage'].fillna(0, inplace=True)\n",
    "    df_enriched['user_age'].fillna(-1, inplace=True)\n",
    "    return df_enriched\n",
    "\n",
    "train_df_v2 = enrich_with_metadata(df_train_sessions, df_product_age, df_discount, df_user_age)\n",
    "test_df_v2 = enrich_with_metadata(df_test_sessions, df_product_age, df_discount, df_user_age)\n",
    "print(\"Meta features added.\")\n",
    "\n",
    "\n",
    "# --- 4. Safely Add Historical User-Content Interaction Features ---\n",
    "print(\"[4/5] Adding historical user-content interaction features (merge_asof)...\")\n",
    "\n",
    "def add_user_content_interaction_features(df_sessions, df_interaction_logs):\n",
    "    logs = df_interaction_logs[['user_id_hashed', 'content_id_hashed', 'ts_hour', 'total_click']].copy()\n",
    "    sessions = df_sessions.copy()\n",
    "\n",
    "    logs['ts_hour'] = pd.to_datetime(logs['ts_hour'])\n",
    "    sessions['ts_hour'] = pd.to_datetime(sessions['ts_hour'])\n",
    "\n",
    "    logs = logs.sort_values('ts_hour')\n",
    "    # Binary click event: 1 if total_click > 0 else 0\n",
    "    logs['click_event'] = (logs['total_click'] > 0).astype(int)\n",
    "    logs['user_content_past_click_count'] = logs.groupby(['user_id_hashed', 'content_id_hashed'])['click_event'].cumsum()\n",
    "\n",
    "    sessions = sessions.sort_values('ts_hour')\n",
    "\n",
    "    df_enriched = pd.merge_asof(\n",
    "        left=sessions,\n",
    "        right=logs[['user_id_hashed', 'content_id_hashed', 'ts_hour', 'user_content_past_click_count']],\n",
    "        on='ts_hour',\n",
    "        by=['user_id_hashed', 'content_id_hashed'],\n",
    "        direction='backward'\n",
    "    )\n",
    "\n",
    "    df_enriched['user_content_past_click_count'].fillna(0, inplace=True)\n",
    "    return df_enriched\n",
    "\n",
    "# Apply interaction feature augmentation\n",
    "train_df_v3 = add_user_content_interaction_features(train_df_v2, df_interaction_logs)\n",
    "test_df_v3 = add_user_content_interaction_features(test_df_v2, df_interaction_logs)\n",
    "print(\"Historical interaction features added.\")\n",
    "\n",
    "\n",
    "# --- 5. Save Results ---\n",
    "print(\"[5/5] Saving final enriched datasets...\")\n",
    "\n",
    "output_path = input_path\n",
    "train_output_path = output_path / 'train_data_v5.parquet'\n",
    "test_output_path = output_path / 'test_data_v5.parquet'\n",
    "\n",
    "train_df_v3.to_parquet(train_output_path, index=False)\n",
    "test_df_v3.to_parquet(test_output_path, index=False)\n",
    "\n",
    "print(\"\\nProcess completed! ðŸš€\")\n",
    "print(f\"Train data saved: {train_output_path}\")\n",
    "print(f\"Test data saved: {test_output_path}\")\n",
    "\n",
    "print(\"\\nFinal state of train set (first 5 rows):\")\n",
    "print(train_df_v3.head())\n",
    "print(\"\\nTrain set info:\")\n",
    "print(train_df_v3.info())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
