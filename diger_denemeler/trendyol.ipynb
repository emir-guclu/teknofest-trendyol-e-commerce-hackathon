{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Teknofest Trendyol E-Ticaret için yapay zeka modeli"
      ],
      "metadata": {
        "id": "3vvbeU6lWXHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deneme 1 Baseline Oluşturma"
      ],
      "metadata": {
        "id": "B_zr3R1kpYko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 1** Eğitim ve modelleri kaydetme"
      ],
      "metadata": {
        "id": "PXOfFp1PpipT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"--- KOD 1: Model Eğitme ve KAYDETME Süreci Başladı ---\")\n",
        "\n",
        "# --- Adım 1: Veri Yükleme ---\n",
        "# Colab'a yüklediğiniz dosyanın tam yolunu buraya yazın\n",
        "TRAIN_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "\n",
        "print(f\"Eğitim verisi yükleniyor: {TRAIN_PATH}\")\n",
        "train_full_df = pl.read_parquet(TRAIN_PATH)\n",
        "print(f\"Toplam eğitim verisi boyutu: {train_full_df.shape}\")\n",
        "\n",
        "\n",
        "# --- Adım 2: Veriyi Zamana Göre Ayırma (Train / Validation Split) ---\n",
        "print(\"Eğitim verisi, zamana göre sıralanıp yüzdesel olarak ayrılıyor...\")\n",
        "if \"ts_hour\" in train_full_df.columns and train_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    train_full_df = train_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "\n",
        "train_full_df = train_full_df.sort(\"ts_hour\")\n",
        "\n",
        "# %85 train, %15 valid olarak ayır\n",
        "split_index = int(len(train_full_df) * 0.85)\n",
        "train_df = train_full_df[:split_index]\n",
        "val_df = train_full_df[split_index:]\n",
        "\n",
        "print(f\"Train parçası boyutu: {train_df.shape}\")\n",
        "print(f\"Validation parçası boyutu: {val_df.shape}\")\n",
        "\n",
        "\n",
        "# --- Adım 3: Veri Hazırlama ve Tip Dönüşümü ---\n",
        "print(\"Veri setleri Pandas'a çevriliyor ve model için hazırlanıyor...\")\n",
        "train_pd = train_df.to_pandas().fillna(0)\n",
        "val_pd = val_df.to_pandas().fillna(0)\n",
        "\n",
        "# Object tipli sütunları 'category' tipine çevirerek hatayı gideriyoruz\n",
        "for col in train_pd.columns:\n",
        "    if train_pd[col].dtype == 'object':\n",
        "        train_pd[col] = train_pd[col].astype('category')\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "# Modelde kullanılacak özellikleri ve hedefleri belirliyoruz\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "# Test setinde olmayan ve hedefle ilişkili sızıntı yapabilecek sütunları hariç tutuyoruz\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features = [col for col in train_pd.columns if col not in exclude_cols]\n",
        "cat_features = [col for col in features if isinstance(train_pd[col].dtype, pd.CategoricalDtype)]\n",
        "\n",
        "print(f\"Kullanılacak özellik sayısı: {len(features)}\")\n",
        "\n",
        "\n",
        "# --- Adım 4: lgb.Dataset Oluşturma (RAM Dostu Kısım) ---\n",
        "print(\"RAM verimliliği için lgb.Dataset nesneleri oluşturuluyor...\")\n",
        "lgb_train_ordered = lgb.Dataset(train_pd[features], label=train_pd['ordered'], categorical_feature=cat_features)\n",
        "lgb_valid_ordered = lgb.Dataset(val_pd[features], label=val_pd['ordered'], categorical_feature=cat_features)\n",
        "\n",
        "lgb_train_clicked = lgb.Dataset(train_pd[features], label=train_pd['clicked'], categorical_feature=cat_features)\n",
        "lgb_valid_clicked = lgb.Dataset(val_pd[features], label=val_pd['clicked'], categorical_feature=cat_features)\n",
        "\n",
        "\n",
        "# --- Adım 5: Model Eğitimi ve Kaydetme ---\n",
        "params = {\n",
        "    \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\",\n",
        "    \"learning_rate\": 0.03, \"num_leaves\": 64, \"verbose\": -1, \"seed\": 42\n",
        "}\n",
        "callbacks = [lgb.early_stopping(100, verbose=True)]\n",
        "\n",
        "# DÜZELTME: Modelleri kaydetmeden önce klasörün var olduğundan emin ol\n",
        "MODEL_DIR = \"models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# 5.1 'ordered' modeli\n",
        "print(\"\\n'ordered' modeli eğitiliyor...\")\n",
        "ordered_counts = train_pd['ordered'].value_counts()\n",
        "params['scale_pos_weight'] = ordered_counts[0] / ordered_counts[1]\n",
        "model_ordered = lgb.train(params, lgb_train_ordered, valid_sets=[lgb_train_ordered, lgb_valid_ordered], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_ordered.save_model(os.path.join(MODEL_DIR, \"model_ordered.txt\"))\n",
        "print(\"✅ 'ordered' modeli 'model_ordered.txt' olarak kaydedildi.\")\n",
        "\n",
        "\n",
        "# 5.2 'clicked' modeli\n",
        "print(\"\\n'clicked' modeli eğitiliyor...\")\n",
        "clicked_counts = train_pd['clicked'].value_counts()\n",
        "params['scale_pos_weight'] = clicked_counts[0] / clicked_counts[1]\n",
        "model_clicked = lgb.train(params, lgb_train_clicked, valid_sets=[lgb_train_clicked, lgb_valid_clicked], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_clicked.save_model(os.path.join(MODEL_DIR, \"model_clicked.txt\"))\n",
        "print(\"✅ 'clicked' modeli 'model_clicked.txt' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n--- Eğitim ve Kaydetme İşlemi Tamamlandı! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfRw9tUKpf_e",
        "outputId": "939d0f68-9693-4273-d59f-a1ab74820fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 1: Model Eğitme ve KAYDETME Süreci Başladı ---\n",
            "Eğitim verisi yükleniyor: /content/Trendyol Veri/enriched_train_data_FINAL.parquet\n",
            "Toplam eğitim verisi boyutu: (2773805, 37)\n",
            "Eğitim verisi, zamana göre sıralanıp yüzdesel olarak ayrılıyor...\n",
            "Train parçası boyutu: (2357734, 37)\n",
            "Validation parçası boyutu: (416071, 37)\n",
            "Veri setleri Pandas'a çevriliyor ve model için hazırlanıyor...\n",
            "Kullanılacak özellik sayısı: 23\n",
            "RAM verimliliği için lgb.Dataset nesneleri oluşturuluyor...\n",
            "\n",
            "'ordered' modeli eğitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttrain's auc: 0.764107\tvalid's auc: 0.700363\n",
            "✅ 'ordered' modeli 'model_ordered.txt' olarak kaydedildi.\n",
            "\n",
            "'clicked' modeli eğitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[142]\ttrain's auc: 0.715502\tvalid's auc: 0.658479\n",
            "✅ 'clicked' modeli 'model_clicked.txt' olarak kaydedildi.\n",
            "\n",
            "--- Eğitim ve Kaydetme İşlemi Tamamlandı! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 2** Tahmini Skor Üretme Ve Kaggle İçin Uygun Submission Dosyası Hazırlama"
      ],
      "metadata": {
        "id": "4l9IB0C1pxb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "# trendyol_metric_group_auc.py dosyasının Colab ortamında olduğundan emin olun\n",
        "from trendyol_metric_group_auc import score\n",
        "\n",
        "print(\"--- KOD 2: Tahmin ve Skorlama Süreci Başladı ---\")\n",
        "\n",
        "# --- Adım 1: Gerekli Veri ve Kaydedilmiş Modelleri Yükleme ---\n",
        "VAL_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "TEST_PATH = \"/content/Trendyol Veri/enriched_test_data_FINAL.parquet\"\n",
        "MODEL_ORDERED_PATH = '/content/models/model_ordered.txt'\n",
        "MODEL_CLICKED_PATH = '/content/models/model_clicked.txt'\n",
        "\n",
        "print(\"Veriler ve kaydedilmiş modeller yükleniyor...\")\n",
        "val_full_df = pl.read_parquet(VAL_PATH)\n",
        "test_df_pl = pl.read_parquet(TEST_PATH)\n",
        "model_ordered = lgb.Booster(model_file=MODEL_ORDERED_PATH)\n",
        "model_clicked = lgb.Booster(model_file=MODEL_CLICKED_PATH)\n",
        "print(\"✅ Veriler ve modeller başarıyla yüklendi.\")\n",
        "\n",
        "# --- Adım 2: Validation Seti Hazırlama ve Lokal Skor Hesaplama ---\n",
        "print(\"\\nLokal skor için validation seti hazırlanıyor...\")\n",
        "if \"ts_hour\" in val_full_df.columns and val_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    val_full_df = val_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "val_full_df = val_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(val_full_df) * 0.85)\n",
        "val_pd = val_full_df[split_index:].to_pandas().fillna(0)\n",
        "\n",
        "# Özellik listesini eğitimdekiyle tutarlı olacak şekilde belirle\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features = [col for col in val_pd.columns if col not in exclude_cols]\n",
        "for col in features:\n",
        "    if val_pd[col].dtype == 'object':\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "val_pd['p_order'] = model_ordered.predict(val_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "val_pd['p_click'] = model_clicked.predict(val_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "val_pd['final_score'] = 0.7 * val_pd['p_order'] + 0.3 * val_pd['p_click']\n",
        "\n",
        "# Skor fonksiyonu için gerekli DataFrame'leri oluştur\n",
        "val_solution = val_pd.groupby('session_id').agg(\n",
        "    ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])),\n",
        "    clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])),\n",
        "    all_items=('content_id_hashed', ' '.join)\n",
        ").reset_index()\n",
        "val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n",
        "val_submission.rename(columns={'content_id_hashed': 'prediction'}, inplace=True)\n",
        "\n",
        "try:\n",
        "    local_final_score = score(val_solution, val_submission, 'session_id')\n",
        "    print(\"\\n-------------------------------------------\")\n",
        "    print(f\"🏆 LOKAL SKORUNUZ: {local_final_score:.5f}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "except Exception as e:\n",
        "    print(f\"Skor hesaplanırken bir hata oluştu: {e}\")\n",
        "\n",
        "# --- Adım 3: Kaggle için Submission Dosyası Oluşturma ---\n",
        "print(\"\\nTest verisi hazırlanıyor ve Kaggle için submission dosyası oluşturuluyor...\")\n",
        "test_pd = test_df_pl.to_pandas().fillna(0)\n",
        "for col in features:\n",
        "    if test_pd[col].dtype == 'object':\n",
        "        test_pd[col] = test_pd[col].astype('category')\n",
        "\n",
        "p_order_test = model_ordered.predict(test_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "p_click_test = model_clicked.predict(test_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "\n",
        "test_df_pl = test_df_pl.with_columns(\n",
        "    final_score=(0.7 * pl.Series(p_order_test)) + (0.3 * pl.Series(p_click_test))\n",
        ")\n",
        "\n",
        "submission_df = test_df_pl.sort([\"session_id\", \"final_score\"], descending=True).group_by(\"session_id\").agg(\n",
        "    pl.col(\"content_id_hashed\").alias(\"prediction\")\n",
        ").with_columns(\n",
        "    pl.col(\"prediction\").list.join(\" \")\n",
        ")\n",
        "\n",
        "# Sağlama kontrolü\n",
        "expected_rows = 18589\n",
        "actual_rows = submission_df.shape[0]\n",
        "print(f\"\\nOluşturulan submission satır sayısı: {actual_rows} (Beklenen: {expected_rows})\")\n",
        "if actual_rows == expected_rows: print(\"✅ Satır sayısı doğru.\")\n",
        "else: print(\"❌ UYARI: Satır sayısı yanlış!\")\n",
        "\n",
        "submission_path = \"submission_final_v4.csv\"\n",
        "submission_df.write_csv(submission_path)\n",
        "print(f\"\\nSubmission dosyası '{submission_path}' olarak kaydedildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jceyqVBRpz-Z",
        "outputId": "a64c6c3e-b13a-4de8-cfc9-31895215ccad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 2: Tahmin ve Skorlama Süreci Başladı ---\n",
            "Veriler ve kaydedilmiş modeller yükleniyor...\n",
            "✅ Veriler ve modeller başarıyla yüklendi.\n",
            "\n",
            "Lokal skor için validation seti hazırlanıyor...\n",
            "Ordered AUC:  0.6506964811575329\n",
            "Clicked AUC:  0.5915140232878667\n",
            "\n",
            "-------------------------------------------\n",
            "🏆 LOKAL SKORUNUZ: 0.63294\n",
            "-------------------------------------------\n",
            "\n",
            "Test verisi hazırlanıyor ve Kaggle için submission dosyası oluşturuluyor...\n",
            "\n",
            "Oluşturulan submission satır sayısı: 18589 (Beklenen: 18589)\n",
            "✅ Satır sayısı doğru.\n",
            "\n",
            "Submission dosyası 'submission_final_v4.csv' olarak kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deneme 2 Günün Saati / Haftanın Günü Özellikleriyle Eğitim"
      ],
      "metadata": {
        "id": "yrB-Np4yyumu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bölüm 1** Yeni eklenen özellik Sütunları ile modelleri eğitmek ve kaydetmek"
      ],
      "metadata": {
        "id": "RnbCPp-vzm6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"--- KOD 1: Model Eğitme (Yeni Zamansal Özellikler ile) ---\")\n",
        "\n",
        "# --- Adım 1: Veri Yükleme ---\n",
        "TRAIN_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "print(f\"Eğitim verisi yükleniyor: {TRAIN_PATH}\")\n",
        "train_full_df = pl.read_parquet(TRAIN_PATH)\n",
        "\n",
        "# --- Adım 2: Veriyi Zamana Göre Ayırma ---\n",
        "print(\"Eğitim verisi, zamana göre sıralanıp yüzdesel olarak ayrılıyor...\")\n",
        "if \"ts_hour\" in train_full_df.columns and train_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    train_full_df = train_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "\n",
        "# --- Adım 2.5: YENİ ZAMANSAL ÖZELLİKLER ÜRETME ---\n",
        "print(\"Yeni zamansal özellikler üretiliyor: 'hour_of_day' ve 'day_of_week'...\")\n",
        "train_full_df = train_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\") # Pazartesi=1, Pazar=7\n",
        ")\n",
        "\n",
        "# --- Adım 3: Train / Validation Split ---\n",
        "train_full_df = train_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(train_full_df) * 0.85)\n",
        "train_df, val_df = train_full_df[:split_index], train_full_df[split_index:]\n",
        "\n",
        "print(f\"Train parçası boyutu: {train_df.shape}\")\n",
        "print(f\"Validation parçası boyutu: {val_df.shape}\")\n",
        "\n",
        "\n",
        "# --- Adım 4: Veri Hazırlama ve Tip Dönüşümü ---\n",
        "print(\"Veri setleri Pandas'a çevriliyor ve model için hazırlanıyor...\")\n",
        "train_pd = train_df.to_pandas().fillna(0)\n",
        "val_pd = val_df.to_pandas().fillna(0)\n",
        "\n",
        "for col in train_pd.columns:\n",
        "    if train_pd[col].dtype == 'object':\n",
        "        train_pd[col] = train_pd[col].astype('category')\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features = [col for col in train_pd.columns if col not in exclude_cols]\n",
        "cat_features = [col for col in features if isinstance(train_pd[col].dtype, pd.CategoricalDtype) or train_pd[col].dtype.name == 'category']\n",
        "\n",
        "print(f\"Kullanılacak özellik sayısı: {len(features)}\")\n",
        "\n",
        "\n",
        "# --- Adım 5: lgb.Dataset Oluşturma ---\n",
        "print(\"lgb.Dataset nesneleri oluşturuluyor...\")\n",
        "lgb_train_ordered = lgb.Dataset(train_pd[features], label=train_pd['ordered'], categorical_feature=cat_features)\n",
        "lgb_valid_ordered = lgb.Dataset(val_pd[features], label=val_pd['ordered'], categorical_feature=cat_features)\n",
        "lgb_train_clicked = lgb.Dataset(train_pd[features], label=train_pd['clicked'], categorical_feature=cat_features)\n",
        "lgb_valid_clicked = lgb.Dataset(val_pd[features], label=val_pd['clicked'], categorical_feature=cat_features)\n",
        "\n",
        "\n",
        "# --- Adım 6: Model Eğitimi ve Kaydetme ---\n",
        "params = {\n",
        "    \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\",\n",
        "    \"learning_rate\": 0.03, \"num_leaves\": 64, \"verbose\": -1, \"seed\": 42\n",
        "}\n",
        "callbacks = [lgb.early_stopping(100, verbose=True)]\n",
        "\n",
        "MODEL_DIR = \"models_2_doldur_HoursAndDay\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# 6.1 'ordered' modeli\n",
        "print(\"\\n'ordered' modeli eğitiliyor...\")\n",
        "ordered_counts = train_pd['ordered'].value_counts()\n",
        "params['scale_pos_weight'] = ordered_counts[0] / ordered_counts[1]\n",
        "model_ordered = lgb.train(params, lgb_train_ordered, valid_sets=[lgb_train_ordered, lgb_valid_ordered], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_ordered.save_model(os.path.join(MODEL_DIR, \"model_ordered.txt\"))\n",
        "print(f\"✅ 'ordered' modeli '{MODEL_DIR}/model_ordered.txt' olarak kaydedildi.\")\n",
        "\n",
        "# 6.2 'clicked' modeli\n",
        "print(\"\\n'clicked' modeli eğitiliyor...\")\n",
        "clicked_counts = train_pd['clicked'].value_counts()\n",
        "params['scale_pos_weight'] = clicked_counts[0] / clicked_counts[1]\n",
        "model_clicked = lgb.train(params, lgb_train_clicked, valid_sets=[lgb_train_clicked, lgb_valid_clicked], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_clicked.save_model(os.path.join(MODEL_DIR, \"model_clicked.txt\"))\n",
        "print(f\"✅ 'clicked' modeli '{MODEL_DIR}/model_clicked_v2.txt' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n--- Yeni özelliklerle eğitim ve kaydetme işlemi tamamlandı! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh1v32Fvy2G5",
        "outputId": "7ce7a1fb-cab5-48c9-c19f-50b75098c627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 1: Model Eğitme (Yeni Zamansal Özellikler ile) ---\n",
            "Eğitim verisi yükleniyor: /content/Trendyol Veri/enriched_train_data_FINAL.parquet\n",
            "Eğitim verisi, zamana göre sıralanıp yüzdesel olarak ayrılıyor...\n",
            "Yeni zamansal özellikler üretiliyor: 'hour_of_day' ve 'day_of_week'...\n",
            "Train parçası boyutu: (2357734, 39)\n",
            "Validation parçası boyutu: (416071, 39)\n",
            "Veri setleri Pandas'a çevriliyor ve model için hazırlanıyor...\n",
            "Kullanılacak özellik sayısı: 25\n",
            "lgb.Dataset nesneleri oluşturuluyor...\n",
            "\n",
            "'ordered' modeli eğitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttrain's auc: 0.764107\tvalid's auc: 0.700363\n",
            "✅ 'ordered' modeli 'models_2_doldur_HoursAndDay/model_ordered.txt' olarak kaydedildi.\n",
            "\n",
            "'clicked' modeli eğitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[122]\ttrain's auc: 0.711014\tvalid's auc: 0.658447\n",
            "✅ 'clicked' modeli 'models_2_doldur_HoursAndDay/model_clicked_v2.txt' olarak kaydedildi.\n",
            "\n",
            "--- Yeni özelliklerle eğitim ve kaydetme işlemi tamamlandı! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bölüm 2** Yeni eklenen özellikleri test etme ve uygun submission oluşturma"
      ],
      "metadata": {
        "id": "2Dy9hEYMz_iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "# trendyol_metric_group_auc.py dosyasının Colab ortamında olduğundan emin olun\n",
        "from trendyol_metric_group_auc import score\n",
        "\n",
        "print(\"--- KOD 2: Güncellenmiş Tahmin ve Skorlama Süreci ---\")\n",
        "\n",
        "# --- Adım 1: Gerekli Veri ve GÜNCELLENMİŞ Modelleri Yükleme ---\n",
        "VAL_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "TEST_PATH = \"/content/Trendyol Veri/enriched_test_data_FINAL.parquet\"\n",
        "MODEL_DIR = \"models_2_doldur_HoursAndDay\"\n",
        "MODEL_ORDERED_PATH = os.path.join(MODEL_DIR, 'model_ordered.txt') # GÜNCELLEME\n",
        "MODEL_CLICKED_PATH = os.path.join(MODEL_DIR, 'model_clicked.txt') # GÜNCELLEME\n",
        "\n",
        "print(\"Veriler ve kaydedilmiş v2 modelleri yükleniyor...\")\n",
        "val_full_df = pl.read_parquet(VAL_PATH)\n",
        "test_df_pl = pl.read_parquet(TEST_PATH)\n",
        "model_ordered = lgb.Booster(model_file=MODEL_ORDERED_PATH)\n",
        "model_clicked = lgb.Booster(model_file=MODEL_CLICKED_PATH)\n",
        "print(\"✅ Veriler ve v2 modelleri başarıyla yüklendi.\")\n",
        "\n",
        "# --- Adım 2: Validation Seti Hazırlama (Yeni Özellikler ile) ---\n",
        "print(\"\\nLokal skor için validation seti hazırlanıyor...\")\n",
        "if \"ts_hour\" in val_full_df.columns and val_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    val_full_df = val_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "\n",
        "# GÜNCELLEME: Eğitimde eklediğimiz zamansal özellikleri buraya da ekliyoruz\n",
        "val_full_df = val_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "\n",
        "val_full_df = val_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(val_full_df) * 0.85)\n",
        "val_pd = val_full_df[split_index:].to_pandas().fillna(0)\n",
        "\n",
        "# Özellik listesini eğitimdekiyle tutarlı olacak şekilde belirle\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features = [col for col in val_pd.columns if col not in exclude_cols]\n",
        "for col in features:\n",
        "    if val_pd[col].dtype == 'object':\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "val_pd['p_order'] = model_ordered.predict(val_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "val_pd['p_click'] = model_clicked.predict(val_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "val_pd['final_score'] = 0.7 * val_pd['p_order'] + 0.3 * val_pd['p_click']\n",
        "\n",
        "# Skor fonksiyonu için gerekli DataFrame'leri oluştur\n",
        "val_solution = val_pd.groupby('session_id').agg(ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), all_items=('content_id_hashed', ' '.join)).reset_index()\n",
        "val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n",
        "val_submission.rename(columns={'content_id_hashed': 'prediction'}, inplace=True)\n",
        "\n",
        "try:\n",
        "    local_final_score = score(val_solution, val_submission, 'session_id')\n",
        "    print(\"\\n-------------------------------------------\")\n",
        "    print(f\"🏆 YENİ MODELLERLE LOKAL SKORUNUZ: {local_final_score:.5f}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "except Exception as e:\n",
        "    print(f\"Skor hesaplanırken bir hata oluştu: {e}\")\n",
        "\n",
        "# --- Adım 3: Kaggle için Submission Dosyası Oluşturma ---\n",
        "print(\"\\nTest verisi hazırlanıyor ve Kaggle için submission dosyası oluşturuluyor...\")\n",
        "\n",
        "# GÜNCELLEME: Eğitimde eklediğimiz zamansal özellikleri test setine de ekliyoruz\n",
        "if \"ts_hour\" in test_df_pl.columns and test_df_pl[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    test_df_pl = test_df_pl.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "test_df_pl = test_df_pl.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "\n",
        "test_pd = test_df_pl.to_pandas().fillna(0)\n",
        "for col in features:\n",
        "    if test_pd[col].dtype == 'object':\n",
        "        test_pd[col] = test_pd[col].astype('category')\n",
        "\n",
        "p_order_test = model_ordered.predict(test_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "p_click_test = model_clicked.predict(test_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "\n",
        "test_df_pl = test_df_pl.with_columns(\n",
        "    final_score=(0.7 * pl.Series(p_order_test)) + (0.3 * pl.Series(p_click_test))\n",
        ")\n",
        "\n",
        "submission_df = test_df_pl.sort([\"session_id\", \"final_score\"], descending=True).group_by(\"session_id\").agg(\n",
        "    pl.col(\"content_id_hashed\").alias(\"prediction\")\n",
        ").with_columns(\n",
        "    pl.col(\"prediction\").list.join(\" \")\n",
        ")\n",
        "\n",
        "# Sağlama kontrolü\n",
        "expected_rows = 18589\n",
        "actual_rows = submission_df.shape[0]\n",
        "print(f\"\\nOluşturulan submission satır sayısı: {actual_rows} (Beklenen: {expected_rows})\")\n",
        "if actual_rows == expected_rows: print(\"✅ Satır sayısı doğru.\")\n",
        "else: print(\"❌ UYARI: Satır sayısı yanlış!\")\n",
        "\n",
        "submission_path = \"submission_final_v5.csv\" # Versiyonu güncelledim\n",
        "submission_df.write_csv(submission_path)\n",
        "print(f\"\\nSubmission dosyası '{submission_path}' olarak kaydedildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvNViBJz0FeI",
        "outputId": "06b4be04-cf87-4f4d-aa1b-152ef8c9808d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 2: Güncellenmiş Tahmin ve Skorlama Süreci ---\n",
            "Veriler ve kaydedilmiş v2 modelleri yükleniyor...\n",
            "✅ Veriler ve v2 modelleri başarıyla yüklendi.\n",
            "\n",
            "Lokal skor için validation seti hazırlanıyor...\n",
            "Ordered AUC:  0.6504508564295987\n",
            "Clicked AUC:  0.5918174552587637\n",
            "\n",
            "-------------------------------------------\n",
            "🏆 YENİ MODELLERLE LOKAL SKORUNUZ: 0.63286\n",
            "-------------------------------------------\n",
            "\n",
            "Test verisi hazırlanıyor ve Kaggle için submission dosyası oluşturuluyor...\n",
            "\n",
            "Oluşturulan submission satır sayısı: 18589 (Beklenen: 18589)\n",
            "✅ Satır sayısı doğru.\n",
            "\n",
            "Submission dosyası 'submission_final_v5.csv' olarak kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------\n",
        "Başka Bir Deneme\n",
        "---------------------------------"
      ],
      "metadata": {
        "id": "Rd1DnckP4OkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"--- KOD 1: Nihai Model Eğitme ve Kaydetme Süreci ---\")\n",
        "\n",
        "# --- Adım 1: Veri Yükleme ---\n",
        "TRAIN_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "print(f\"Eğitim verisi yükleniyor: {TRAIN_PATH}\")\n",
        "train_full_df = pl.read_parquet(TRAIN_PATH)\n",
        "print(f\"Toplam eğitim verisi boyutu: {train_full_df.shape}\")\n",
        "\n",
        "\n",
        "# --- Adım 2: Özellik Mühendisliği ve Veri Ayırma ---\n",
        "print(\"Yeni zamansal özellikler üretiliyor ve veri ayrılıyor...\")\n",
        "if \"ts_hour\" in train_full_df.columns and train_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    train_full_df = train_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "\n",
        "# 2.1 Zamansal Özellikleri Üretme\n",
        "train_full_df = train_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "\n",
        "# 2.2 Veriyi Zamana Göre Train/Validation Olarak Ayırma\n",
        "train_full_df = train_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(train_full_df) * 0.85)\n",
        "train_df, val_df = train_full_df[:split_index], train_full_df[split_index:]\n",
        "\n",
        "print(f\"Train parçası boyutu: {train_df.shape}\")\n",
        "print(f\"Validation parçası boyutu: {val_df.shape}\")\n",
        "\n",
        "\n",
        "# --- Adım 3: Veri Hazırlama ve Tip Dönüşümü ---\n",
        "print(\"Veri setleri Pandas'a çevriliyor ve model için hazırlanıyor...\")\n",
        "train_pd = train_df.to_pandas().fillna(0)\n",
        "val_pd = val_df.to_pandas().fillna(0)\n",
        "\n",
        "# Kategorik olarak ele alınacak sütunları belirliyoruz\n",
        "string_cols = [col for col in train_pd.columns if train_pd[col].dtype == 'object']\n",
        "temporal_cols_as_cat = [\"hour_of_day\", \"day_of_week\"]\n",
        "cols_to_categorize = string_cols + temporal_cols_as_cat\n",
        "\n",
        "# Belirlenen sütunların tipini 'category' olarak değiştiriyoruz\n",
        "for col in cols_to_categorize:\n",
        "    if col in train_pd.columns:\n",
        "        train_pd[col] = train_pd[col].astype('category')\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "# Modelde kullanılacak özellikleri ve hedefleri belirliyoruz\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features = [col for col in train_pd.columns if col not in exclude_cols]\n",
        "cat_features = [col for col in features if isinstance(train_pd[col].dtype, pd.CategoricalDtype)]\n",
        "\n",
        "print(f\"Kullanılacak özellik sayısı: {len(features)}\")\n",
        "print(f\"Kategorik olarak belirlenen özellik sayısı: {len(cat_features)}\")\n",
        "\n",
        "\n",
        "# --- Adım 4: lgb.Dataset Oluşturma ---\n",
        "print(\"lgb.Dataset nesneleri oluşturuluyor...\")\n",
        "lgb_train_ordered = lgb.Dataset(train_pd[features], label=train_pd['ordered'], categorical_feature=cat_features)\n",
        "lgb_valid_ordered = lgb.Dataset(val_pd[features], label=val_pd['ordered'], categorical_feature=cat_features)\n",
        "\n",
        "lgb_train_clicked = lgb.Dataset(train_pd[features], label=train_pd['clicked'], categorical_feature=cat_features)\n",
        "lgb_valid_clicked = lgb.Dataset(val_pd[features], label=val_pd['clicked'], categorical_feature=cat_features)\n",
        "\n",
        "\n",
        "# --- Adım 5: Model Eğitimi ve Kaydetme ---\n",
        "params = {\n",
        "    \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\",\n",
        "    \"learning_rate\": 0.03, \"num_leaves\": 64, \"verbose\": -1, \"seed\": 42\n",
        "}\n",
        "callbacks = [lgb.early_stopping(100, verbose=True)]\n",
        "\n",
        "MODEL_DIR = \"models_3_doldur_HoursAndDayCategory\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# 5.1 'ordered' modeli\n",
        "print(\"\\n'ordered' modeli eğitiliyor...\")\n",
        "ordered_counts = train_pd['ordered'].value_counts()\n",
        "params['scale_pos_weight'] = ordered_counts[0] / ordered_counts[1]\n",
        "model_ordered = lgb.train(params, lgb_train_ordered, valid_sets=[lgb_train_ordered, lgb_valid_ordered], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_ordered.save_model(os.path.join(MODEL_DIR, \"model_ordered.txt\"))\n",
        "print(f\"✅ 'ordered' modeli '{MODEL_DIR}/model_ordered.txt' olarak kaydedildi.\")\n",
        "\n",
        "# 5.2 'clicked' modeli\n",
        "print(\"\\n'clicked' modeli eğitiliyor...\")\n",
        "clicked_counts = train_pd['clicked'].value_counts()\n",
        "params['scale_pos_weight'] = clicked_counts[0] / clicked_counts[1]\n",
        "model_clicked = lgb.train(params, lgb_train_clicked, valid_sets=[lgb_train_clicked, lgb_valid_clicked], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_clicked.save_model(os.path.join(MODEL_DIR, \"model_clicked.txt\"))\n",
        "print(f\"✅ 'clicked' modeli '{MODEL_DIR}/model_clicked.txt' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n--- Eğitim ve Kaydetme İşlemi Tamamlandı! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxEsw1yZ4P4G",
        "outputId": "6ec78fd6-8943-49fa-ccc3-3a21ac1a22d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 1: Nihai Model Eğitme ve Kaydetme Süreci ---\n",
            "Eğitim verisi yükleniyor: /content/Trendyol Veri/enriched_train_data_FINAL.parquet\n",
            "Toplam eğitim verisi boyutu: (2773805, 37)\n",
            "Yeni zamansal özellikler üretiliyor ve veri ayrılıyor...\n",
            "Train parçası boyutu: (2357734, 39)\n",
            "Validation parçası boyutu: (416071, 39)\n",
            "Veri setleri Pandas'a çevriliyor ve model için hazırlanıyor...\n",
            "Kullanılacak özellik sayısı: 25\n",
            "Kategorik olarak belirlenen özellik sayısı: 4\n",
            "lgb.Dataset nesneleri oluşturuluyor...\n",
            "\n",
            "'ordered' modeli eğitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttrain's auc: 0.764107\tvalid's auc: 0.700363\n",
            "✅ 'ordered' modeli 'models_3_doldur_HoursAndDayCategory/model_ordered.txt' olarak kaydedildi.\n",
            "\n",
            "'clicked' modeli eğitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[127]\ttrain's auc: 0.712264\tvalid's auc: 0.659234\n",
            "✅ 'clicked' modeli 'models_3_doldur_HoursAndDayCategory/model_clicked.txt' olarak kaydedildi.\n",
            "\n",
            "--- Eğitim ve Kaydetme İşlemi Tamamlandı! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "# trendyol_metric_group_auc.py dosyasının Colab ortamında olduğundan emin olun\n",
        "from trendyol_metric_group_auc import score\n",
        "\n",
        "print(\"--- KOD 2: Nihai Tahmin ve Skorlama Süreci ---\")\n",
        "\n",
        "# --- Adım 1: Gerekli Veri ve Kaydedilmiş Modelleri Yükleme ---\n",
        "VAL_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "TEST_PATH = \"/content/Trendyol Veri/enriched_test_data_FINAL.parquet\"\n",
        "MODEL_DIR = \"models_3_doldur_HoursAndDayCategory\"\n",
        "MODEL_ORDERED_PATH = os.path.join(MODEL_DIR, 'model_ordered.txt')\n",
        "MODEL_CLICKED_PATH = os.path.join(MODEL_DIR, 'model_clicked.txt')\n",
        "\n",
        "print(\"Veriler ve kaydedilmiş modeller yükleniyor...\")\n",
        "val_full_df = pl.read_parquet(VAL_PATH)\n",
        "test_df_pl = pl.read_parquet(TEST_PATH)\n",
        "model_ordered = lgb.Booster(model_file=MODEL_ORDERED_PATH)\n",
        "model_clicked = lgb.Booster(model_file=MODEL_CLICKED_PATH)\n",
        "print(\"✅ Veriler ve modeller başarıyla yüklendi.\")\n",
        "\n",
        "\n",
        "# --- Adım 2: Lokal Skor İçin Validation Seti Hazırlama ---\n",
        "print(\"\\nLokal skor için validation seti hazırlanıyor...\")\n",
        "if \"ts_hour\" in val_full_df.columns and val_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    val_full_df = val_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "\n",
        "# Yeni zamansal özellikleri ekle\n",
        "val_full_df = val_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "\n",
        "val_full_df = val_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(val_full_df) * 0.85)\n",
        "val_pd = val_full_df[split_index:].to_pandas().fillna(0)\n",
        "\n",
        "# Özellik listesini ve kategorik tipleri eğitimdekiyle tutarlı yap\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features = [col for col in val_pd.columns if col not in exclude_cols]\n",
        "\n",
        "string_cols = [col for col in val_pd.columns if val_pd[col].dtype == 'object']\n",
        "temporal_cols_as_cat = [\"hour_of_day\", \"day_of_week\"]\n",
        "cols_to_categorize = string_cols + temporal_cols_as_cat\n",
        "for col in cols_to_categorize:\n",
        "    if col in val_pd.columns:\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "# Lokal skor için tahmin üret\n",
        "val_pd['p_order'] = model_ordered.predict(val_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "val_pd['p_click'] = model_clicked.predict(val_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "val_pd['final_score'] = 0.7 * val_pd['p_order'] + 0.3 * val_pd['p_click']\n",
        "\n",
        "# Skor fonksiyonu için gerekli DataFrame'leri oluştur\n",
        "val_solution = val_pd.groupby('session_id').agg(ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), all_items=('content_id_hashed', ' '.join)).reset_index()\n",
        "val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n",
        "val_submission.rename(columns={'content_id_hashed': 'prediction'}, inplace=True)\n",
        "\n",
        "try:\n",
        "    local_final_score = score(val_solution, val_submission, 'session_id')\n",
        "    print(\"\\n-------------------------------------------\")\n",
        "    print(f\"🏆 LOKAL SKORUNUZ: {local_final_score:.5f}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "except Exception as e:\n",
        "    print(f\"Skor hesaplanırken bir hata oluştu: {e}\")\n",
        "\n",
        "\n",
        "# --- Adım 3: Kaggle için Submission Dosyası Oluşturma ---\n",
        "print(\"\\nTest verisi hazırlanıyor ve Kaggle için submission dosyası oluşturuluyor...\")\n",
        "if \"ts_hour\" in test_df_pl.columns and test_df_pl[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    test_df_pl = test_df_pl.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "test_df_pl = test_df_pl.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "test_pd = test_df_pl.to_pandas().fillna(0)\n",
        "for col in cols_to_categorize:\n",
        "    if col in test_pd.columns:\n",
        "        test_pd[col] = test_pd[col].astype('category')\n",
        "\n",
        "p_order_test = model_ordered.predict(test_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "p_click_test = model_clicked.predict(test_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "\n",
        "test_df_pl = test_df_pl.with_columns(\n",
        "    final_score=(0.7 * pl.Series(p_order_test)) + (0.3 * pl.Series(p_click_test))\n",
        ")\n",
        "submission_df = test_df_pl.sort([\"session_id\", \"final_score\"], descending=True).group_by(\"session_id\").agg(\n",
        "    pl.col(\"content_id_hashed\").alias(\"prediction\")\n",
        ").with_columns(\n",
        "    pl.col(\"prediction\").list.join(\" \")\n",
        ")\n",
        "\n",
        "# Sağlama kontrolü\n",
        "expected_rows = 18589\n",
        "actual_rows = submission_df.shape[0]\n",
        "print(f\"\\nOluşturulan submission satır sayısı: {actual_rows} (Beklenen: {expected_rows})\")\n",
        "if actual_rows == expected_rows: print(\"✅ Satır sayısı doğru.\")\n",
        "else: print(\"❌ UYARI: Satır sayısı yanlış!\")\n",
        "\n",
        "submission_path = \"submission.csv\"\n",
        "submission_df.write_csv(submission_path)\n",
        "print(f\"\\nSubmission dosyası '{submission_path}' olarak kaydedildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo8_cTcf4w8B",
        "outputId": "db9e884f-585c-49ab-9405-6cb6c4e04b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 2: Nihai Tahmin ve Skorlama Süreci ---\n",
            "Veriler ve kaydedilmiş modeller yükleniyor...\n",
            "✅ Veriler ve modeller başarıyla yüklendi.\n",
            "\n",
            "Lokal skor için validation seti hazırlanıyor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3194590946.py:59: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  val_solution = val_pd.groupby('session_id').agg(ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), all_items=('content_id_hashed', ' '.join)).reset_index()\n",
            "/tmp/ipython-input-3194590946.py:60: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ordered AUC:  0.6504243273845817\n",
            "Clicked AUC:  0.5914654200400459\n",
            "\n",
            "-------------------------------------------\n",
            "🏆 LOKAL SKORUNUZ: 0.63274\n",
            "-------------------------------------------\n",
            "\n",
            "Test verisi hazırlanıyor ve Kaggle için submission dosyası oluşturuluyor...\n",
            "\n",
            "Oluşturulan submission satır sayısı: 18589 (Beklenen: 18589)\n",
            "✅ Satır sayısı doğru.\n",
            "\n",
            "Submission dosyası 'submission.csv' olarak kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deneme 3 Fiyat Uygunluğu"
      ],
      "metadata": {
        "id": "oKKmsWikDSpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 1** Fiyat uygunluğuna göre eğitim"
      ],
      "metadata": {
        "id": "P7fQFd5TDZFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"--- KOD 1: Model Eğitme (Ayrı Özellik Setleriyle) ---\")\n",
        "\n",
        "# --- Adım 1: Veri Yükleme ---\n",
        "TRAIN_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "train_full_df = pl.read_parquet(TRAIN_PATH)\n",
        "print(f\"Toplam eğitim verisi boyutu: {train_full_df.shape}\")\n",
        "\n",
        "# --- Adım 2: Ön Özellikler ve Train/Validation Ayrımı ---\n",
        "print(\"Ön özellikler üretiliyor ve veri zamana göre ayrılıyor...\")\n",
        "if \"ts_hour\" in train_full_df.columns and train_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    train_full_df = train_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "train_full_df = train_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "train_full_df = train_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(train_full_df) * 0.85)\n",
        "train_df, val_df = train_full_df[:split_index], train_full_df[split_index:]\n",
        "\n",
        "# --- Adım 3: Etkileşim Özelliğini SADECE Train Setinden Hesaplama ---\n",
        "print(\"Fiyat etkileşim özelliği, SADECE train verisi üzerinden hesaplanıyor...\")\n",
        "price_features_lazy = pl.scan_parquet(TRAIN_PATH).select([\"content_id_hashed\", \"selling_price\", \"update_date\"])\n",
        "latest_price_features = price_features_lazy.sort(\"update_date\", descending=True).group_by(\"content_id_hashed\").first().collect()\n",
        "user_orders = train_df.filter(pl.col(\"ordered\") == 1).select([\"user_id_hashed\", \"content_id_hashed\"])\n",
        "user_orders_with_price = user_orders.join(latest_price_features, on=\"content_id_hashed\", how=\"left\")\n",
        "agg_user_price_features = user_orders_with_price.group_by(\"user_id_hashed\").agg(pl.mean(\"selling_price\").alias(\"user_avg_order_price\"))\n",
        "FEATURES_DIR = \"features\"\n",
        "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
        "agg_user_price_features.write_parquet(os.path.join(FEATURES_DIR, \"user_price_features_final.parquet\"))\n",
        "print(f\"✅ Sızıntısız kullanıcı fiyat özellikleri kaydedildi.\")\n",
        "\n",
        "# --- Adım 4: Özellikleri Ekleme ve Akıllı Boşluk Doldurma ---\n",
        "global_avg_order_price = agg_user_price_features.select(pl.mean(\"user_avg_order_price\")).item()\n",
        "train_df = train_df.join(agg_user_price_features, on=\"user_id_hashed\", how=\"left\").with_columns(pl.col(\"user_avg_order_price\").fill_null(global_avg_order_price)).with_columns((pl.col(\"selling_price\") / pl.col(\"user_avg_order_price\")).alias(\"price_vs_user_avg_ratio\"))\n",
        "val_df = val_df.join(agg_user_price_features, on=\"user_id_hashed\", how=\"left\").with_columns(pl.col(\"user_avg_order_price\").fill_null(global_avg_order_price)).with_columns((pl.col(\"selling_price\") / pl.col(\"user_avg_order_price\")).alias(\"price_vs_user_avg_ratio\"))\n",
        "\n",
        "# --- Adım 5: Veri Hazırlama ve AYRI ÖZELLİK SETLERİ OLUŞTURMA ---\n",
        "print(\"Veri setleri Pandas'a çevriliyor ve ayrı özellik setleri oluşturuluyor...\")\n",
        "train_pd = train_df.to_pandas().fillna(0)\n",
        "val_pd = val_df.to_pandas().fillna(0)\n",
        "string_cols = [col for col in train_pd.columns if train_pd[col].dtype == 'object']\n",
        "temporal_cols_as_cat = [\"hour_of_day\", \"day_of_week\"]\n",
        "cols_to_categorize = string_cols + temporal_cols_as_cat\n",
        "for col in cols_to_categorize:\n",
        "    if col in train_pd.columns:\n",
        "        train_pd[col], val_pd[col] = train_pd[col].astype('category'), val_pd[col].astype('category')\n",
        "\n",
        "# 5.1 Temel Özellik Listesi\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features_base = [col for col in train_pd.columns if col not in exclude_cols]\n",
        "\n",
        "# 5.2 Modele Özel Özellik Listeleri\n",
        "price_features_to_exclude = [\"user_avg_order_price\", \"price_vs_user_avg_ratio\"]\n",
        "features_for_ordered = [f for f in features_base if f not in price_features_to_exclude]\n",
        "features_for_clicked = features_base # clicked modeli tüm özellikleri kullanabilir\n",
        "\n",
        "cat_features_ordered = [f for f in features_for_ordered if isinstance(train_pd[f].dtype, pd.CategoricalDtype)]\n",
        "cat_features_clicked = [f for f in features_for_clicked if isinstance(train_pd[f].dtype, pd.CategoricalDtype)]\n",
        "\n",
        "print(f\"'ordered' modeli için kullanılacak özellik sayısı: {len(features_for_ordered)}\")\n",
        "print(f\"'clicked' modeli için kullanılacak özellik sayısı: {len(features_for_clicked)}\")\n",
        "\n",
        "# --- Adım 6: lgb.Dataset Oluşturma (Ayrı Ayrı) ---\n",
        "print(\"Modele özel lgb.Dataset nesneleri oluşturuluyor...\")\n",
        "lgb_train_ordered = lgb.Dataset(train_pd[features_for_ordered], label=train_pd['ordered'], categorical_feature=cat_features_ordered)\n",
        "lgb_valid_ordered = lgb.Dataset(val_pd[features_for_ordered], label=val_pd['ordered'], categorical_feature=cat_features_ordered)\n",
        "\n",
        "lgb_train_clicked = lgb.Dataset(train_pd[features_for_clicked], label=train_pd['clicked'], categorical_feature=cat_features_clicked)\n",
        "lgb_valid_clicked = lgb.Dataset(val_pd[features_for_clicked], label=val_pd['clicked'], categorical_feature=cat_features_clicked)\n",
        "\n",
        "# --- Adım 7: Model Eğitimi ve Kaydetme ---\n",
        "params = { \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\", \"learning_rate\": 0.03, \"num_leaves\": 64, \"verbose\": -1, \"seed\": 42 }\n",
        "callbacks = [lgb.early_stopping(100, verbose=True)]\n",
        "MODEL_DIR = \"models_5_doldur_FiyatUygunlukAyrı\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(\"\\n'ordered' modeli (sınırlı özelliklerle) eğitiliyor...\")\n",
        "ordered_counts = train_pd['ordered'].value_counts()\n",
        "params['scale_pos_weight'] = ordered_counts[0] / ordered_counts[1]\n",
        "model_ordered = lgb.train(params, lgb_train_ordered, valid_sets=[lgb_train_ordered, lgb_valid_ordered], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_ordered.save_model(os.path.join(MODEL_DIR, \"model_ordered_final_split.txt\"))\n",
        "print(f\"✅ 'ordered' modeli '{MODEL_DIR}/model_ordered_final_split.txt' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n'clicked' modeli (tüm özelliklerle) eğitiliyor...\")\n",
        "clicked_counts = train_pd['clicked'].value_counts()\n",
        "params['scale_pos_weight'] = clicked_counts[0] / clicked_counts[1]\n",
        "model_clicked = lgb.train(params, lgb_train_clicked, valid_sets=[lgb_train_clicked, lgb_valid_clicked], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_clicked.save_model(os.path.join(MODEL_DIR, \"model_clicked_final_split.txt\"))\n",
        "print(f\"✅ 'clicked' modeli '{MODEL_DIR}/model_clicked_final_split.txt' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n--- Eğitim ve Kaydetme İşlemi Tamamlandı! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLh0sqmnDe1E",
        "outputId": "983e8584-36be-4921-f345-44e06fa2e6cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 1: Model Eğitme (Ayrı Özellik Setleriyle) ---\n",
            "Toplam eğitim verisi boyutu: (2773805, 37)\n",
            "Ön özellikler üretiliyor ve veri zamana göre ayrılıyor...\n",
            "Fiyat etkileşim özelliği, SADECE train verisi üzerinden hesaplanıyor...\n",
            "✅ Sızıntısız kullanıcı fiyat özellikleri kaydedildi.\n",
            "Veri setleri Pandas'a çevriliyor ve ayrı özellik setleri oluşturuluyor...\n",
            "'ordered' modeli için kullanılacak özellik sayısı: 25\n",
            "'clicked' modeli için kullanılacak özellik sayısı: 27\n",
            "Modele özel lgb.Dataset nesneleri oluşturuluyor...\n",
            "\n",
            "'ordered' modeli (sınırlı özelliklerle) eğitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttrain's auc: 0.764107\tvalid's auc: 0.700363\n",
            "✅ 'ordered' modeli 'models_5_doldur_FiyatUygunlukAyrı/model_ordered_final_split.txt' olarak kaydedildi.\n",
            "\n",
            "'clicked' modeli (tüm özelliklerle) eğitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[122]\ttrain's auc: 0.732438\tvalid's auc: 0.65745\n",
            "✅ 'clicked' modeli 'models_5_doldur_FiyatUygunlukAyrı/model_clicked_final_split.txt' olarak kaydedildi.\n",
            "\n",
            "--- Eğitim ve Kaydetme İşlemi Tamamlandı! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 2** Fiyat uygunluğuna göre test ve submission oluşturma"
      ],
      "metadata": {
        "id": "dqQDTPYgELLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "from trendyol_metric_group_auc import score\n",
        "\n",
        "print(\"--- KOD 2: Nihai Tahmin ve Skorlama (Ayrı Özellik Setleriyle) ---\")\n",
        "\n",
        "# --- Adım 1: Gerekli Dosyaları Yükleme ---\n",
        "VAL_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "TEST_PATH = \"/content/Trendyol Veri/enriched_test_data_FINAL.parquet\"\n",
        "FEATURES_DIR = \"features\"\n",
        "MODELS_DIR = \"models_5_doldur_FiyatUygunlukAyrı\"\n",
        "USER_PRICE_FEATURES_PATH = os.path.join(FEATURES_DIR, \"user_price_features_final.parquet\")\n",
        "MODEL_ORDERED_PATH = os.path.join(MODELS_DIR, 'model_ordered_final_split.txt')\n",
        "MODEL_CLICKED_PATH = os.path.join(MODELS_DIR, 'model_clicked_final_split.txt')\n",
        "\n",
        "print(\"Veriler, özellikler ve kaydedilmiş nihai modeller yükleniyor...\")\n",
        "val_full_df = pl.read_parquet(VAL_PATH)\n",
        "test_df_pl = pl.read_parquet(TEST_PATH)\n",
        "user_price_features = pl.read_parquet(USER_PRICE_FEATURES_PATH)\n",
        "model_ordered = lgb.Booster(model_file=MODEL_ORDERED_PATH)\n",
        "model_clicked = lgb.Booster(model_file=MODEL_CLICKED_PATH)\n",
        "print(\"✅ Yükleme tamamlandı.\")\n",
        "\n",
        "# --- Adım 2: Lokal Skor İçin Validation Seti Hazırlama ---\n",
        "print(\"\\nLokal skor için validation seti hazırlanıyor...\")\n",
        "if \"ts_hour\" in val_full_df.columns and val_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    val_full_df = val_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "val_full_df = val_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "global_avg_order_price = user_price_features.select(pl.mean(\"user_avg_order_price\")).item()\n",
        "val_full_df = val_full_df.join(user_price_features, on=\"user_id_hashed\", how=\"left\").with_columns(pl.col(\"user_avg_order_price\").fill_null(global_avg_order_price)).with_columns((pl.col(\"selling_price\") / pl.col(\"user_avg_order_price\")).alias(\"price_vs_user_avg_ratio\"))\n",
        "\n",
        "val_full_df = val_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(val_full_df) * 0.85)\n",
        "val_pd = val_full_df[split_index:].to_pandas().fillna(0)\n",
        "\n",
        "# Ayrı özellik setlerini burada da tanımla\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features_base = [col for col in val_pd.columns if col not in exclude_cols]\n",
        "price_features_to_exclude = [\"user_avg_order_price\", \"price_vs_user_avg_ratio\"]\n",
        "features_for_ordered = [f for f in features_base if f not in price_features_to_exclude]\n",
        "features_for_clicked = features_base\n",
        "string_cols = [col for col in val_pd.columns if val_pd[col].dtype == 'object']\n",
        "temporal_cols_as_cat = [\"hour_of_day\", \"day_of_week\"]\n",
        "cols_to_categorize = string_cols + temporal_cols_as_cat\n",
        "for col in cols_to_categorize:\n",
        "    if col in val_pd.columns:\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "# Tahmin yaparken doğru özellik setini kullan\n",
        "val_pd['p_order'] = model_ordered.predict(val_pd[features_for_ordered], num_iteration=model_ordered.best_iteration)\n",
        "val_pd['p_click'] = model_clicked.predict(val_pd[features_for_clicked], num_iteration=model_clicked.best_iteration)\n",
        "val_pd['final_score'] = 0.7 * val_pd['p_order'] + 0.3 * val_pd['p_click']\n",
        "\n",
        "# Skorlama ve submission (bu kısımlar aynı)\n",
        "val_solution = val_pd.groupby('session_id').agg(ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), all_items=('content_id_hashed', ' '.join)).reset_index()\n",
        "val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n",
        "val_submission.rename(columns={'content_id_hashed': 'prediction'}, inplace=True)\n",
        "try:\n",
        "    local_final_score = score(val_solution, val_submission, 'session_id')\n",
        "    print(\"\\n-------------------------------------------\")\n",
        "    print(f\"🏆 LOKAL SKORUNUZ (Ayrı Özelliklerle): {local_final_score:.5f}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "except Exception as e:\n",
        "    print(f\"Skor hesaplanırken bir hata oluştu: {e}\")\n",
        "\n",
        "# --- Adım 3: Kaggle için Submission Dosyası Oluşturma ---\n",
        "print(\"\\nTest verisi hazırlanıyor ve Kaggle için submission dosyası oluşturuluyor...\")\n",
        "if \"ts_hour\" in test_df_pl.columns and test_df_pl[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    test_df_pl = test_df_pl.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "test_df_pl = test_df_pl.with_columns(pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"), pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\"))\n",
        "test_df_pl = test_df_pl.join(user_price_features, on=\"user_id_hashed\", how=\"left\").with_columns(pl.col(\"user_avg_order_price\").fill_null(global_avg_order_price)).with_columns((pl.col(\"selling_price\") / pl.col(\"user_avg_order_price\")).alias(\"price_vs_user_avg_ratio\"))\n",
        "test_pd = test_df_pl.to_pandas().fillna(0)\n",
        "for col in cols_to_categorize:\n",
        "    if col in test_pd.columns:\n",
        "        test_pd[col] = test_pd[col].astype('category')\n",
        "\n",
        "# Tahmin yaparken doğru özellik setini kullan\n",
        "p_order_test = model_ordered.predict(test_pd[features_for_ordered], num_iteration=model_ordered.best_iteration)\n",
        "p_click_test = model_clicked.predict(test_pd[features_for_clicked], num_iteration=model_clicked.best_iteration)\n",
        "\n",
        "test_df_pl = test_df_pl.with_columns(final_score=(0.7 * pl.Series(p_order_test)) + (0.3 * pl.Series(p_click_test)))\n",
        "submission_df = test_df_pl.sort([\"session_id\", \"final_score\"], descending=True).group_by(\"session_id\").agg(pl.col(\"content_id_hashed\").alias(\"prediction\")).with_columns(pl.col(\"prediction\").list.join(\" \"))\n",
        "expected_rows = 18589\n",
        "actual_rows = submission_df.shape[0]\n",
        "print(f\"\\nOluşturulan submission satır sayısı: {actual_rows} (Beklenen: {expected_rows})\")\n",
        "if actual_rows == expected_rows: print(\"✅ Satır sayısı doğru.\")\n",
        "else: print(\"❌ UYARI: Satır sayısı yanlış!\")\n",
        "submission_path = \"submission.csv\"\n",
        "submission_df.write_csv(submission_path)\n",
        "print(f\"\\nSubmission dosyası '{submission_path}' olarak kaydedildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6wBMTgTEWX7",
        "outputId": "fb32210b-1b95-4492-aec3-5c56fe6cc813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 2: Nihai Tahmin ve Skorlama (Ayrı Özellik Setleriyle) ---\n",
            "Veriler, özellikler ve kaydedilmiş nihai modeller yükleniyor...\n",
            "✅ Yükleme tamamlandı.\n",
            "\n",
            "Lokal skor için validation seti hazırlanıyor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1547542627.py:62: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  val_solution = val_pd.groupby('session_id').agg(ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), all_items=('content_id_hashed', ' '.join)).reset_index()\n",
            "/tmp/ipython-input-1547542627.py:63: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ordered AUC:  0.6501352808701953\n",
            "Clicked AUC:  0.5901990785840607\n",
            "\n",
            "-------------------------------------------\n",
            "🏆 LOKAL SKORUNUZ (Ayrı Özelliklerle): 0.63215\n",
            "-------------------------------------------\n",
            "\n",
            "Test verisi hazırlanıyor ve Kaggle için submission dosyası oluşturuluyor...\n",
            "\n",
            "Oluşturulan submission satır sayısı: 18589 (Beklenen: 18589)\n",
            "✅ Satır sayısı doğru.\n",
            "\n",
            "Submission dosyası 'submission.csv' olarak kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Sızıntısını Engelleyerek Yeni Eğitim**"
      ],
      "metadata": {
        "id": "Bfg61JZuInox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"--- KOD 1: Model Eğitme (Fiyat Özellikleri ÇIKARILDI, Zamansal Özellikler KALDI) ---\")\n",
        "\n",
        "# --- Adım 1: Veri Yükleme ---\n",
        "TRAIN_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "print(f\"Eğitim verisi yükleniyor: {TRAIN_PATH}\")\n",
        "train_full_df = pl.read_parquet(TRAIN_PATH)\n",
        "print(f\"Toplam eğitim verisi boyutu: {train_full_df.shape}\")\n",
        "\n",
        "\n",
        "# --- Adım 2: Ön Özellikler (Zamansal) ve Train/Validation Ayrımı ---\n",
        "print(\"Zamansal özellikler üretiliyor ve veri zamana göre ayrılıyor...\")\n",
        "if \"ts_hour\" in train_full_df.columns and train_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    train_full_df = train_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "\n",
        "train_full_df = train_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "\n",
        "train_full_df = train_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(train_full_df) * 0.85)\n",
        "train_df, val_df = train_full_df[:split_index], train_full_df[split_index:]\n",
        "print(f\"Train parçası boyutu: {train_df.shape}\")\n",
        "print(f\"Validation parçası boyutu: {val_df.shape}\")\n",
        "\n",
        "\n",
        "# --- Adım 3: Fiyat Etkileşim Özellikleri ÇIKARILDI ---\n",
        "# Bu adımda artık kullanıcı ortalama fiyatı hesaplanmıyor ve birleştirilmiyor.\n",
        "print(\"✅ Fiyat etkileşim özellikleri bu denemede kullanılmayacaktır.\")\n",
        "\n",
        "\n",
        "# --- Adım 4: Veri Hazırlama ve Tip Dönüşümü ---\n",
        "print(\"Veri setleri Pandas'a çevriliyor ve model için hazırlanıyor...\")\n",
        "\n",
        "train_pd = train_df.to_pandas()\n",
        "val_pd = val_df.to_pandas()\n",
        "\n",
        "# Sayısal sütunlardaki NaN/Inf değerlerini yönetme (artık fiyat oranları yok, genel fillna(0) yapabiliriz)\n",
        "numeric_cols = train_pd.select_dtypes(include=np.number).columns.tolist()\n",
        "for col in numeric_cols:\n",
        "    # Sonsuz değerleri NaN yapıp sonra NaN'ları 0 ile dolduralım\n",
        "    train_pd[col] = train_pd[col].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "    val_pd[col] = val_pd[col].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "\n",
        "string_cols = [col for col in train_pd.columns if train_pd[col].dtype == 'object']\n",
        "temporal_cols_as_cat = [\"hour_of_day\", \"day_of_week\"]\n",
        "cols_to_categorize = string_cols + temporal_cols_as_cat\n",
        "for col in cols_to_categorize:\n",
        "    if col in train_pd.columns:\n",
        "        train_pd[col] = train_pd[col].astype('category')\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "# Düzeltme: exclude_cols listesinden price_vs_user_avg_ratio ve user_avg_order_price çıkarıldı\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\", \"price_vs_user_avg_ratio\", \"user_avg_order_price\"]\n",
        "features = [col for col in train_pd.columns if col not in exclude_cols]\n",
        "cat_features = [col for col in features if train_pd[col].dtype.name == 'category']\n",
        "\n",
        "print(f\"Kullanılacak özellik sayısı: {len(features)}\")\n",
        "print(f\"Güncel Kategorik özellik sayısı: {len(cat_features)}\")\n",
        "\n",
        "\n",
        "# --- Adım 5: lgb.Dataset Oluşturma ---\n",
        "print(\"lgb.Dataset nesneleri oluşturuluyor...\")\n",
        "# Categorical features listesini yeniden kontrol edelim\n",
        "cat_features = [col for col in features if train_pd[col].dtype.name == 'category']\n",
        "print(f\"Son Kategorik özellik sayısı: {len(cat_features)}\")\n",
        "\n",
        "lgb_train_ordered = lgb.Dataset(train_pd[features], label=train_pd['ordered'], categorical_feature=cat_features if cat_features else None)\n",
        "lgb_valid_ordered = lgb.Dataset(val_pd[features], label=val_pd['ordered'], categorical_feature=cat_features if cat_features else None)\n",
        "\n",
        "lgb_train_clicked = lgb.Dataset(train_pd[features], label=train_pd['clicked'], categorical_feature=cat_features if cat_features else None)\n",
        "lgb_valid_clicked = lgb.Dataset(val_pd[features], label=val_pd['clicked'], categorical_feature=cat_features if cat_features else None)\n",
        "\n",
        "\n",
        "# --- Adım 6: Model Eğitimi ve Kaydetme ---\n",
        "params = { \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\", \"learning_rate\": 0.03, \"num_leaves\": 64, \"verbose\": -1, \"seed\": 42 }\n",
        "callbacks = [lgb.early_stopping(100, verbose=True)]\n",
        "MODEL_DIR = \"models_4_doldur_FiyatUygunluk\" # Klasör adını değiştirmedim, üzerine yazacak\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(\"\\n'ordered' modeli eğitiliyor...\")\n",
        "ordered_counts = train_pd['ordered'].value_counts()\n",
        "params['scale_pos_weight'] = ordered_counts[0] / ordered_counts[1]\n",
        "model_ordered = lgb.train(params, lgb_train_ordered, valid_sets=[lgb_train_ordered, lgb_valid_ordered], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_ordered_path = os.path.join(MODEL_DIR, \"model_ordered_v6_no_price_features.txt\") # Dosya adını güncelledim\n",
        "model_ordered.save_model(model_ordered_path)\n",
        "print(f\"✅ 'ordered' modeli '{model_ordered_path}' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n'clicked' modeli eğitiliyor...\")\n",
        "clicked_counts = train_pd['clicked'].value_counts()\n",
        "params['scale_pos_weight'] = clicked_counts[0] / clicked_counts[1]\n",
        "model_clicked = lgb.train(params, lgb_train_clicked, valid_sets=[lgb_train_clicked, lgb_valid_clicked], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_clicked_path = os.path.join(MODEL_DIR, \"model_clicked_v6_no_price_features.txt\") # Dosya adını güncelledim\n",
        "model_clicked.save_model(model_clicked_path)\n",
        "print(f\"✅ 'clicked' modeli '{model_clicked_path}' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n--- Eğitim ve Kaydetme İşlemi Tamamlandı! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3tquE-TIwE5",
        "outputId": "d14739c5-0db9-4574-982a-bf8f1c11c31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 1: Model Eğitme (Fiyat Özellikleri ÇIKARILDI, Zamansal Özellikler KALDI) ---\n",
            "Eğitim verisi yükleniyor: /content/Trendyol Veri/enriched_train_data_FINAL.parquet\n",
            "Toplam eğitim verisi boyutu: (2773805, 37)\n",
            "Zamansal özellikler üretiliyor ve veri zamana göre ayrılıyor...\n",
            "Train parçası boyutu: (2357734, 39)\n",
            "Validation parçası boyutu: (416071, 39)\n",
            "✅ Fiyat etkileşim özellikleri bu denemede kullanılmayacaktır.\n",
            "Veri setleri Pandas'a çevriliyor ve model için hazırlanıyor...\n",
            "Kullanılacak özellik sayısı: 25\n",
            "Güncel Kategorik özellik sayısı: 4\n",
            "lgb.Dataset nesneleri oluşturuluyor...\n",
            "Son Kategorik özellik sayısı: 4\n",
            "\n",
            "'ordered' modeli eğitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttrain's auc: 0.764107\tvalid's auc: 0.700363\n",
            "✅ 'ordered' modeli 'models_4_doldur_FiyatUygunluk/model_ordered_v6_no_price_features.txt' olarak kaydedildi.\n",
            "\n",
            "'clicked' modeli eğitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[127]\ttrain's auc: 0.712264\tvalid's auc: 0.659234\n",
            "✅ 'clicked' modeli 'models_4_doldur_FiyatUygunluk/model_clicked_v6_no_price_features.txt' olarak kaydedildi.\n",
            "\n",
            "--- Eğitim ve Kaydetme İşlemi Tamamlandı! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 2** data sızıntısı giderilmiş"
      ],
      "metadata": {
        "id": "vZ1SGdM0Jid1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "from trendyol_metric_group_auc import score\n",
        "\n",
        "print(\"--- KOD 2: Nihai Tahmin ve Skorlama (Fiyat Özellikleri ÇIKARILMIŞ) ---\")\n",
        "\n",
        "# --- Adım 1: Gerekli Dosyaları Yükleme ---\n",
        "VAL_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "TEST_PATH = \"/content/Trendyol Veri/enriched_test_data_FINAL.parquet\"\n",
        "MODELS_DIR = \"models_4_doldur_FiyatUygunluk\" # Model klasörü aynı kaldı\n",
        "# Düzeltme: Model dosya adlarını güncelledik (no_price_features versiyonları)\n",
        "MODEL_ORDERED_PATH = os.path.join(MODELS_DIR, 'model_ordered_v6_no_price_features.txt')\n",
        "MODEL_CLICKED_PATH = os.path.join(MODELS_DIR, 'model_clicked_v6_no_price_features.txt')\n",
        "\n",
        "\n",
        "print(\"Veriler ve kaydedilmiş v6 modelleri yükleniyor...\")\n",
        "val_full_df = pl.read_parquet(VAL_PATH)\n",
        "test_df_pl = pl.read_parquet(TEST_PATH)\n",
        "model_ordered = lgb.Booster(model_file=MODEL_ORDERED_PATH)\n",
        "model_clicked = lgb.Booster(model_file=MODEL_CLICKED_PATH)\n",
        "print(\"✅ Yükleme tamamlandı.\")\n",
        "\n",
        "\n",
        "# --- Adım 2: Lokal Skor İçin Validation Seti Hazırlama ---\n",
        "print(\"\\nLokal skor için validation seti hazırlanıyor...\")\n",
        "if \"ts_hour\" in val_full_df.columns and val_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    val_full_df = val_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "\n",
        "# Zamansal özellikleri ekle (Bunlar hala kullanılıyor)\n",
        "val_full_df = val_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "\n",
        "# Düzeltme: Fiyat özelliği ekleme ve işleme adımlarını ÇIKARDIK\n",
        "\n",
        "\n",
        "val_full_df = val_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(val_full_df) * 0.85)\n",
        "# Pandas'a çevir ve sayısal null'ları 0 doldur (fiyat özellikleri artık olmadığı için genel fillna(0) güvenli)\n",
        "val_pd = val_full_df[split_index:].to_pandas()\n",
        "\n",
        "# Sayısal sütunlardaki NaN/Inf değerlerini yönetme ve 0 ile doldurma\n",
        "numeric_cols = val_pd.select_dtypes(include=np.number).columns.tolist()\n",
        "for col in numeric_cols:\n",
        "    val_pd[col] = val_pd[col].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "# Düzeltme: exclude_cols listesi eğitimdekiyle aynı (fiyat özellikleri çıkarılmış hali)\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\", \"price_vs_user_avg_ratio\", \"user_avg_order_price\"]\n",
        "features = [col for col in val_pd.columns if col not in exclude_cols] # Düzeltme: features listesi exclude_cols'a göre güncellendi\n",
        "\n",
        "string_cols = [col for col in val_pd.columns if val_pd[col].dtype == 'object']\n",
        "temporal_cols_as_cat = [\"hour_of_day\", \"day_of_week\"]\n",
        "cols_to_categorize = string_cols + temporal_cols_as_cat\n",
        "for col in cols_to_categorize:\n",
        "    if col in val_pd.columns:\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "# Tahmin üret\n",
        "val_pd['p_order'] = model_ordered.predict(val_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "val_pd['p_click'] = model_clicked.predict(val_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "val_pd['final_score'] = 0.7 * val_pd['p_order'] + 0.3 * val_pd['p_click']\n",
        "\n",
        "# Skor fonksiyonu için gerekli DataFrame'leri oluştur\n",
        "val_solution = val_pd.groupby('session_id', observed=False).agg(ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), all_items=('content_id_hashed', ' '.join)).reset_index()\n",
        "val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id', observed=False)['content_id_hashed'].apply(' '.join).reset_index()\n",
        "val_submission.rename(columns={'content_id_hashed': 'prediction'}, inplace=True)\n",
        "\n",
        "try:\n",
        "    local_final_score = score(val_solution, val_submission, 'session_id')\n",
        "    print(\"\\n-------------------------------------------\")\n",
        "    print(f\"🏆 LOKAL SKORUNUZ (Fiyat Özellikleri Çıkarılmış): {local_final_score:.5f}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "except Exception as e:\n",
        "    print(f\"Skor hesaplanırken bir hata oluştu: {e}\")\n",
        "\n",
        "\n",
        "# --- Adım 3: Kaggle için Submission Dosyası Oluşturma ---\n",
        "print(\"\\nTest verisi hazırlanıyor ve Kaggle için submission dosyası oluşturuluyor...\")\n",
        "if \"ts_hour\" in test_df_pl.columns and test_df_pl[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    test_df_pl = test_df_pl.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "# Zamansal özellikleri ekle (Bunlar hala kullanılıyor)\n",
        "test_df_pl = test_df_pl.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "# Düzeltme: Fiyat özelliği ekleme ve işleme adımlarını ÇIKARDIK\n",
        "\n",
        "# Pandas'a çevir ve sayısal null'ları 0 doldur\n",
        "test_pd = test_df_pl.to_pandas()\n",
        "numeric_cols_test = test_pd.select_dtypes(include=np.number).columns.tolist()\n",
        "for col in numeric_cols_test:\n",
        "     test_pd[col] = test_pd[col].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "\n",
        "for col in cols_to_categorize:\n",
        "    if col in test_pd.columns:\n",
        "        test_pd[col] = test_pd[col].astype('category')\n",
        "\n",
        "# Features listesi eğitimdekiyle aynı olmalı\n",
        "# exclude_cols listesi eğitimdekiyle aynı (fiyat özellikleri çıkarılmış hali)\n",
        "exclude_cols_test = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\", \"price_vs_user_avg_ratio\", \"user_avg_order_price\"]\n",
        "features_test = [col for col in test_pd.columns if col not in exclude_cols_test] # Düzeltme: features_test listesi exclude_cols_test'e göre güncellendi\n",
        "\n",
        "p_order_test = model_ordered.predict(test_pd[features_test], num_iteration=model_ordered.best_iteration)\n",
        "p_click_test = model_clicked.predict(test_pd[features_test], num_iteration=model_clicked.best_iteration)\n",
        "\n",
        "\n",
        "test_df_pl = test_df_pl.with_columns(\n",
        "    final_score=(0.7 * pl.Series(p_order_test)) + (0.3 * pl.Series(p_click_test))\n",
        ")\n",
        "submission_df = test_df_pl.sort([\"session_id\", \"final_score\"], descending=True).group_by(\"session_id\").agg(\n",
        "    pl.col(\"content_id_hashed\").alias(\"prediction\")\n",
        ").with_columns(\n",
        "    pl.col(\"prediction\").list.join(\" \")\n",
        ")\n",
        "\n",
        "expected_rows = 18589\n",
        "actual_rows = submission_df.shape[0]\n",
        "print(f\"\\nOluşturulan submission satır sayısı: {actual_rows} (Beklenen: {expected_rows})\")\n",
        "if actual_rows == expected_rows: print(\"✅ Satır sayısı doğru.\")\n",
        "else: print(\"❌ UYARI: Satır sayısı yanlış!\")\n",
        "\n",
        "submission_path = \"submission_no_price_features.csv\" # Dosya adını güncelledim\n",
        "submission_df.write_csv(submission_path)\n",
        "print(f\"\\nSubmission dosyası '{submission_path}' olarak kaydedildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjPYqB-KJqAg",
        "outputId": "a44a1393-68ec-436d-a42b-9a3b0c3d5cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 2: Nihai Tahmin ve Skorlama (Fiyat Özellikleri ÇIKARILMIŞ) ---\n",
            "Veriler ve kaydedilmiş v6 modelleri yükleniyor...\n",
            "✅ Yükleme tamamlandı.\n",
            "\n",
            "Lokal skor için validation seti hazırlanıyor...\n",
            "Ordered AUC:  0.6504243273845817\n",
            "Clicked AUC:  0.5914654200400459\n",
            "\n",
            "-------------------------------------------\n",
            "🏆 LOKAL SKORUNUZ (Fiyat Özellikleri Çıkarılmış): 0.63274\n",
            "-------------------------------------------\n",
            "\n",
            "Test verisi hazırlanıyor ve Kaggle için submission dosyası oluşturuluyor...\n",
            "\n",
            "Oluşturulan submission satır sayısı: 18589 (Beklenen: 18589)\n",
            "✅ Satır sayısı doğru.\n",
            "\n",
            "Submission dosyası 'submission_no_price_features.csv' olarak kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deneme 4 Alaka Düzeyi"
      ],
      "metadata": {
        "id": "WqLtGSFGdSJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 1** Alaka Düzeyine göre eğitmek ve modelleri kaydetmek"
      ],
      "metadata": {
        "id": "wTgktMeGdXs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"--- KOD 1: Model Eğitme (Yeni Alaka Düzeyi Özellikleriyle) ---\")\n",
        "\n",
        "# --- Adım 1: Veri Yükleme ---\n",
        "TRAIN_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "train_full_df = pl.read_parquet(TRAIN_PATH)\n",
        "print(f\"Toplam eğitim verisi boyutu: {train_full_df.shape}\")\n",
        "\n",
        "# --- Adım 2: Özellik Mühendisliği ve Veri Ayırma ---\n",
        "print(\"Ön özellikler üretiliyor...\")\n",
        "if \"ts_hour\" in train_full_df.columns and train_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    train_full_df = train_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "\n",
        "# 2.1 Zamansal Özellikler\n",
        "train_full_df = train_full_df.with_columns(\n",
        "    pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"),\n",
        "    pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\")\n",
        ")\n",
        "\n",
        "# 2.2 Alaka Düzeyi (Relevance) Özellikleri\n",
        "print(\"Yeni Alaka Düzeyi (Relevance) özellikleri üretiliyor...\")\n",
        "train_full_df = train_full_df.with_columns(pl.col(\"search_term_normalized\").fill_null(\"\"), pl.col(\"cv_tags\").fill_null(\"\"))\n",
        "search_words = pl.col(\"search_term_normalized\").str.split(\" \")\n",
        "tag_words = pl.col(\"cv_tags\").str.split(\" \")\n",
        "intersection_len = search_words.list.set_intersection(tag_words).list.len()\n",
        "union_len = search_words.list.set_union(tag_words).list.len()\n",
        "train_full_df = train_full_df.with_columns(\n",
        "    term_in_tags_match = (intersection_len > 0).cast(pl.Int8),\n",
        "    jaccard_similarity_tags = pl.when(union_len > 0).then(intersection_len / union_len).otherwise(0.0)\n",
        ")\n",
        "\n",
        "# 2.3 Train/Validation Ayrımı (ÖZELLİK ÜRETİMİNDEN ÖNCE)\n",
        "train_full_df = train_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(train_full_df) * 0.85)\n",
        "train_df, val_df = train_full_df[:split_index], train_full_df[split_index:]\n",
        "print(f\"Train parçası boyutu: {train_df.shape}\")\n",
        "print(f\"Validation parçası boyutu: {val_df.shape}\")\n",
        "\n",
        "# 2.4 Fiyat Etkileşim Özelliğini SADECE Train Setinden Hesaplama\n",
        "print(\"Fiyat etkileşim özelliği, SADECE train verisi üzerinden hesaplanıyor...\")\n",
        "price_features_lazy = pl.scan_parquet(TRAIN_PATH).select([\"content_id_hashed\", \"selling_price\", \"update_date\"])\n",
        "latest_price_features = price_features_lazy.sort(\"update_date\", descending=True).group_by(\"content_id_hashed\").first().collect()\n",
        "user_orders = train_df.filter(pl.col(\"ordered\") == 1).select([\"user_id_hashed\", \"content_id_hashed\"])\n",
        "user_orders_with_price = user_orders.join(latest_price_features, on=\"content_id_hashed\", how=\"left\")\n",
        "agg_user_price_features = user_orders_with_price.group_by(\"user_id_hashed\").agg(pl.mean(\"selling_price\").alias(\"user_avg_order_price\"))\n",
        "FEATURES_DIR = \"features\"\n",
        "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
        "agg_user_price_features.write_parquet(os.path.join(FEATURES_DIR, \"user_price_features_final.parquet\"))\n",
        "print(f\"✅ Sızıntısız kullanıcı fiyat özellikleri kaydedildi.\")\n",
        "\n",
        "# 2.5 Özellikleri Ekleme ve Akıllı Boşluk Doldurma\n",
        "global_avg_order_price = agg_user_price_features.select(pl.mean(\"user_avg_order_price\")).item()\n",
        "train_df = train_df.join(agg_user_price_features, on=\"user_id_hashed\", how=\"left\").with_columns(pl.col(\"user_avg_order_price\").fill_null(global_avg_order_price)).with_columns((pl.col(\"selling_price\") / pl.col(\"user_avg_order_price\")).alias(\"price_vs_user_avg_ratio\"))\n",
        "val_df = val_df.join(agg_user_price_features, on=\"user_id_hashed\", how=\"left\").with_columns(pl.col(\"user_avg_order_price\").fill_null(global_avg_order_price)).with_columns((pl.col(\"selling_price\") / pl.col(\"user_avg_order_price\")).alias(\"price_vs_user_avg_ratio\"))\n",
        "\n",
        "# --- Adım 3: Veri Hazırlama ve AYRI ÖZELLİK SETLERİ OLUŞTURMA ---\n",
        "print(\"Veri setleri Pandas'a çevriliyor ve ayrı özellik setleri oluşturuluyor...\")\n",
        "train_pd = train_df.to_pandas().fillna(0)\n",
        "val_pd = val_df.to_pandas().fillna(0)\n",
        "string_cols = [col for col in train_pd.columns if train_pd[col].dtype == 'object']\n",
        "temporal_cols_as_cat = [\"hour_of_day\", \"day_of_week\"]\n",
        "cols_to_categorize = string_cols + temporal_cols_as_cat\n",
        "for col in cols_to_categorize:\n",
        "    if col in train_pd.columns:\n",
        "        train_pd[col], val_pd[col] = train_pd[col].astype('category'), val_pd[col].astype('category')\n",
        "\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features_base = [col for col in train_pd.columns if col not in exclude_cols]\n",
        "price_features_to_exclude = [\"user_avg_order_price\", \"price_vs_user_avg_ratio\"]\n",
        "features_for_ordered = [f for f in features_base if f not in price_features_to_exclude]\n",
        "features_for_clicked = features_base\n",
        "cat_features_ordered = [f for f in features_for_ordered if isinstance(train_pd[f].dtype, pd.CategoricalDtype)]\n",
        "cat_features_clicked = [f for f in features_for_clicked if isinstance(train_pd[f].dtype, pd.CategoricalDtype)]\n",
        "print(f\"'ordered' modeli için özellik sayısı: {len(features_for_ordered)}\")\n",
        "print(f\"'clicked' modeli için özellik sayısı: {len(features_for_clicked)}\")\n",
        "\n",
        "# --- Adım 4: lgb.Dataset Oluşturma ---\n",
        "print(\"Modele özel lgb.Dataset nesneleri oluşturuluyor...\")\n",
        "lgb_train_ordered = lgb.Dataset(train_pd[features_for_ordered], label=train_pd['ordered'], categorical_feature=cat_features_ordered)\n",
        "lgb_valid_ordered = lgb.Dataset(val_pd[features_for_ordered], label=val_pd['ordered'], categorical_feature=cat_features_ordered)\n",
        "lgb_train_clicked = lgb.Dataset(train_pd[features_for_clicked], label=train_pd['clicked'], categorical_feature=cat_features_clicked)\n",
        "lgb_valid_clicked = lgb.Dataset(val_pd[features_for_clicked], label=val_pd['clicked'], categorical_feature=cat_features_clicked)\n",
        "\n",
        "# --- Adım 5: Model Eğitimi ve Kaydetme ---\n",
        "params = { \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\", \"learning_rate\": 0.03, \"num_leaves\": 64, \"verbose\": -1, \"seed\": 42 }\n",
        "callbacks = [lgb.early_stopping(100, verbose=True)]\n",
        "MODEL_DIR = \"models_6_doldur_AlakaDuzeyi\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(\"\\n'ordered' modeli (sınırlı özelliklerle) eğitiliyor...\")\n",
        "ordered_counts = train_pd['ordered'].value_counts()\n",
        "params['scale_pos_weight'] = ordered_counts[0] / ordered_counts[1]\n",
        "model_ordered = lgb.train(params, lgb_train_ordered, valid_sets=[lgb_train_ordered, lgb_valid_ordered], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_ordered.save_model(os.path.join(MODEL_DIR, \"model_ordered_v6_relevance.txt\"))\n",
        "print(f\"✅ 'ordered' modeli '{MODEL_DIR}/model_ordered_v6_relevance.txt' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n'clicked' modeli (tüm özelliklerle) eğitiliyor...\")\n",
        "clicked_counts = train_pd['clicked'].value_counts()\n",
        "params['scale_pos_weight'] = clicked_counts[0] / clicked_counts[1]\n",
        "model_clicked = lgb.train(params, lgb_train_clicked, valid_sets=[lgb_train_clicked, lgb_valid_clicked], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_clicked.save_model(os.path.join(MODEL_DIR, \"model_clicked_v6_relevance.txt\"))\n",
        "print(f\"✅ 'clicked' modeli '{MODEL_DIR}/model_clicked_v6_relevance.txt' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n--- Eğitim ve Kaydetme İşlemi Tamamlandı! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-7VfOPOddum",
        "outputId": "79174104-acae-4961-8629-4599ff6d3eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 1: Model Eğitme (Yeni Alaka Düzeyi Özellikleriyle) ---\n",
            "Toplam eğitim verisi boyutu: (2773805, 37)\n",
            "Ön özellikler üretiliyor...\n",
            "Yeni Alaka Düzeyi (Relevance) özellikleri üretiliyor...\n",
            "Train parçası boyutu: (2357734, 41)\n",
            "Validation parçası boyutu: (416071, 41)\n",
            "Fiyat etkileşim özelliği, SADECE train verisi üzerinden hesaplanıyor...\n",
            "✅ Sızıntısız kullanıcı fiyat özellikleri kaydedildi.\n",
            "Veri setleri Pandas'a çevriliyor ve ayrı özellik setleri oluşturuluyor...\n",
            "'ordered' modeli için özellik sayısı: 27\n",
            "'clicked' modeli için özellik sayısı: 29\n",
            "Modele özel lgb.Dataset nesneleri oluşturuluyor...\n",
            "\n",
            "'ordered' modeli (sınırlı özelliklerle) eğitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttrain's auc: 0.764107\tvalid's auc: 0.700363\n",
            "✅ 'ordered' modeli 'models_6_doldur_AlakaDuzeyi/model_ordered_v6_relevance.txt' olarak kaydedildi.\n",
            "\n",
            "'clicked' modeli (tüm özelliklerle) eğitiliyor...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[122]\ttrain's auc: 0.732438\tvalid's auc: 0.65745\n",
            "✅ 'clicked' modeli 'models_6_doldur_AlakaDuzeyi/model_clicked_v6_relevance.txt' olarak kaydedildi.\n",
            "\n",
            "--- Eğitim ve Kaydetme İşlemi Tamamlandı! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 2** Alaka Düzeyi ile test etme ve submissions oluşturma"
      ],
      "metadata": {
        "id": "R-7_GpBfeWfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "from trendyol_metric_group_auc import score\n",
        "\n",
        "print(\"--- KOD 2: Nihai Tahmin ve Skorlama (Yeni Alaka Düzeyi Özellikleriyle) ---\")\n",
        "\n",
        "# --- Adım 1: Gerekli Dosyaları Yükleme ---\n",
        "VAL_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "TEST_PATH = \"/content/Trendyol Veri/enriched_test_data_FINAL.parquet\"\n",
        "FEATURES_DIR = \"features\"\n",
        "MODELS_DIR = \"models_6_doldur_AlakaDuzeyi\"\n",
        "USER_PRICE_FEATURES_PATH = os.path.join(FEATURES_DIR, \"user_price_features_final.parquet\")\n",
        "MODEL_ORDERED_PATH = os.path.join(MODELS_DIR, 'model_ordered_v6_relevance.txt')\n",
        "MODEL_CLICKED_PATH = os.path.join(MODELS_DIR, 'model_clicked_v6_relevance.txt')\n",
        "\n",
        "print(\"Veriler, özellikler ve kaydedilmiş nihai modeller yükleniyor...\")\n",
        "val_full_df = pl.read_parquet(VAL_PATH)\n",
        "test_df_pl = pl.read_parquet(TEST_PATH)\n",
        "user_price_features = pl.read_parquet(USER_PRICE_FEATURES_PATH)\n",
        "model_ordered = lgb.Booster(model_file=MODEL_ORDERED_PATH)\n",
        "model_clicked = lgb.Booster(model_file=MODEL_CLICKED_PATH)\n",
        "print(\"✅ Yükleme tamamlandı.\")\n",
        "\n",
        "# --- Adım 2: Lokal Skor İçin Validation Seti Hazırlama ---\n",
        "print(\"\\nLokal skor için validation seti hazırlanıyor...\")\n",
        "# 2.1 Tüm Özellikleri Ekle\n",
        "if \"ts_hour\" in val_full_df.columns and val_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    val_full_df = val_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "val_full_df = val_full_df.with_columns(pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"), pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\"))\n",
        "val_full_df = val_full_df.with_columns(pl.col(\"search_term_normalized\").fill_null(\"\"), pl.col(\"cv_tags\").fill_null(\"\"))\n",
        "search_words_val = pl.col(\"search_term_normalized\").str.split(\" \")\n",
        "tag_words_val = pl.col(\"cv_tags\").str.split(\" \")\n",
        "intersection_len_val = search_words_val.list.set_intersection(tag_words_val).list.len()\n",
        "union_len_val = search_words_val.list.set_union(tag_words_val).list.len()\n",
        "val_full_df = val_full_df.with_columns(term_in_tags_match=(intersection_len_val > 0).cast(pl.Int8), jaccard_similarity_tags=pl.when(union_len_val > 0).then(intersection_len_val / union_len_val).otherwise(0.0))\n",
        "global_avg_order_price = user_price_features.select(pl.mean(\"user_avg_order_price\")).item()\n",
        "val_full_df = val_full_df.join(user_price_features, on=\"user_id_hashed\", how=\"left\").with_columns(pl.col(\"user_avg_order_price\").fill_null(global_avg_order_price)).with_columns((pl.col(\"selling_price\") / pl.col(\"user_avg_order_price\")).alias(\"price_vs_user_avg_ratio\"))\n",
        "\n",
        "# 2.2 Veriyi Ayır ve Hazırla\n",
        "val_full_df = val_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(val_full_df) * 0.85)\n",
        "val_pd = val_full_df[split_index:].to_pandas().fillna(0)\n",
        "\n",
        "# 2.3 Ayrı Özellik Setlerini Tanımla\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features_base = [col for col in val_pd.columns if col not in exclude_cols]\n",
        "price_features_to_exclude = [\"user_avg_order_price\", \"price_vs_user_avg_ratio\"]\n",
        "features_for_ordered = [f for f in features_base if f not in price_features_to_exclude]\n",
        "features_for_clicked = features_base\n",
        "string_cols = [col for col in val_pd.columns if val_pd[col].dtype == 'object']\n",
        "temporal_cols_as_cat = [\"hour_of_day\", \"day_of_week\"]\n",
        "cols_to_categorize = string_cols + temporal_cols_as_cat\n",
        "for col in cols_to_categorize:\n",
        "    if col in val_pd.columns:\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "# 2.4 Tahmin ve Skorlama\n",
        "val_pd['p_order'] = model_ordered.predict(val_pd[features_for_ordered], num_iteration=model_ordered.best_iteration)\n",
        "val_pd['p_click'] = model_clicked.predict(val_pd[features_for_clicked], num_iteration=model_clicked.best_iteration)\n",
        "val_pd['final_score'] = 0.7 * val_pd['p_order'] + 0.3 * val_pd['p_click']\n",
        "val_solution = val_pd.groupby('session_id').agg(ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), all_items=('content_id_hashed', ' '.join)).reset_index()\n",
        "val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n",
        "val_submission.rename(columns={'content_id_hashed': 'prediction'}, inplace=True)\n",
        "try:\n",
        "    local_final_score = score(val_solution, val_submission, 'session_id')\n",
        "    print(\"\\n-------------------------------------------\")\n",
        "    print(f\"🏆 LOKAL SKORUNUZ (Yeni Özelliklerle): {local_final_score:.5f}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "except Exception as e:\n",
        "    print(f\"Skor hesaplanırken bir hata oluştu: {e}\")\n",
        "\n",
        "# --- Adım 3: Kaggle için Submission Dosyası Oluşturma ---\n",
        "print(\"\\nTest verisi hazırlanıyor ve Kaggle için submission dosyası oluşturuluyor...\")\n",
        "if \"ts_hour\" in test_df_pl.columns and test_df_pl[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    test_df_pl = test_df_pl.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "test_df_pl = test_df_pl.with_columns(pl.col(\"ts_hour\").dt.hour().alias(\"hour_of_day\"), pl.col(\"ts_hour\").dt.weekday().alias(\"day_of_week\"))\n",
        "test_df_pl = test_df_pl.with_columns(pl.col(\"search_term_normalized\").fill_null(\"\"), pl.col(\"cv_tags\").fill_null(\"\"))\n",
        "search_words_test = pl.col(\"search_term_normalized\").str.split(\" \")\n",
        "tag_words_test = pl.col(\"cv_tags\").str.split(\" \")\n",
        "intersection_len_test = search_words_test.list.set_intersection(tag_words_test).list.len()\n",
        "union_len_test = search_words_test.list.set_union(tag_words_test).list.len()\n",
        "test_df_pl = test_df_pl.with_columns(term_in_tags_match=(intersection_len_test > 0).cast(pl.Int8), jaccard_similarity_tags=pl.when(union_len_test > 0).then(intersection_len_test / union_len_test).otherwise(0.0))\n",
        "test_df_pl = test_df_pl.join(user_price_features, on=\"user_id_hashed\", how=\"left\").with_columns(pl.col(\"user_avg_order_price\").fill_null(global_avg_order_price)).with_columns((pl.col(\"selling_price\") / pl.col(\"user_avg_order_price\")).alias(\"price_vs_user_avg_ratio\"))\n",
        "\n",
        "test_pd = test_df_pl.to_pandas().fillna(0)\n",
        "for col in cols_to_categorize:\n",
        "    if col in test_pd.columns:\n",
        "        test_pd[col] = test_pd[col].astype('category')\n",
        "\n",
        "p_order_test = model_ordered.predict(test_pd[features_for_ordered], num_iteration=model_ordered.best_iteration)\n",
        "p_click_test = model_clicked.predict(test_pd[features_for_clicked], num_iteration=model_clicked.best_iteration)\n",
        "\n",
        "test_df_pl = test_df_pl.with_columns(final_score=(0.7 * pl.Series(p_order_test)) + (0.3 * pl.Series(p_click_test)))\n",
        "submission_df = test_df_pl.sort([\"session_id\", \"final_score\"], descending=True).group_by(\"session_id\").agg(pl.col(\"content_id_hashed\").alias(\"prediction\")).with_columns(pl.col(\"prediction\").list.join(\" \"))\n",
        "expected_rows = 18589\n",
        "actual_rows = submission_df.shape[0]\n",
        "print(f\"\\nOluşturulan submission satır sayısı: {actual_rows} (Beklenen: {expected_rows})\")\n",
        "if actual_rows == expected_rows: print(\"✅ Satır sayısı doğru.\")\n",
        "else: print(\"❌ UYARI: Satır sayısı yanlış!\")\n",
        "submission_path = \"submission.csv\"\n",
        "submission_df.write_csv(submission_path)\n",
        "print(f\"\\nSubmission dosyası '{submission_path}' olarak kaydedildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un0mBHjRebKL",
        "outputId": "abf0ebc4-fcff-4096-f8bf-777a2087d210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 2: Nihai Tahmin ve Skorlama (Yeni Alaka Düzeyi Özellikleriyle) ---\n",
            "Veriler, özellikler ve kaydedilmiş nihai modeller yükleniyor...\n",
            "✅ Yükleme tamamlandı.\n",
            "\n",
            "Lokal skor için validation seti hazırlanıyor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1787726108.py:65: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  val_solution = val_pd.groupby('session_id').agg(ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])), all_items=('content_id_hashed', ' '.join)).reset_index()\n",
            "/tmp/ipython-input-1787726108.py:66: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ordered AUC:  0.6501352808701953\n",
            "Clicked AUC:  0.5901990785840607\n",
            "\n",
            "-------------------------------------------\n",
            "🏆 LOKAL SKORUNUZ (Yeni Özelliklerle): 0.63215\n",
            "-------------------------------------------\n",
            "\n",
            "Test verisi hazırlanıyor ve Kaggle için submission dosyası oluşturuluyor...\n",
            "\n",
            "Oluşturulan submission satır sayısı: 18589 (Beklenen: 18589)\n",
            "✅ Satır sayısı doğru.\n",
            "\n",
            "Submission dosyası 'submission.csv' olarak kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deneme 5 Hiperparametre Optimizasyonu"
      ],
      "metadata": {
        "id": "TRdXhjYphJoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 1** Optuna kullnarak hiperparemetre optimizasyonu yapmak"
      ],
      "metadata": {
        "id": "9YKrMku8hN2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M9cemRajRNb",
        "outputId": "71cf23cd-8941-4271-bc31-a2141419c655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/395.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m389.1/395.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/247.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import os\n",
        "import optuna # Optuna kütüphanesini ekledik\n",
        "\n",
        "print(\"--- KOD 1: Baseline Model İçin Optuna ile HPO ve Eğitim ---\")\n",
        "\n",
        "# --- Adım 1: Veri Yükleme ---\n",
        "TRAIN_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "print(f\"Eğitim verisi yükleniyor: {TRAIN_PATH}\")\n",
        "train_full_df = pl.read_parquet(TRAIN_PATH)\n",
        "print(f\"Toplam eğitim verisi boyutu: {train_full_df.shape}\")\n",
        "\n",
        "\n",
        "# --- Adım 2: Veriyi Zamana Göre Ayırma (Train / Validation Split) ---\n",
        "print(\"Eğitim verisi, zamana göre sıralanıp yüzdesel olarak ayrılıyor...\")\n",
        "if \"ts_hour\" in train_full_df.columns and train_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    train_full_df = train_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "train_full_df = train_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(train_full_df) * 0.85)\n",
        "train_df = train_full_df[:split_index]\n",
        "val_df = train_full_df[split_index:]\n",
        "print(f\"Train parçası boyutu: {train_df.shape}\")\n",
        "print(f\"Validation parçası boyutu: {val_df.shape}\")\n",
        "\n",
        "\n",
        "# --- Adım 3: Veri Hazırlama ve Tip Dönüşümü ---\n",
        "print(\"Veri setleri Pandas'a çevriliyor ve model için hazırlanıyor...\")\n",
        "train_pd = train_df.to_pandas().fillna(0)\n",
        "val_pd = val_df.to_pandas().fillna(0)\n",
        "for col in train_pd.columns:\n",
        "    if train_pd[col].dtype == 'object':\n",
        "        train_pd[col] = train_pd[col].astype('category')\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features = [col for col in train_pd.columns if col not in exclude_cols]\n",
        "cat_features = [col for col in features if isinstance(train_pd[col].dtype, pd.CategoricalDtype)]\n",
        "print(f\"Kullanılacak özellik sayısı: {len(features)}\")\n",
        "\n",
        "\n",
        "# --- YENİ Adım 4: Optuna Optimizasyon Fonksiyonu ('ordered' için) ---\n",
        "# Bu fonksiyon, Optuna'nın her denemede çalıştıracağı model eğitim sürecini tanımlar\n",
        "def objective_ordered(trial):\n",
        "    # Optuna'nın deneyeceği hiperparametre aralıklarını belirliyoruz\n",
        "    params = {\n",
        "        \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\",\n",
        "        \"seed\": 42, \"verbose\": -1, \"n_jobs\": -1,\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
        "    }\n",
        "    # Sınıf dengesizliği için ağırlığı ekle\n",
        "    ordered_counts = train_pd['ordered'].value_counts()\n",
        "    params['scale_pos_weight'] = ordered_counts[0] / ordered_counts[1]\n",
        "\n",
        "    # Modeli eğit\n",
        "    lgb_train = lgb.Dataset(train_pd[features], label=train_pd['ordered'], categorical_feature=cat_features)\n",
        "    lgb_valid = lgb.Dataset(val_pd[features], label=val_pd['ordered'], categorical_feature=cat_features)\n",
        "    model = lgb.train(params, lgb_train, valid_sets=[lgb_valid], valid_names=[\"valid\"], num_boost_round=2000, callbacks=[lgb.early_stopping(50, verbose=False)])\n",
        "\n",
        "    # Optuna, bu döndürülen 'valid's auc' skorunu maksimize etmeye çalışacak\n",
        "    return model.best_score['valid']['auc']\n",
        "\n",
        "print(\"\\n'ordered' modeli için hiperparametre optimizasyonu başlıyor...\")\n",
        "# Optimizasyon çalışmasını oluştur ve başlat\n",
        "study_ordered = optuna.create_study(direction='maximize')\n",
        "study_ordered.optimize(objective_ordered, n_trials=50) # Deneme sayısını ihtiyaca göre ayarlayabilirsiniz\n",
        "\n",
        "# En iyi sonuçları al\n",
        "best_params_ordered = study_ordered.best_params\n",
        "print(\"En iyi 'ordered' parametreleri bulundu:\", best_params_ordered)\n",
        "print(f\"En iyi validation AUC skoru: {study_ordered.best_value}\")\n",
        "\n",
        "# --- Adım 5: Nihai Modelleri En İyi Parametrelerle Eğitme ---\n",
        "print(\"\\nNihai modeller en iyi bulunan parametrelerle eğitiliyor...\")\n",
        "# Temel sabit parametreler\n",
        "base_params = { \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\", \"verbose\": -1, \"seed\": 42, \"n_jobs\": -1 }\n",
        "callbacks = [lgb.early_stopping(100, verbose=True)]\n",
        "MODEL_DIR = \"models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# 5.1 'ordered' modeli (Optimize Edilmiş Parametrelerle)\n",
        "final_params_ordered = base_params.copy()\n",
        "final_params_ordered.update(best_params_ordered) # Bulunan en iyi parametreleri ekle\n",
        "ordered_counts = train_pd['ordered'].value_counts()\n",
        "final_params_ordered['scale_pos_weight'] = ordered_counts[0] / ordered_counts[1]\n",
        "lgb_train_ord = lgb.Dataset(train_pd[features], label=train_pd['ordered'], categorical_feature=cat_features)\n",
        "lgb_valid_ord = lgb.Dataset(val_pd[features], label=val_pd['ordered'], categorical_feature=cat_features)\n",
        "model_ordered = lgb.train(final_params_ordered, lgb_train_ord, valid_sets=[lgb_train_ord, lgb_valid_ord], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_ordered.save_model(os.path.join(MODEL_DIR, \"model_ordered_hpo_baseline.txt\"))\n",
        "print(f\"✅ Optimize 'ordered' modeli '{MODEL_DIR}/model_ordered_hpo_baseline.txt' olarak kaydedildi.\")\n",
        "\n",
        "# 5.2 'clicked' modeli (Standart Parametrelerle)\n",
        "print(\"\\n'clicked' modeli eğitiliyor...\")\n",
        "final_params_clicked = base_params.copy()\n",
        "final_params_clicked.update({'learning_rate': 0.03, 'num_leaves': 64}) # Standart iyi parametreler\n",
        "clicked_counts = train_pd['clicked'].value_counts()\n",
        "final_params_clicked['scale_pos_weight'] = clicked_counts[0] / clicked_counts[1]\n",
        "lgb_train_clk = lgb.Dataset(train_pd[features], label=train_pd['clicked'], categorical_feature=cat_features)\n",
        "lgb_valid_clk = lgb.Dataset(val_pd[features], label=val_pd['clicked'], categorical_feature=cat_features)\n",
        "model_clicked = lgb.train(final_params_clicked, lgb_train_clk, valid_sets=[lgb_train_clk, lgb_valid_clk], valid_names=[\"train\", \"valid\"], num_boost_round=2000, callbacks=callbacks)\n",
        "model_clicked.save_model(os.path.join(MODEL_DIR, \"model_clicked_standard_baseline.txt\"))\n",
        "print(f\"✅ 'clicked' modeli '{MODEL_DIR}/model_clicked_standard_baseline.txt' olarak kaydedildi.\")\n",
        "\n",
        "print(\"\\n--- Optimizasyon ve Eğitim İşlemi Tamamlandı! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6r6M86OKhTuY",
        "outputId": "ae0fbffe-b50d-406b-9519-0d89958da842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KOD 1: Baseline Model İçin Optuna ile HPO ve Eğitim ---\n",
            "Eğitim verisi yükleniyor: /content/Trendyol Veri/enriched_train_data_FINAL.parquet\n",
            "Toplam eğitim verisi boyutu: (2773805, 37)\n",
            "Eğitim verisi, zamana göre sıralanıp yüzdesel olarak ayrılıyor...\n",
            "Train parçası boyutu: (2357734, 37)\n",
            "Validation parçası boyutu: (416071, 37)\n",
            "Veri setleri Pandas'a çevriliyor ve model için hazırlanıyor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-10 15:17:21,958] A new study created in memory with name: no-name-c7e75471-104b-45e4-a704-d1eadef2bf47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kullanılacak özellik sayısı: 23\n",
            "\n",
            "'ordered' modeli için hiperparametre optimizasyonu başlıyor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-10 15:18:00,665] Trial 0 finished with value: 0.7013007336326083 and parameters: {'learning_rate': 0.014947202519089677, 'num_leaves': 73, 'feature_fraction': 0.7584936395519258, 'bagging_fraction': 0.81685896433614, 'bagging_freq': 6, 'lambda_l1': 0.16457017997277235, 'lambda_l2': 0.020673747840117242}. Best is trial 0 with value: 0.7013007336326083.\n",
            "[I 2025-08-10 15:18:48,161] Trial 1 finished with value: 0.7119243462315256 and parameters: {'learning_rate': 0.025261338950755174, 'num_leaves': 40, 'feature_fraction': 0.7889317140090981, 'bagging_fraction': 0.9836379980755368, 'bagging_freq': 3, 'lambda_l1': 0.00026920731374345286, 'lambda_l2': 0.03238193256203181}. Best is trial 1 with value: 0.7119243462315256.\n",
            "[I 2025-08-10 15:19:10,344] Trial 2 finished with value: 0.7043060733532196 and parameters: {'learning_rate': 0.060112489822934954, 'num_leaves': 26, 'feature_fraction': 0.6801168600829408, 'bagging_fraction': 0.6337996810831215, 'bagging_freq': 1, 'lambda_l1': 0.0036206062969729802, 'lambda_l2': 4.964194047232771e-07}. Best is trial 1 with value: 0.7119243462315256.\n",
            "[I 2025-08-10 15:19:31,398] Trial 3 finished with value: 0.6994110335300089 and parameters: {'learning_rate': 0.06648518912786645, 'num_leaves': 73, 'feature_fraction': 0.6436961705535847, 'bagging_fraction': 0.850512026756212, 'bagging_freq': 5, 'lambda_l1': 1.2302638089545537e-07, 'lambda_l2': 3.851355097820139e-05}. Best is trial 1 with value: 0.7119243462315256.\n",
            "[I 2025-08-10 15:20:11,336] Trial 4 finished with value: 0.6909834581995229 and parameters: {'learning_rate': 0.045531297505928124, 'num_leaves': 86, 'feature_fraction': 0.9673989737092311, 'bagging_fraction': 0.7974167431563236, 'bagging_freq': 5, 'lambda_l1': 0.00914161913435502, 'lambda_l2': 1.002853848913955e-06}. Best is trial 1 with value: 0.7119243462315256.\n",
            "[I 2025-08-10 15:20:46,537] Trial 5 finished with value: 0.6812341309313784 and parameters: {'learning_rate': 0.05105736410263445, 'num_leaves': 76, 'feature_fraction': 0.8080264931988437, 'bagging_fraction': 0.7397472947830492, 'bagging_freq': 2, 'lambda_l1': 3.3302432970265814, 'lambda_l2': 0.15120476025349047}. Best is trial 1 with value: 0.7119243462315256.\n",
            "[I 2025-08-10 15:24:22,290] Trial 6 finished with value: 0.7101845956285633 and parameters: {'learning_rate': 0.01605299721097892, 'num_leaves': 84, 'feature_fraction': 0.9547594296411026, 'bagging_fraction': 0.9147402973892569, 'bagging_freq': 2, 'lambda_l1': 2.78491741321844e-05, 'lambda_l2': 0.009573921809314023}. Best is trial 1 with value: 0.7119243462315256.\n",
            "[I 2025-08-10 15:26:10,048] Trial 7 finished with value: 0.7132401251561211 and parameters: {'learning_rate': 0.02466423696325306, 'num_leaves': 32, 'feature_fraction': 0.6505893215458181, 'bagging_fraction': 0.9023703757954353, 'bagging_freq': 1, 'lambda_l1': 6.5835732748459925, 'lambda_l2': 0.00016065926456537927}. Best is trial 7 with value: 0.7132401251561211.\n",
            "[I 2025-08-10 15:30:07,445] Trial 8 finished with value: 0.7202131331951266 and parameters: {'learning_rate': 0.0121399179698488, 'num_leaves': 27, 'feature_fraction': 0.7035905481879955, 'bagging_fraction': 0.7554001825932966, 'bagging_freq': 2, 'lambda_l1': 4.2300720497636693e-08, 'lambda_l2': 3.275962333947313e-08}. Best is trial 8 with value: 0.7202131331951266.\n",
            "[I 2025-08-10 15:30:32,709] Trial 9 finished with value: 0.702530732130137 and parameters: {'learning_rate': 0.05179912759775082, 'num_leaves': 26, 'feature_fraction': 0.88720285160763, 'bagging_fraction': 0.8073725140044434, 'bagging_freq': 6, 'lambda_l1': 4.251824861988653e-07, 'lambda_l2': 0.031442033781253396}. Best is trial 8 with value: 0.7202131331951266.\n",
            "[I 2025-08-10 15:31:05,468] Trial 10 finished with value: 0.7044367950911302 and parameters: {'learning_rate': 0.010833331184176353, 'num_leaves': 50, 'feature_fraction': 0.7281630427422774, 'bagging_fraction': 0.6673590695126174, 'bagging_freq': 4, 'lambda_l1': 3.996454287542767e-06, 'lambda_l2': 1.5067048660399392e-08}. Best is trial 8 with value: 0.7202131331951266.\n",
            "[I 2025-08-10 15:34:00,608] Trial 11 finished with value: 0.7151231498383697 and parameters: {'learning_rate': 0.026450655435820206, 'num_leaves': 41, 'feature_fraction': 0.6058630798134133, 'bagging_fraction': 0.9141149648719081, 'bagging_freq': 1, 'lambda_l1': 2.5065602589073685e-08, 'lambda_l2': 3.614247596813938}. Best is trial 8 with value: 0.7202131331951266.\n",
            "[I 2025-08-10 15:35:14,122] Trial 12 finished with value: 0.7061793332135715 and parameters: {'learning_rate': 0.032166602734370074, 'num_leaves': 51, 'feature_fraction': 0.6314114034744364, 'bagging_fraction': 0.7152529778300574, 'bagging_freq': 2, 'lambda_l1': 1.0135732305076654e-08, 'lambda_l2': 3.6156837763754504}. Best is trial 8 with value: 0.7202131331951266.\n",
            "[I 2025-08-10 15:35:39,900] Trial 13 finished with value: 0.7052803921964734 and parameters: {'learning_rate': 0.09112488500630007, 'num_leaves': 40, 'feature_fraction': 0.61073414061404, 'bagging_fraction': 0.9752051965232567, 'bagging_freq': 3, 'lambda_l1': 9.893330860199337e-08, 'lambda_l2': 1.0388008860145003e-08}. Best is trial 8 with value: 0.7202131331951266.\n",
            "[I 2025-08-10 15:39:11,631] Trial 14 finished with value: 0.7216759686537499 and parameters: {'learning_rate': 0.011071347066905811, 'num_leaves': 20, 'feature_fraction': 0.7138739757223733, 'bagging_fraction': 0.7389156666893876, 'bagging_freq': 1, 'lambda_l1': 1.222238170715288e-08, 'lambda_l2': 6.1507827081461835e-06}. Best is trial 14 with value: 0.7216759686537499.\n",
            "[I 2025-08-10 15:45:24,989] Trial 15 finished with value: 0.7209417412952868 and parameters: {'learning_rate': 0.010331585578846617, 'num_leaves': 20, 'feature_fraction': 0.7067537288640453, 'bagging_fraction': 0.7356998873702343, 'bagging_freq': 3, 'lambda_l1': 1.4708355542339797e-06, 'lambda_l2': 1.7679760534477504e-06}. Best is trial 14 with value: 0.7216759686537499.\n",
            "[W 2025-08-10 15:47:05,024] Trial 16 failed with parameters: {'learning_rate': 0.01804417823067493, 'num_leaves': 100, 'feature_fraction': 0.8398309511807325, 'bagging_fraction': 0.6838769240766708, 'bagging_freq': 4, 'lambda_l1': 2.2936483489974114e-06, 'lambda_l2': 5.93876830896991e-06} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1005364002.py\", line 66, in objective_ordered\n",
            "    model = lgb.train(params, lgb_train, valid_sets=[lgb_valid], valid_names=[\"valid\"], num_boost_round=2000, callbacks=[lgb.early_stopping(50, verbose=False)])\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py\", line 322, in train\n",
            "    booster.update(fobj=fobj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\", line 4155, in update\n",
            "    _LIB.LGBM_BoosterUpdateOneIter(\n",
            "KeyboardInterrupt\n",
            "[W 2025-08-10 15:47:05,031] Trial 16 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1005364002.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m# Optimizasyon çalışmasını oluştur ve başlat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mstudy_ordered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mstudy_ordered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_ordered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Deneme sayısını ihtiyaca göre ayarlayabilirsiniz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# En iyi sonuçları al\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \"\"\"\n\u001b[0;32m--> 489\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     ):\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1005364002.py\u001b[0m in \u001b[0;36mobjective_ordered\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mlgb_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_pd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ordered'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mlgb_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_pd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_pd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ordered'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlgb_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlgb_valid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Optuna, bu döndürülen 'valid's auc' skorunu maksimize etmeye çalışacak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    320\u001b[0m             )\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_LGBM_BoosterEvalMethodResultType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4153\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot update due to null objective function.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4154\u001b[0m             _safe_call(\n\u001b[0;32m-> 4155\u001b[0;31m                 _LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   4156\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4157\u001b[0m                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kod 2** Test Etme"
      ],
      "metadata": {
        "id": "Q-eMfGlhhabl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "# trendyol_metric_group_auc.py dosyasının Colab ortamında olduğundan emin olun\n",
        "from trendyol_metric_group_auc import score\n",
        "\n",
        "print(\"--- KOD 2: Tahmin ve Skorlama Süreci Başladı ---\")\n",
        "\n",
        "# --- Adım 1: Gerekli Veri ve Kaydedilmiş Modelleri Yükleme ---\n",
        "VAL_PATH = \"/content/Trendyol Veri/enriched_train_data_FINAL.parquet\"\n",
        "TEST_PATH = \"/content/Trendyol Veri/enriched_test_data_FINAL.parquet\"\n",
        "MODEL_ORDERED_PATH = '/content/models/model_ordered_hpo_baseline.txt'\n",
        "MODEL_CLICKED_PATH = '/content/models/model_clicked_standard_baseline.txt'\n",
        "\n",
        "print(\"Veriler ve kaydedilmiş modeller yükleniyor...\")\n",
        "val_full_df = pl.read_parquet(VAL_PATH)\n",
        "test_df_pl = pl.read_parquet(TEST_PATH)\n",
        "model_ordered = lgb.Booster(model_file=MODEL_ORDERED_PATH)\n",
        "model_clicked = lgb.Booster(model_file=MODEL_CLICKED_PATH)\n",
        "print(\"✅ Veriler ve modeller başarıyla yüklendi.\")\n",
        "\n",
        "# --- Adım 2: Validation Seti Hazırlama ve Lokal Skor Hesaplama ---\n",
        "print(\"\\nLokal skor için validation seti hazırlanıyor...\")\n",
        "if \"ts_hour\" in val_full_df.columns and val_full_df[\"ts_hour\"].dtype != pl.Datetime:\n",
        "    val_full_df = val_full_df.with_columns(pl.col(\"ts_hour\").str.to_datetime())\n",
        "val_full_df = val_full_df.sort(\"ts_hour\")\n",
        "split_index = int(len(val_full_df) * 0.85)\n",
        "val_pd = val_full_df[split_index:].to_pandas().fillna(0)\n",
        "\n",
        "# Özellik listesini eğitimdekiyle tutarlı olacak şekilde belirle\n",
        "targets = [\"ordered\", \"clicked\"]\n",
        "exclude_cols = targets + [\"added_to_cart\", \"added_to_fav\", \"ts_hour\", \"session_id\", \"user_id_hashed\", \"content_id_hashed\", \"search_term_normalized\", \"level2_category_name\", \"level1_category_name\", \"cv_tags\", \"update_date\", \"content_creation_date\"]\n",
        "features = [col for col in val_pd.columns if col not in exclude_cols]\n",
        "for col in features:\n",
        "    if val_pd[col].dtype == 'object':\n",
        "        val_pd[col] = val_pd[col].astype('category')\n",
        "\n",
        "val_pd['p_order'] = model_ordered.predict(val_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "val_pd['p_click'] = model_clicked.predict(val_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "val_pd['final_score'] = 0.7 * val_pd['p_order'] + 0.3 * val_pd['p_click']\n",
        "\n",
        "# Skor fonksiyonu için gerekli DataFrame'leri oluştur\n",
        "val_solution = val_pd.groupby('session_id').agg(\n",
        "    ordered_items=('ordered', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])),\n",
        "    clicked_items=('clicked', lambda x: ' '.join(val_pd.loc[x.index][x == 1]['content_id_hashed'])),\n",
        "    all_items=('content_id_hashed', ' '.join)\n",
        ").reset_index()\n",
        "val_submission = val_pd.sort_values(['session_id', 'final_score'], ascending=[True, False]).groupby('session_id')['content_id_hashed'].apply(' '.join).reset_index()\n",
        "val_submission.rename(columns={'content_id_hashed': 'prediction'}, inplace=True)\n",
        "\n",
        "try:\n",
        "    local_final_score = score(val_solution, val_submission, 'session_id')\n",
        "    print(\"\\n-------------------------------------------\")\n",
        "    print(f\"🏆 LOKAL SKORUNUZ: {local_final_score:.5f}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "except Exception as e:\n",
        "    print(f\"Skor hesaplanırken bir hata oluştu: {e}\")\n",
        "\n",
        "# --- Adım 3: Kaggle için Submission Dosyası Oluşturma ---\n",
        "print(\"\\nTest verisi hazırlanıyor ve Kaggle için submission dosyası oluşturuluyor...\")\n",
        "test_pd = test_df_pl.to_pandas().fillna(0)\n",
        "for col in features:\n",
        "    if test_pd[col].dtype == 'object':\n",
        "        test_pd[col] = test_pd[col].astype('category')\n",
        "\n",
        "p_order_test = model_ordered.predict(test_pd[features], num_iteration=model_ordered.best_iteration)\n",
        "p_click_test = model_clicked.predict(test_pd[features], num_iteration=model_clicked.best_iteration)\n",
        "\n",
        "test_df_pl = test_df_pl.with_columns(\n",
        "    final_score=(0.7 * pl.Series(p_order_test)) + (0.3 * pl.Series(p_click_test))\n",
        ")\n",
        "\n",
        "submission_df = test_df_pl.sort([\"session_id\", \"final_score\"], descending=True).group_by(\"session_id\").agg(\n",
        "    pl.col(\"content_id_hashed\").alias(\"prediction\")\n",
        ").with_columns(\n",
        "    pl.col(\"prediction\").list.join(\" \")\n",
        ")\n",
        "\n",
        "# Sağlama kontrolü\n",
        "expected_rows = 18589\n",
        "actual_rows = submission_df.shape[0]\n",
        "print(f\"\\nOluşturulan submission satır sayısı: {actual_rows} (Beklenen: {expected_rows})\")\n",
        "if actual_rows == expected_rows: print(\"✅ Satır sayısı doğru.\")\n",
        "else: print(\"❌ UYARI: Satır sayısı yanlış!\")\n",
        "\n",
        "submission_path = \"submission_final_v4.csv\"\n",
        "submission_df.write_csv(submission_path)\n",
        "print(f\"\\nSubmission dosyası '{submission_path}' olarak kaydedildi.\")"
      ],
      "metadata": {
        "id": "3yQVA6yMha99"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}